{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"noise45.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D1</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Kurtotis1</th>\n",
       "      <th>Skew1</th>\n",
       "      <th>Entrophy1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.301090e+04</td>\n",
       "      <td>7.013850e+06</td>\n",
       "      <td>7.111279e+14</td>\n",
       "      <td>213.806538</td>\n",
       "      <td>379.176862</td>\n",
       "      <td>36.366694</td>\n",
       "      <td>3.347934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.254956e+05</td>\n",
       "      <td>4.464237e+07</td>\n",
       "      <td>7.927528e+15</td>\n",
       "      <td>98.944204</td>\n",
       "      <td>166.832505</td>\n",
       "      <td>20.732566</td>\n",
       "      <td>2.927391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.713131e-02</td>\n",
       "      <td>8.951112e+00</td>\n",
       "      <td>4.006120e+01</td>\n",
       "      <td>41.103255</td>\n",
       "      <td>125.992591</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.201003e+01</td>\n",
       "      <td>3.360204e+03</td>\n",
       "      <td>8.621839e+06</td>\n",
       "      <td>145.686862</td>\n",
       "      <td>233.440708</td>\n",
       "      <td>18.260208</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.869597e+02</td>\n",
       "      <td>7.657313e+04</td>\n",
       "      <td>3.210000e+09</td>\n",
       "      <td>192.607618</td>\n",
       "      <td>347.142722</td>\n",
       "      <td>31.213259</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.876562e+03</td>\n",
       "      <td>6.592431e+05</td>\n",
       "      <td>2.397620e+11</td>\n",
       "      <td>282.445804</td>\n",
       "      <td>500.304287</td>\n",
       "      <td>56.015115</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.635434e+06</td>\n",
       "      <td>6.992452e+08</td>\n",
       "      <td>1.630000e+17</td>\n",
       "      <td>648.975640</td>\n",
       "      <td>847.222371</td>\n",
       "      <td>97.910728</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mean          S.D1      Variance    Kurtotis1        Skew1  \\\n",
       "count  2.009000e+03  2.009000e+03  2.009000e+03  2009.000000  2009.000000   \n",
       "mean   5.301090e+04  7.013850e+06  7.111279e+14   213.806538   379.176862   \n",
       "std    3.254956e+05  4.464237e+07  7.927528e+15    98.944204   166.832505   \n",
       "min    5.713131e-02  8.951112e+00  4.006120e+01    41.103255   125.992591   \n",
       "25%    2.201003e+01  3.360204e+03  8.621839e+06   145.686862   233.440708   \n",
       "50%    5.869597e+02  7.657313e+04  3.210000e+09   192.607618   347.142722   \n",
       "75%    4.876562e+03  6.592431e+05  2.397620e+11   282.445804   500.304287   \n",
       "max    4.635434e+06  6.992452e+08  1.630000e+17   648.975640   847.222371   \n",
       "\n",
       "         Entrophy1        Class  \n",
       "count  2009.000000  2009.000000  \n",
       "mean     36.366694     3.347934  \n",
       "std      20.732566     2.927391  \n",
       "min       0.069054     0.000000  \n",
       "25%      18.260208     1.000000  \n",
       "50%      31.213259     3.000000  \n",
       "75%      56.015115     6.000000  \n",
       "max      97.910728     9.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=data.pop(\"Class\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d = tsne.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAE/CAYAAABLrsQiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ninAaCJRSKsL9P95fT67nN2ioAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634fb277b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "target_ids = range(len(target_names))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'silver', 'orange', 'purple'\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=label)\n",
    "plt.legend( loc='lower left')\n",
    "plt.savefig('t-SNE.fig', format='eps', dpi=1000)\n",
    "plt.savefig('t-SNE.eps', format='eps', dpi=1000)\n",
    "plt.title('t-SNE plot for the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from numpy import nan\n",
    "\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, random_state = 42)\n",
    "nb_classes = 10\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506\n",
      "503\n",
      "6 dims\n",
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'dims')\n",
    "print(\"Building model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_train = X_train.min(axis=0)\n",
    "range_train = (X_train - min_train).max(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - min_train)/range_train\n",
    "\n",
    "#print('Minimum per feature\\n{}'.format(X_train_scaled.min(axis=0)))\n",
    "#print('Maximum per feature\\n{}'.format(X_train_scaled.max(axis=0)))\n",
    "\n",
    "X_test_scaled = (X_test - min_train)/range_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE/CAYAAABFK3gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXHTRkFoZjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634b42cb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_2d = tsne.fit_transform(X_train_scaled)\n",
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "target_ids = range(len(target_names))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'silver', 'orange', 'purple'\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(X_2d[y_train == i, 0], X_2d[y_train == i, 1], c=c, label=label)\n",
    "plt.legend( loc='lower left')\n",
    "plt.savefig('t-SNE_for_scaled.fig', format='eps', dpi=1000)\n",
    "plt.savefig('t-SNE_for_scaled.eps', format='eps', dpi=1000)\n",
    "plt.title('t-SNE plot for the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE/CAYAAABFK3gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634ca83f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_2d = tsne.fit_transform(X_test_scaled)\n",
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "target_ids = range(len(target_names))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'silver', 'orange', 'purple'\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(X_2d[y_test == i, 0], X_2d[y_test == i, 1], c=c, label=label)\n",
    "plt.legend( loc='lower left')\n",
    "plt.savefig('t-SNE_for_scaled_testdata.fig', format='eps', dpi=1000)\n",
    "plt.savefig('t-SNE_for_scaled_testdata.eps', format='eps', dpi=1000)\n",
    "plt.title('t-SNE plot for the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/3000\n",
      "1506/1506 [==============================] - 2s 1ms/step - loss: 2.2240 - acc: 0.1906 - val_loss: 2.1323 - val_acc: 0.2366\n",
      "Epoch 2/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 2.0820 - acc: 0.2178 - val_loss: 1.9835 - val_acc: 0.2604\n",
      "Epoch 3/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.9349 - acc: 0.3028 - val_loss: 1.8225 - val_acc: 0.4692\n",
      "Epoch 4/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.7542 - acc: 0.4210 - val_loss: 1.6341 - val_acc: 0.4672\n",
      "Epoch 5/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.5807 - acc: 0.4210 - val_loss: 1.4934 - val_acc: 0.4672\n",
      "Epoch 6/3000\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 1.4509 - acc: 0.4329 - val_loss: 1.3932 - val_acc: 0.4791\n",
      "Epoch 7/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 1.3544 - acc: 0.4509 - val_loss: 1.3178 - val_acc: 0.5209\n",
      "Epoch 8/3000\n",
      "1506/1506 [==============================] - 0s 45us/step - loss: 1.2716 - acc: 0.5113 - val_loss: 1.2473 - val_acc: 0.5666\n",
      "Epoch 9/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 1.1957 - acc: 0.5691 - val_loss: 1.1870 - val_acc: 0.6481\n",
      "Epoch 10/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.1257 - acc: 0.6753 - val_loss: 1.1224 - val_acc: 0.6799\n",
      "Epoch 11/3000\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 1.0553 - acc: 0.6926 - val_loss: 1.0610 - val_acc: 0.7117\n",
      "Epoch 12/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.9878 - acc: 0.7205 - val_loss: 0.9975 - val_acc: 0.7117\n",
      "Epoch 13/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.9298 - acc: 0.7576 - val_loss: 0.9450 - val_acc: 0.7396\n",
      "Epoch 14/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.8723 - acc: 0.7742 - val_loss: 0.8982 - val_acc: 0.7813\n",
      "Epoch 15/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.8232 - acc: 0.7922 - val_loss: 0.8576 - val_acc: 0.8191\n",
      "Epoch 16/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.7793 - acc: 0.8207 - val_loss: 0.8152 - val_acc: 0.8231\n",
      "Epoch 17/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.7402 - acc: 0.8287 - val_loss: 0.7866 - val_acc: 0.8290\n",
      "Epoch 18/3000\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 0.7085 - acc: 0.8440 - val_loss: 0.7485 - val_acc: 0.8310\n",
      "Epoch 19/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6793 - acc: 0.8393 - val_loss: 0.7250 - val_acc: 0.8310\n",
      "Epoch 20/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.6566 - acc: 0.8473 - val_loss: 0.7057 - val_acc: 0.8310\n",
      "Epoch 21/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6319 - acc: 0.8519 - val_loss: 0.6867 - val_acc: 0.8390\n",
      "Epoch 22/3000\n",
      "1506/1506 [==============================] - 0s 45us/step - loss: 0.6140 - acc: 0.8459 - val_loss: 0.6684 - val_acc: 0.8330\n",
      "Epoch 23/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.5962 - acc: 0.8606 - val_loss: 0.6525 - val_acc: 0.8310\n",
      "Epoch 24/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.5798 - acc: 0.8599 - val_loss: 0.6330 - val_acc: 0.8350\n",
      "Epoch 25/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5642 - acc: 0.8533 - val_loss: 0.6232 - val_acc: 0.8310\n",
      "Epoch 26/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5477 - acc: 0.8592 - val_loss: 0.6102 - val_acc: 0.8429\n",
      "Epoch 27/3000\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 0.5356 - acc: 0.8705 - val_loss: 0.6024 - val_acc: 0.8390\n",
      "Epoch 28/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.5256 - acc: 0.8586 - val_loss: 0.5866 - val_acc: 0.8390\n",
      "Epoch 29/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5106 - acc: 0.8718 - val_loss: 0.5789 - val_acc: 0.8449\n",
      "Epoch 30/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5048 - acc: 0.8692 - val_loss: 0.5722 - val_acc: 0.8410\n",
      "Epoch 31/3000\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 0.4962 - acc: 0.8712 - val_loss: 0.5585 - val_acc: 0.8509\n",
      "Epoch 32/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.4854 - acc: 0.8705 - val_loss: 0.5617 - val_acc: 0.8489\n",
      "Epoch 33/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.4779 - acc: 0.8745 - val_loss: 0.5468 - val_acc: 0.8509\n",
      "Epoch 34/3000\n",
      "1506/1506 [==============================] - 0s 49us/step - loss: 0.4703 - acc: 0.8765 - val_loss: 0.5378 - val_acc: 0.8469\n",
      "Epoch 35/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.4614 - acc: 0.8765 - val_loss: 0.5331 - val_acc: 0.8469\n",
      "Epoch 36/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.4539 - acc: 0.8792 - val_loss: 0.5254 - val_acc: 0.8549\n",
      "Epoch 37/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.4472 - acc: 0.8845 - val_loss: 0.5202 - val_acc: 0.8509\n",
      "Epoch 38/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.4449 - acc: 0.8831 - val_loss: 0.5131 - val_acc: 0.8469\n",
      "Epoch 39/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.4387 - acc: 0.8792 - val_loss: 0.5084 - val_acc: 0.8469\n",
      "Epoch 40/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.4309 - acc: 0.8845 - val_loss: 0.5086 - val_acc: 0.8529\n",
      "Epoch 41/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4273 - acc: 0.8811 - val_loss: 0.5028 - val_acc: 0.8489\n",
      "Epoch 42/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4229 - acc: 0.8865 - val_loss: 0.4956 - val_acc: 0.8509\n",
      "Epoch 43/3000\n",
      "1506/1506 [==============================] - 0s 47us/step - loss: 0.4186 - acc: 0.8845 - val_loss: 0.4912 - val_acc: 0.8549\n",
      "Epoch 44/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.4166 - acc: 0.8878 - val_loss: 0.4897 - val_acc: 0.8588\n",
      "Epoch 45/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.4108 - acc: 0.8891 - val_loss: 0.4894 - val_acc: 0.8489\n",
      "Epoch 46/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4151 - acc: 0.8865 - val_loss: 0.4873 - val_acc: 0.8569\n",
      "Epoch 47/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4110 - acc: 0.8891 - val_loss: 0.4841 - val_acc: 0.8588\n",
      "Epoch 48/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.4010 - acc: 0.8878 - val_loss: 0.4843 - val_acc: 0.8608\n",
      "Epoch 49/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3990 - acc: 0.8918 - val_loss: 0.4837 - val_acc: 0.8588\n",
      "Epoch 50/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3937 - acc: 0.8891 - val_loss: 0.4746 - val_acc: 0.8608\n",
      "Epoch 51/3000\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 0.3948 - acc: 0.8891 - val_loss: 0.4694 - val_acc: 0.8529\n",
      "Epoch 52/3000\n",
      "1506/1506 [==============================] - 0s 46us/step - loss: 0.3892 - acc: 0.8904 - val_loss: 0.4716 - val_acc: 0.8608\n",
      "Epoch 53/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3886 - acc: 0.8931 - val_loss: 0.4674 - val_acc: 0.8608\n",
      "Epoch 54/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3854 - acc: 0.8938 - val_loss: 0.4639 - val_acc: 0.8588\n",
      "Epoch 55/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3835 - acc: 0.8924 - val_loss: 0.4550 - val_acc: 0.8569\n",
      "Epoch 56/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3815 - acc: 0.8938 - val_loss: 0.4583 - val_acc: 0.8529\n",
      "Epoch 57/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3777 - acc: 0.8964 - val_loss: 0.4597 - val_acc: 0.8529\n",
      "Epoch 58/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3757 - acc: 0.8918 - val_loss: 0.4512 - val_acc: 0.8569\n",
      "Epoch 59/3000\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3729 - acc: 0.8911 - val_loss: 0.4522 - val_acc: 0.8569\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3693 - acc: 0.8924 - val_loss: 0.4527 - val_acc: 0.8648\n",
      "Epoch 61/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3676 - acc: 0.8984 - val_loss: 0.4496 - val_acc: 0.8569\n",
      "Epoch 62/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3721 - acc: 0.8931 - val_loss: 0.4507 - val_acc: 0.8588\n",
      "Epoch 63/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3696 - acc: 0.8958 - val_loss: 0.4475 - val_acc: 0.8648\n",
      "Epoch 64/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3669 - acc: 0.8958 - val_loss: 0.4454 - val_acc: 0.8688\n",
      "Epoch 65/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3616 - acc: 0.8977 - val_loss: 0.4448 - val_acc: 0.8588\n",
      "Epoch 66/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3622 - acc: 0.8931 - val_loss: 0.4445 - val_acc: 0.8628\n",
      "Epoch 67/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3577 - acc: 0.8958 - val_loss: 0.4386 - val_acc: 0.8648\n",
      "Epoch 68/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3561 - acc: 0.8958 - val_loss: 0.4375 - val_acc: 0.8668\n",
      "Epoch 69/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3596 - acc: 0.8997 - val_loss: 0.4403 - val_acc: 0.8648\n",
      "Epoch 70/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3541 - acc: 0.9004 - val_loss: 0.4404 - val_acc: 0.8708\n",
      "Epoch 71/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.3563 - acc: 0.8944 - val_loss: 0.4330 - val_acc: 0.8628\n",
      "Epoch 72/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3494 - acc: 0.8991 - val_loss: 0.4333 - val_acc: 0.8608\n",
      "Epoch 73/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3500 - acc: 0.8971 - val_loss: 0.4337 - val_acc: 0.8628\n",
      "Epoch 74/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3489 - acc: 0.8997 - val_loss: 0.4258 - val_acc: 0.8688\n",
      "Epoch 75/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3468 - acc: 0.8984 - val_loss: 0.4263 - val_acc: 0.8628\n",
      "Epoch 76/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3460 - acc: 0.8997 - val_loss: 0.4302 - val_acc: 0.8708\n",
      "Epoch 77/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3464 - acc: 0.8977 - val_loss: 0.4217 - val_acc: 0.8648\n",
      "Epoch 78/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3476 - acc: 0.8984 - val_loss: 0.4245 - val_acc: 0.8608\n",
      "Epoch 79/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3431 - acc: 0.8997 - val_loss: 0.4273 - val_acc: 0.8708\n",
      "Epoch 80/3000\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3432 - acc: 0.8977 - val_loss: 0.4280 - val_acc: 0.8688\n",
      "Epoch 81/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3477 - acc: 0.8938 - val_loss: 0.4238 - val_acc: 0.8748\n",
      "Epoch 82/3000\n",
      "1506/1506 [==============================] - 0s 43us/step - loss: 0.3413 - acc: 0.8971 - val_loss: 0.4228 - val_acc: 0.8688\n",
      "Epoch 83/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3394 - acc: 0.8977 - val_loss: 0.4279 - val_acc: 0.8748\n",
      "Epoch 84/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3383 - acc: 0.8964 - val_loss: 0.4183 - val_acc: 0.8728\n",
      "Epoch 85/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3369 - acc: 0.9024 - val_loss: 0.4127 - val_acc: 0.8648\n",
      "Epoch 86/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3456 - acc: 0.8904 - val_loss: 0.4267 - val_acc: 0.8668\n",
      "Epoch 87/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3563 - acc: 0.8958 - val_loss: 0.4139 - val_acc: 0.8708\n",
      "Epoch 88/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3334 - acc: 0.8997 - val_loss: 0.4102 - val_acc: 0.8708\n",
      "Epoch 89/3000\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3321 - acc: 0.8997 - val_loss: 0.4106 - val_acc: 0.8748\n",
      "Epoch 90/3000\n",
      "1506/1506 [==============================] - 0s 46us/step - loss: 0.3305 - acc: 0.9011 - val_loss: 0.4149 - val_acc: 0.8708\n",
      "Epoch 91/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3277 - acc: 0.8997 - val_loss: 0.4187 - val_acc: 0.8787\n",
      "Epoch 92/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3282 - acc: 0.9050 - val_loss: 0.4132 - val_acc: 0.8668\n",
      "Epoch 93/3000\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 0.3331 - acc: 0.8984 - val_loss: 0.4183 - val_acc: 0.8748\n",
      "Epoch 94/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3293 - acc: 0.9004 - val_loss: 0.4097 - val_acc: 0.8748\n",
      "Epoch 95/3000\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 0.3283 - acc: 0.9050 - val_loss: 0.4039 - val_acc: 0.8708\n",
      "Epoch 96/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.3283 - acc: 0.8997 - val_loss: 0.4029 - val_acc: 0.8728\n",
      "Epoch 97/3000\n",
      "1506/1506 [==============================] - 0s 125us/step - loss: 0.3225 - acc: 0.9037 - val_loss: 0.4085 - val_acc: 0.8728\n",
      "Epoch 98/3000\n",
      "1506/1506 [==============================] - 0s 139us/step - loss: 0.3300 - acc: 0.8997 - val_loss: 0.4052 - val_acc: 0.8648\n",
      "Epoch 99/3000\n",
      "1506/1506 [==============================] - 0s 118us/step - loss: 0.3278 - acc: 0.8997 - val_loss: 0.4083 - val_acc: 0.8728\n",
      "Epoch 100/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3244 - acc: 0.9044 - val_loss: 0.4045 - val_acc: 0.8787\n",
      "Epoch 101/3000\n",
      "1506/1506 [==============================] - 0s 46us/step - loss: 0.3229 - acc: 0.8997 - val_loss: 0.4120 - val_acc: 0.8787\n",
      "Epoch 102/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.3213 - acc: 0.9011 - val_loss: 0.4036 - val_acc: 0.8728\n",
      "Epoch 103/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3239 - acc: 0.9024 - val_loss: 0.4062 - val_acc: 0.8787\n",
      "Epoch 104/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3212 - acc: 0.9050 - val_loss: 0.4160 - val_acc: 0.8748\n",
      "Epoch 105/3000\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 0.3301 - acc: 0.8997 - val_loss: 0.3976 - val_acc: 0.8787\n",
      "Epoch 106/3000\n",
      "1506/1506 [==============================] - 0s 47us/step - loss: 0.3296 - acc: 0.8984 - val_loss: 0.4070 - val_acc: 0.8767\n",
      "Epoch 107/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3224 - acc: 0.9004 - val_loss: 0.4012 - val_acc: 0.8767\n",
      "Epoch 108/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3201 - acc: 0.9044 - val_loss: 0.4055 - val_acc: 0.8807\n",
      "Epoch 109/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3184 - acc: 0.9044 - val_loss: 0.3998 - val_acc: 0.8787\n",
      "Epoch 110/3000\n",
      "1506/1506 [==============================] - 0s 124us/step - loss: 0.3150 - acc: 0.9050 - val_loss: 0.3969 - val_acc: 0.8807\n",
      "Epoch 111/3000\n",
      "1506/1506 [==============================] - 0s 123us/step - loss: 0.3271 - acc: 0.8944 - val_loss: 0.4007 - val_acc: 0.8748\n",
      "Epoch 112/3000\n",
      "1506/1506 [==============================] - 0s 124us/step - loss: 0.3189 - acc: 0.9031 - val_loss: 0.3960 - val_acc: 0.8847\n",
      "Epoch 113/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3136 - acc: 0.9017 - val_loss: 0.3917 - val_acc: 0.8728\n",
      "Epoch 114/3000\n",
      "1506/1506 [==============================] - 0s 140us/step - loss: 0.3123 - acc: 0.9050 - val_loss: 0.3936 - val_acc: 0.8767\n",
      "Epoch 115/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3132 - acc: 0.9024 - val_loss: 0.3922 - val_acc: 0.8787\n",
      "Epoch 116/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3129 - acc: 0.9017 - val_loss: 0.4122 - val_acc: 0.8748\n",
      "Epoch 117/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3151 - acc: 0.9050 - val_loss: 0.3901 - val_acc: 0.8787\n",
      "Epoch 118/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3140 - acc: 0.9064 - val_loss: 0.3913 - val_acc: 0.8787\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3135 - acc: 0.9050 - val_loss: 0.3985 - val_acc: 0.8628\n",
      "Epoch 120/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3113 - acc: 0.9017 - val_loss: 0.3972 - val_acc: 0.8787\n",
      "Epoch 121/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3109 - acc: 0.9011 - val_loss: 0.3880 - val_acc: 0.8767\n",
      "Epoch 122/3000\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3077 - acc: 0.9037 - val_loss: 0.3924 - val_acc: 0.8787\n",
      "Epoch 123/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.3134 - acc: 0.8997 - val_loss: 0.4002 - val_acc: 0.8827\n",
      "Epoch 124/3000\n",
      "1506/1506 [==============================] - 0s 49us/step - loss: 0.3098 - acc: 0.9064 - val_loss: 0.3978 - val_acc: 0.8827\n",
      "Epoch 125/3000\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3083 - acc: 0.9031 - val_loss: 0.3904 - val_acc: 0.8827\n",
      "Epoch 126/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.3087 - acc: 0.9070 - val_loss: 0.3931 - val_acc: 0.8787\n",
      "Epoch 127/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3070 - acc: 0.9037 - val_loss: 0.3954 - val_acc: 0.8767\n",
      "Epoch 128/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3072 - acc: 0.9057 - val_loss: 0.3952 - val_acc: 0.8807\n",
      "Epoch 129/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3057 - acc: 0.9050 - val_loss: 0.3916 - val_acc: 0.8807\n",
      "Epoch 130/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3050 - acc: 0.9070 - val_loss: 0.3879 - val_acc: 0.8827\n",
      "Epoch 131/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3040 - acc: 0.9070 - val_loss: 0.3837 - val_acc: 0.8827\n",
      "Epoch 132/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3053 - acc: 0.9070 - val_loss: 0.3891 - val_acc: 0.8767\n",
      "Epoch 133/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3075 - acc: 0.9004 - val_loss: 0.3909 - val_acc: 0.8787\n",
      "Epoch 134/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.3034 - acc: 0.9064 - val_loss: 0.3858 - val_acc: 0.8827\n",
      "Epoch 135/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3022 - acc: 0.9064 - val_loss: 0.3817 - val_acc: 0.8728\n",
      "Epoch 136/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3025 - acc: 0.9057 - val_loss: 0.3835 - val_acc: 0.8827\n",
      "Epoch 137/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3105 - acc: 0.9031 - val_loss: 0.3799 - val_acc: 0.8887\n",
      "Epoch 138/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2985 - acc: 0.9064 - val_loss: 0.3817 - val_acc: 0.8807\n",
      "Epoch 139/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3020 - acc: 0.9050 - val_loss: 0.3750 - val_acc: 0.8807\n",
      "Epoch 140/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.2985 - acc: 0.9104 - val_loss: 0.3805 - val_acc: 0.8807\n",
      "Epoch 141/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.2986 - acc: 0.9070 - val_loss: 0.3785 - val_acc: 0.8767\n",
      "Epoch 142/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3016 - acc: 0.9064 - val_loss: 0.3850 - val_acc: 0.8787\n",
      "Epoch 143/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2958 - acc: 0.9044 - val_loss: 0.3899 - val_acc: 0.8847\n",
      "Epoch 144/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3045 - acc: 0.9097 - val_loss: 0.3948 - val_acc: 0.8807\n",
      "Epoch 145/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2971 - acc: 0.9104 - val_loss: 0.3753 - val_acc: 0.8827\n",
      "Epoch 146/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2979 - acc: 0.9084 - val_loss: 0.3852 - val_acc: 0.8807\n",
      "Epoch 147/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3001 - acc: 0.9064 - val_loss: 0.3793 - val_acc: 0.8787\n",
      "Epoch 148/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2992 - acc: 0.9077 - val_loss: 0.3800 - val_acc: 0.8807\n",
      "Epoch 149/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3078 - acc: 0.9057 - val_loss: 0.3722 - val_acc: 0.8807\n",
      "Epoch 150/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3031 - acc: 0.9104 - val_loss: 0.3838 - val_acc: 0.8807\n",
      "Epoch 151/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2980 - acc: 0.9077 - val_loss: 0.3795 - val_acc: 0.8847\n",
      "Epoch 152/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2947 - acc: 0.9070 - val_loss: 0.3681 - val_acc: 0.8827\n",
      "Epoch 153/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2973 - acc: 0.9077 - val_loss: 0.3797 - val_acc: 0.8867\n",
      "Epoch 154/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2957 - acc: 0.9084 - val_loss: 0.3710 - val_acc: 0.8787\n",
      "Epoch 155/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2982 - acc: 0.9077 - val_loss: 0.3856 - val_acc: 0.8827\n",
      "Epoch 156/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2931 - acc: 0.9090 - val_loss: 0.3709 - val_acc: 0.8827\n",
      "Epoch 157/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2924 - acc: 0.9117 - val_loss: 0.3672 - val_acc: 0.8827\n",
      "Epoch 158/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3015 - acc: 0.9057 - val_loss: 0.3644 - val_acc: 0.8867\n",
      "Epoch 159/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2926 - acc: 0.9097 - val_loss: 0.3731 - val_acc: 0.8827\n",
      "Epoch 160/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2927 - acc: 0.9070 - val_loss: 0.3776 - val_acc: 0.8767\n",
      "Epoch 161/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2925 - acc: 0.9104 - val_loss: 0.3623 - val_acc: 0.8867\n",
      "Epoch 162/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2922 - acc: 0.9050 - val_loss: 0.3720 - val_acc: 0.8847\n",
      "Epoch 163/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2897 - acc: 0.9090 - val_loss: 0.3679 - val_acc: 0.8827\n",
      "Epoch 164/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2897 - acc: 0.9070 - val_loss: 0.3769 - val_acc: 0.8887\n",
      "Epoch 165/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2907 - acc: 0.9070 - val_loss: 0.3760 - val_acc: 0.8827\n",
      "Epoch 166/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2896 - acc: 0.9097 - val_loss: 0.3648 - val_acc: 0.8847\n",
      "Epoch 167/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2894 - acc: 0.9097 - val_loss: 0.3599 - val_acc: 0.8867\n",
      "Epoch 168/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2912 - acc: 0.9084 - val_loss: 0.3639 - val_acc: 0.8867\n",
      "Epoch 169/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2916 - acc: 0.9057 - val_loss: 0.3666 - val_acc: 0.8807\n",
      "Epoch 170/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2877 - acc: 0.9090 - val_loss: 0.3615 - val_acc: 0.8867\n",
      "Epoch 171/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2874 - acc: 0.9097 - val_loss: 0.3779 - val_acc: 0.8847\n",
      "Epoch 172/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2916 - acc: 0.9097 - val_loss: 0.3657 - val_acc: 0.8827\n",
      "Epoch 173/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2908 - acc: 0.9090 - val_loss: 0.3647 - val_acc: 0.8867\n",
      "Epoch 174/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2883 - acc: 0.9057 - val_loss: 0.3590 - val_acc: 0.8847\n",
      "Epoch 175/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2902 - acc: 0.9097 - val_loss: 0.3641 - val_acc: 0.8847\n",
      "Epoch 176/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2891 - acc: 0.9084 - val_loss: 0.3664 - val_acc: 0.8867\n",
      "Epoch 177/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2869 - acc: 0.9084 - val_loss: 0.3779 - val_acc: 0.8827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2876 - acc: 0.9077 - val_loss: 0.3711 - val_acc: 0.8867\n",
      "Epoch 179/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2849 - acc: 0.9117 - val_loss: 0.3564 - val_acc: 0.8807\n",
      "Epoch 180/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2874 - acc: 0.9084 - val_loss: 0.3580 - val_acc: 0.8827\n",
      "Epoch 181/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2828 - acc: 0.9117 - val_loss: 0.3669 - val_acc: 0.8827\n",
      "Epoch 182/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2835 - acc: 0.9084 - val_loss: 0.3609 - val_acc: 0.8867\n",
      "Epoch 183/3000\n",
      "1506/1506 [==============================] - 0s 127us/step - loss: 0.2880 - acc: 0.9097 - val_loss: 0.3730 - val_acc: 0.8867\n",
      "Epoch 184/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.2878 - acc: 0.9090 - val_loss: 0.3522 - val_acc: 0.8887\n",
      "Epoch 185/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.2884 - acc: 0.9097 - val_loss: 0.3513 - val_acc: 0.8847\n",
      "Epoch 186/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2889 - acc: 0.9090 - val_loss: 0.3569 - val_acc: 0.8847\n",
      "Epoch 187/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2820 - acc: 0.9077 - val_loss: 0.3614 - val_acc: 0.8867\n",
      "Epoch 188/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2837 - acc: 0.9097 - val_loss: 0.3833 - val_acc: 0.8867\n",
      "Epoch 189/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2812 - acc: 0.9110 - val_loss: 0.3532 - val_acc: 0.8847\n",
      "Epoch 190/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2814 - acc: 0.9090 - val_loss: 0.3550 - val_acc: 0.8847\n",
      "Epoch 191/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2833 - acc: 0.9117 - val_loss: 0.3494 - val_acc: 0.8827\n",
      "Epoch 192/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2817 - acc: 0.9084 - val_loss: 0.3547 - val_acc: 0.8847\n",
      "Epoch 193/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2830 - acc: 0.9097 - val_loss: 0.3523 - val_acc: 0.8867\n",
      "Epoch 194/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2864 - acc: 0.9090 - val_loss: 0.3487 - val_acc: 0.8827\n",
      "Epoch 195/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2815 - acc: 0.9124 - val_loss: 0.3635 - val_acc: 0.8767\n",
      "Epoch 196/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.2806 - acc: 0.9090 - val_loss: 0.3639 - val_acc: 0.8867\n",
      "Epoch 197/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2807 - acc: 0.9070 - val_loss: 0.3632 - val_acc: 0.8887\n",
      "Epoch 198/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2821 - acc: 0.9097 - val_loss: 0.3520 - val_acc: 0.8867\n",
      "Epoch 199/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2799 - acc: 0.9110 - val_loss: 0.3478 - val_acc: 0.8827\n",
      "Epoch 200/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2827 - acc: 0.9070 - val_loss: 0.3549 - val_acc: 0.8867\n",
      "Epoch 201/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.2790 - acc: 0.9130 - val_loss: 0.3532 - val_acc: 0.8867\n",
      "Epoch 202/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2799 - acc: 0.9124 - val_loss: 0.3512 - val_acc: 0.8847\n",
      "Epoch 203/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2790 - acc: 0.9117 - val_loss: 0.3502 - val_acc: 0.8867\n",
      "Epoch 204/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2786 - acc: 0.9104 - val_loss: 0.3517 - val_acc: 0.8887\n",
      "Epoch 205/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2766 - acc: 0.9097 - val_loss: 0.3665 - val_acc: 0.8867\n",
      "Epoch 206/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2772 - acc: 0.9110 - val_loss: 0.3684 - val_acc: 0.8867\n",
      "Epoch 207/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2796 - acc: 0.9110 - val_loss: 0.3461 - val_acc: 0.8847\n",
      "Epoch 208/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2773 - acc: 0.9090 - val_loss: 0.3467 - val_acc: 0.8867\n",
      "Epoch 209/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2782 - acc: 0.9084 - val_loss: 0.3641 - val_acc: 0.8907\n",
      "Epoch 210/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2953 - acc: 0.9064 - val_loss: 0.3467 - val_acc: 0.8907\n",
      "Epoch 211/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2752 - acc: 0.9117 - val_loss: 0.3539 - val_acc: 0.8867\n",
      "Epoch 212/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2750 - acc: 0.9137 - val_loss: 0.3454 - val_acc: 0.8847\n",
      "Epoch 213/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2719 - acc: 0.9104 - val_loss: 0.3519 - val_acc: 0.8807\n",
      "Epoch 214/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2737 - acc: 0.9104 - val_loss: 0.3475 - val_acc: 0.8847\n",
      "Epoch 215/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2755 - acc: 0.9130 - val_loss: 0.3481 - val_acc: 0.8887\n",
      "Epoch 216/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2761 - acc: 0.9110 - val_loss: 0.3503 - val_acc: 0.8867\n",
      "Epoch 217/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2717 - acc: 0.9137 - val_loss: 0.3418 - val_acc: 0.8847\n",
      "Epoch 218/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2764 - acc: 0.9097 - val_loss: 0.3415 - val_acc: 0.8847\n",
      "Epoch 219/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2743 - acc: 0.9157 - val_loss: 0.3519 - val_acc: 0.8926\n",
      "Epoch 220/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2725 - acc: 0.9150 - val_loss: 0.3514 - val_acc: 0.8887\n",
      "Epoch 221/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2741 - acc: 0.9150 - val_loss: 0.3509 - val_acc: 0.8867\n",
      "Epoch 222/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2748 - acc: 0.9117 - val_loss: 0.3462 - val_acc: 0.8847\n",
      "Epoch 223/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2784 - acc: 0.9104 - val_loss: 0.3399 - val_acc: 0.8887\n",
      "Epoch 224/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2712 - acc: 0.9137 - val_loss: 0.3478 - val_acc: 0.8887\n",
      "Epoch 225/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2754 - acc: 0.9110 - val_loss: 0.3441 - val_acc: 0.8887\n",
      "Epoch 226/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.2730 - acc: 0.9117 - val_loss: 0.3429 - val_acc: 0.8827\n",
      "Epoch 227/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.2686 - acc: 0.9124 - val_loss: 0.3525 - val_acc: 0.8867\n",
      "Epoch 228/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2722 - acc: 0.9117 - val_loss: 0.3418 - val_acc: 0.8827\n",
      "Epoch 229/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.2689 - acc: 0.9117 - val_loss: 0.3459 - val_acc: 0.8847\n",
      "Epoch 230/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.2706 - acc: 0.9150 - val_loss: 0.3437 - val_acc: 0.8827\n",
      "Epoch 231/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.2710 - acc: 0.9097 - val_loss: 0.3407 - val_acc: 0.8847\n",
      "Epoch 232/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2694 - acc: 0.9137 - val_loss: 0.3450 - val_acc: 0.8847\n",
      "Epoch 233/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2724 - acc: 0.9137 - val_loss: 0.3379 - val_acc: 0.8887\n",
      "Epoch 234/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.2702 - acc: 0.9150 - val_loss: 0.3452 - val_acc: 0.8827\n",
      "Epoch 235/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.2692 - acc: 0.9143 - val_loss: 0.3409 - val_acc: 0.8907\n",
      "Epoch 236/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2698 - acc: 0.9117 - val_loss: 0.3388 - val_acc: 0.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.2688 - acc: 0.9137 - val_loss: 0.3420 - val_acc: 0.8887\n",
      "Epoch 238/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.2691 - acc: 0.9143 - val_loss: 0.3381 - val_acc: 0.8867\n",
      "Epoch 239/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.2680 - acc: 0.9090 - val_loss: 0.3354 - val_acc: 0.8887\n",
      "Epoch 240/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2806 - acc: 0.9110 - val_loss: 0.3406 - val_acc: 0.8887\n",
      "Epoch 241/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2669 - acc: 0.9137 - val_loss: 0.3494 - val_acc: 0.8867\n",
      "Epoch 242/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2708 - acc: 0.9110 - val_loss: 0.3407 - val_acc: 0.8847\n",
      "Epoch 243/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2663 - acc: 0.9104 - val_loss: 0.3335 - val_acc: 0.8907\n",
      "Epoch 244/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2645 - acc: 0.9130 - val_loss: 0.3361 - val_acc: 0.8867\n",
      "Epoch 245/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.2663 - acc: 0.9137 - val_loss: 0.3338 - val_acc: 0.8827\n",
      "Epoch 246/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2679 - acc: 0.9157 - val_loss: 0.3379 - val_acc: 0.8867\n",
      "Epoch 247/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.2647 - acc: 0.9150 - val_loss: 0.3388 - val_acc: 0.8887\n",
      "Epoch 248/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.2646 - acc: 0.9157 - val_loss: 0.3345 - val_acc: 0.8887\n",
      "Epoch 249/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2696 - acc: 0.9124 - val_loss: 0.3310 - val_acc: 0.8907\n",
      "Epoch 250/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2662 - acc: 0.9143 - val_loss: 0.3321 - val_acc: 0.8887\n",
      "Epoch 251/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2650 - acc: 0.9163 - val_loss: 0.3414 - val_acc: 0.8827\n",
      "Epoch 252/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2674 - acc: 0.9124 - val_loss: 0.3464 - val_acc: 0.8887\n",
      "Epoch 253/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2633 - acc: 0.9130 - val_loss: 0.3314 - val_acc: 0.8926\n",
      "Epoch 254/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2622 - acc: 0.9143 - val_loss: 0.3333 - val_acc: 0.8867\n",
      "Epoch 255/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2647 - acc: 0.9170 - val_loss: 0.3255 - val_acc: 0.8946\n",
      "Epoch 256/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2630 - acc: 0.9183 - val_loss: 0.3403 - val_acc: 0.8926\n",
      "Epoch 257/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2635 - acc: 0.9143 - val_loss: 0.3473 - val_acc: 0.8887\n",
      "Epoch 258/3000\n",
      "1506/1506 [==============================] - 0s 97us/step - loss: 0.2650 - acc: 0.9130 - val_loss: 0.3310 - val_acc: 0.8926\n",
      "Epoch 259/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.2623 - acc: 0.9150 - val_loss: 0.3371 - val_acc: 0.8887\n",
      "Epoch 260/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.2607 - acc: 0.9137 - val_loss: 0.3224 - val_acc: 0.8887\n",
      "Epoch 261/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.2624 - acc: 0.9150 - val_loss: 0.3269 - val_acc: 0.8907\n",
      "Epoch 262/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.2644 - acc: 0.9130 - val_loss: 0.3244 - val_acc: 0.8907\n",
      "Epoch 263/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.2622 - acc: 0.9110 - val_loss: 0.3338 - val_acc: 0.8907\n",
      "Epoch 264/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.2618 - acc: 0.9117 - val_loss: 0.3298 - val_acc: 0.8926\n",
      "Epoch 265/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.2602 - acc: 0.9137 - val_loss: 0.3279 - val_acc: 0.8907\n",
      "Epoch 266/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.2635 - acc: 0.9117 - val_loss: 0.3277 - val_acc: 0.8926\n",
      "Epoch 267/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.2645 - acc: 0.9150 - val_loss: 0.3206 - val_acc: 0.8946\n",
      "Epoch 268/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.2716 - acc: 0.9163 - val_loss: 0.3221 - val_acc: 0.8966\n",
      "Epoch 269/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.2640 - acc: 0.9177 - val_loss: 0.3335 - val_acc: 0.8907\n",
      "Epoch 270/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2598 - acc: 0.9157 - val_loss: 0.3325 - val_acc: 0.8946\n",
      "Epoch 271/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2578 - acc: 0.9137 - val_loss: 0.3280 - val_acc: 0.8867\n",
      "Epoch 272/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2570 - acc: 0.9163 - val_loss: 0.3211 - val_acc: 0.8907\n",
      "Epoch 273/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2590 - acc: 0.9137 - val_loss: 0.3433 - val_acc: 0.8907\n",
      "Epoch 274/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2586 - acc: 0.9163 - val_loss: 0.3357 - val_acc: 0.8827\n",
      "Epoch 275/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2591 - acc: 0.9150 - val_loss: 0.3287 - val_acc: 0.8867\n",
      "Epoch 276/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2581 - acc: 0.9190 - val_loss: 0.3345 - val_acc: 0.8926\n",
      "Epoch 277/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2586 - acc: 0.9150 - val_loss: 0.3352 - val_acc: 0.8926\n",
      "Epoch 278/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2566 - acc: 0.9137 - val_loss: 0.3259 - val_acc: 0.8946\n",
      "Epoch 279/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2579 - acc: 0.9157 - val_loss: 0.3260 - val_acc: 0.8946\n",
      "Epoch 280/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2590 - acc: 0.9170 - val_loss: 0.3216 - val_acc: 0.8907\n",
      "Epoch 281/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2572 - acc: 0.9163 - val_loss: 0.3458 - val_acc: 0.8926\n",
      "Epoch 282/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2571 - acc: 0.9163 - val_loss: 0.3192 - val_acc: 0.8926\n",
      "Epoch 283/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2536 - acc: 0.9163 - val_loss: 0.3214 - val_acc: 0.8907\n",
      "Epoch 284/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2608 - acc: 0.9183 - val_loss: 0.3257 - val_acc: 0.8867\n",
      "Epoch 285/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2543 - acc: 0.9163 - val_loss: 0.3264 - val_acc: 0.8907\n",
      "Epoch 286/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2640 - acc: 0.9170 - val_loss: 0.3264 - val_acc: 0.8867\n",
      "Epoch 287/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2575 - acc: 0.9170 - val_loss: 0.3330 - val_acc: 0.8887\n",
      "Epoch 288/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2534 - acc: 0.9150 - val_loss: 0.3280 - val_acc: 0.8847\n",
      "Epoch 289/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2619 - acc: 0.9137 - val_loss: 0.3242 - val_acc: 0.8966\n",
      "Epoch 290/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2535 - acc: 0.9157 - val_loss: 0.3268 - val_acc: 0.8867\n",
      "Epoch 291/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2545 - acc: 0.9137 - val_loss: 0.3180 - val_acc: 0.8986\n",
      "Epoch 292/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2533 - acc: 0.9177 - val_loss: 0.3227 - val_acc: 0.9026\n",
      "Epoch 293/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2542 - acc: 0.9137 - val_loss: 0.3257 - val_acc: 0.8907\n",
      "Epoch 294/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2531 - acc: 0.9183 - val_loss: 0.3207 - val_acc: 0.8946\n",
      "Epoch 295/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2504 - acc: 0.9170 - val_loss: 0.3237 - val_acc: 0.8946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2509 - acc: 0.9177 - val_loss: 0.3265 - val_acc: 0.8926\n",
      "Epoch 297/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2539 - acc: 0.9143 - val_loss: 0.3198 - val_acc: 0.8926\n",
      "Epoch 298/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.2519 - acc: 0.9183 - val_loss: 0.3248 - val_acc: 0.8946\n",
      "Epoch 299/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.2844 - acc: 0.9097 - val_loss: 0.3246 - val_acc: 0.8847\n",
      "Epoch 300/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2515 - acc: 0.9190 - val_loss: 0.3177 - val_acc: 0.8946\n",
      "Epoch 301/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2553 - acc: 0.9163 - val_loss: 0.3326 - val_acc: 0.8926\n",
      "Epoch 302/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2529 - acc: 0.9170 - val_loss: 0.3187 - val_acc: 0.8966\n",
      "Epoch 303/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2523 - acc: 0.9183 - val_loss: 0.3297 - val_acc: 0.8887\n",
      "Epoch 304/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2568 - acc: 0.9143 - val_loss: 0.3276 - val_acc: 0.8867\n",
      "Epoch 305/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2478 - acc: 0.9183 - val_loss: 0.3212 - val_acc: 0.8887\n",
      "Epoch 306/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2582 - acc: 0.9090 - val_loss: 0.3222 - val_acc: 0.8867\n",
      "Epoch 307/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2529 - acc: 0.9137 - val_loss: 0.3204 - val_acc: 0.8986\n",
      "Epoch 308/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.2568 - acc: 0.9157 - val_loss: 0.3392 - val_acc: 0.8847\n",
      "Epoch 309/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2520 - acc: 0.9170 - val_loss: 0.3301 - val_acc: 0.8926\n",
      "Epoch 310/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.2507 - acc: 0.9177 - val_loss: 0.3127 - val_acc: 0.9006\n",
      "Epoch 311/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2654 - acc: 0.9117 - val_loss: 0.3135 - val_acc: 0.8946\n",
      "Epoch 312/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2469 - acc: 0.9210 - val_loss: 0.3131 - val_acc: 0.8926\n",
      "Epoch 313/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2470 - acc: 0.9190 - val_loss: 0.3264 - val_acc: 0.8986\n",
      "Epoch 314/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2537 - acc: 0.9163 - val_loss: 0.3135 - val_acc: 0.8986\n",
      "Epoch 315/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2443 - acc: 0.9183 - val_loss: 0.3135 - val_acc: 0.9006\n",
      "Epoch 316/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2475 - acc: 0.9197 - val_loss: 0.3148 - val_acc: 0.8946\n",
      "Epoch 317/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2493 - acc: 0.9170 - val_loss: 0.3290 - val_acc: 0.8887\n",
      "Epoch 318/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.2714 - acc: 0.9143 - val_loss: 0.3193 - val_acc: 0.8827\n",
      "Epoch 319/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2474 - acc: 0.9163 - val_loss: 0.3097 - val_acc: 0.8966\n",
      "Epoch 320/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2457 - acc: 0.9190 - val_loss: 0.3146 - val_acc: 0.8907\n",
      "Epoch 321/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2460 - acc: 0.9170 - val_loss: 0.3079 - val_acc: 0.9006\n",
      "Epoch 322/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2441 - acc: 0.9223 - val_loss: 0.3111 - val_acc: 0.8926\n",
      "Epoch 323/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2456 - acc: 0.9177 - val_loss: 0.3178 - val_acc: 0.8986\n",
      "Epoch 324/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2437 - acc: 0.9177 - val_loss: 0.3143 - val_acc: 0.9006\n",
      "Epoch 325/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2449 - acc: 0.9203 - val_loss: 0.3223 - val_acc: 0.8887\n",
      "Epoch 326/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2461 - acc: 0.9197 - val_loss: 0.3101 - val_acc: 0.9006\n",
      "Epoch 327/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2426 - acc: 0.9197 - val_loss: 0.3135 - val_acc: 0.8946\n",
      "Epoch 328/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.2422 - acc: 0.9203 - val_loss: 0.3236 - val_acc: 0.8887\n",
      "Epoch 329/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2459 - acc: 0.9177 - val_loss: 0.3242 - val_acc: 0.8867\n",
      "Epoch 330/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2410 - acc: 0.9203 - val_loss: 0.3201 - val_acc: 0.8907\n",
      "Epoch 331/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2419 - acc: 0.9170 - val_loss: 0.3057 - val_acc: 0.8946\n",
      "Epoch 332/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2414 - acc: 0.9170 - val_loss: 0.3122 - val_acc: 0.9006\n",
      "Epoch 333/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2394 - acc: 0.9197 - val_loss: 0.3324 - val_acc: 0.8867\n",
      "Epoch 334/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2438 - acc: 0.9183 - val_loss: 0.3069 - val_acc: 0.8926\n",
      "Epoch 335/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2446 - acc: 0.9190 - val_loss: 0.3000 - val_acc: 0.9046\n",
      "Epoch 336/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2641 - acc: 0.9183 - val_loss: 0.3065 - val_acc: 0.9006\n",
      "Epoch 337/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2454 - acc: 0.9230 - val_loss: 0.3061 - val_acc: 0.8986\n",
      "Epoch 338/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2531 - acc: 0.9183 - val_loss: 0.3154 - val_acc: 0.8946\n",
      "Epoch 339/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2390 - acc: 0.9216 - val_loss: 0.3071 - val_acc: 0.8966\n",
      "Epoch 340/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2392 - acc: 0.9236 - val_loss: 0.3175 - val_acc: 0.8887\n",
      "Epoch 341/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2403 - acc: 0.9177 - val_loss: 0.3081 - val_acc: 0.8966\n",
      "Epoch 342/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2437 - acc: 0.9236 - val_loss: 0.2997 - val_acc: 0.9046\n",
      "Epoch 343/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2411 - acc: 0.9183 - val_loss: 0.3116 - val_acc: 0.8966\n",
      "Epoch 344/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2383 - acc: 0.9230 - val_loss: 0.2966 - val_acc: 0.9046\n",
      "Epoch 345/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2423 - acc: 0.9203 - val_loss: 0.3163 - val_acc: 0.8946\n",
      "Epoch 346/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2418 - acc: 0.9190 - val_loss: 0.3051 - val_acc: 0.9006\n",
      "Epoch 347/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2397 - acc: 0.9216 - val_loss: 0.2977 - val_acc: 0.9006\n",
      "Epoch 348/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2469 - acc: 0.9230 - val_loss: 0.3096 - val_acc: 0.8986\n",
      "Epoch 349/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2439 - acc: 0.9223 - val_loss: 0.3029 - val_acc: 0.8966\n",
      "Epoch 350/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2388 - acc: 0.9197 - val_loss: 0.3047 - val_acc: 0.8907\n",
      "Epoch 351/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2358 - acc: 0.9197 - val_loss: 0.3054 - val_acc: 0.8986\n",
      "Epoch 352/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2361 - acc: 0.9197 - val_loss: 0.3014 - val_acc: 0.9006\n",
      "Epoch 353/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2458 - acc: 0.9197 - val_loss: 0.3405 - val_acc: 0.8847\n",
      "Epoch 354/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2405 - acc: 0.9197 - val_loss: 0.3023 - val_acc: 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2367 - acc: 0.9203 - val_loss: 0.3168 - val_acc: 0.8887\n",
      "Epoch 356/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.2400 - acc: 0.9230 - val_loss: 0.3062 - val_acc: 0.8907\n",
      "Epoch 357/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2412 - acc: 0.9203 - val_loss: 0.3036 - val_acc: 0.8887\n",
      "Epoch 358/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.2433 - acc: 0.9157 - val_loss: 0.2962 - val_acc: 0.9085\n",
      "Epoch 359/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.2354 - acc: 0.9216 - val_loss: 0.3009 - val_acc: 0.8946\n",
      "Epoch 360/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2336 - acc: 0.9197 - val_loss: 0.3013 - val_acc: 0.9046\n",
      "Epoch 361/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.2347 - acc: 0.9216 - val_loss: 0.3052 - val_acc: 0.8946\n",
      "Epoch 362/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2351 - acc: 0.9236 - val_loss: 0.3045 - val_acc: 0.9066\n",
      "Epoch 363/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.2352 - acc: 0.9216 - val_loss: 0.2953 - val_acc: 0.9145\n",
      "Epoch 364/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2353 - acc: 0.9230 - val_loss: 0.2937 - val_acc: 0.9145\n",
      "Epoch 365/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2439 - acc: 0.9263 - val_loss: 0.3103 - val_acc: 0.8946\n",
      "Epoch 366/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2342 - acc: 0.9223 - val_loss: 0.2969 - val_acc: 0.9085\n",
      "Epoch 367/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2341 - acc: 0.9190 - val_loss: 0.2997 - val_acc: 0.9026\n",
      "Epoch 368/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2310 - acc: 0.9230 - val_loss: 0.3062 - val_acc: 0.8986\n",
      "Epoch 369/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2340 - acc: 0.9190 - val_loss: 0.3111 - val_acc: 0.8946\n",
      "Epoch 370/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2331 - acc: 0.9190 - val_loss: 0.3003 - val_acc: 0.9185\n",
      "Epoch 371/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2367 - acc: 0.9197 - val_loss: 0.2895 - val_acc: 0.9185\n",
      "Epoch 372/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2341 - acc: 0.9243 - val_loss: 0.2948 - val_acc: 0.9145\n",
      "Epoch 373/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2318 - acc: 0.9256 - val_loss: 0.2973 - val_acc: 0.8926\n",
      "Epoch 374/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2322 - acc: 0.9197 - val_loss: 0.3155 - val_acc: 0.8926\n",
      "Epoch 375/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2349 - acc: 0.9230 - val_loss: 0.2894 - val_acc: 0.9105\n",
      "Epoch 376/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2339 - acc: 0.9276 - val_loss: 0.2965 - val_acc: 0.9225\n",
      "Epoch 377/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2284 - acc: 0.9230 - val_loss: 0.3104 - val_acc: 0.9046\n",
      "Epoch 378/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2468 - acc: 0.9124 - val_loss: 0.2981 - val_acc: 0.8966\n",
      "Epoch 379/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2281 - acc: 0.9216 - val_loss: 0.3024 - val_acc: 0.8926\n",
      "Epoch 380/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2308 - acc: 0.9230 - val_loss: 0.2901 - val_acc: 0.9066\n",
      "Epoch 381/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2281 - acc: 0.9223 - val_loss: 0.3000 - val_acc: 0.8946\n",
      "Epoch 382/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2308 - acc: 0.9216 - val_loss: 0.2993 - val_acc: 0.9085\n",
      "Epoch 383/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2312 - acc: 0.9210 - val_loss: 0.2890 - val_acc: 0.9046\n",
      "Epoch 384/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2308 - acc: 0.9223 - val_loss: 0.2889 - val_acc: 0.9145\n",
      "Epoch 385/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2284 - acc: 0.9230 - val_loss: 0.3040 - val_acc: 0.8966\n",
      "Epoch 386/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2349 - acc: 0.9197 - val_loss: 0.2877 - val_acc: 0.9085\n",
      "Epoch 387/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2329 - acc: 0.9243 - val_loss: 0.3076 - val_acc: 0.8966\n",
      "Epoch 388/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2292 - acc: 0.9250 - val_loss: 0.2967 - val_acc: 0.8966\n",
      "Epoch 389/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2276 - acc: 0.9243 - val_loss: 0.2956 - val_acc: 0.8966\n",
      "Epoch 390/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2262 - acc: 0.9250 - val_loss: 0.2959 - val_acc: 0.9026\n",
      "Epoch 391/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2317 - acc: 0.9223 - val_loss: 0.3007 - val_acc: 0.8966\n",
      "Epoch 392/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2390 - acc: 0.9190 - val_loss: 0.3003 - val_acc: 0.9046\n",
      "Epoch 393/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2335 - acc: 0.9243 - val_loss: 0.2850 - val_acc: 0.9165\n",
      "Epoch 394/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2279 - acc: 0.9236 - val_loss: 0.2870 - val_acc: 0.9085\n",
      "Epoch 395/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2265 - acc: 0.9223 - val_loss: 0.2909 - val_acc: 0.9066\n",
      "Epoch 396/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2268 - acc: 0.9236 - val_loss: 0.2892 - val_acc: 0.9046\n",
      "Epoch 397/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2243 - acc: 0.9223 - val_loss: 0.2866 - val_acc: 0.9046\n",
      "Epoch 398/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2254 - acc: 0.9243 - val_loss: 0.3027 - val_acc: 0.8986\n",
      "Epoch 399/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2280 - acc: 0.9203 - val_loss: 0.3160 - val_acc: 0.9145\n",
      "Epoch 400/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2307 - acc: 0.9197 - val_loss: 0.2871 - val_acc: 0.9105\n",
      "Epoch 401/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2262 - acc: 0.9250 - val_loss: 0.2847 - val_acc: 0.9125\n",
      "Epoch 402/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2261 - acc: 0.9236 - val_loss: 0.2823 - val_acc: 0.9185\n",
      "Epoch 403/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2280 - acc: 0.9223 - val_loss: 0.2852 - val_acc: 0.9066\n",
      "Epoch 404/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2431 - acc: 0.9197 - val_loss: 0.2834 - val_acc: 0.9125\n",
      "Epoch 405/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2278 - acc: 0.9210 - val_loss: 0.2887 - val_acc: 0.9125\n",
      "Epoch 406/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2271 - acc: 0.9223 - val_loss: 0.2955 - val_acc: 0.8966\n",
      "Epoch 407/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.2504 - acc: 0.919 - 0s 61us/step - loss: 0.2252 - acc: 0.9263 - val_loss: 0.2846 - val_acc: 0.9026\n",
      "Epoch 408/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2213 - acc: 0.9256 - val_loss: 0.2847 - val_acc: 0.9046\n",
      "Epoch 409/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2248 - acc: 0.9210 - val_loss: 0.3097 - val_acc: 0.8946\n",
      "Epoch 410/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2349 - acc: 0.9210 - val_loss: 0.2830 - val_acc: 0.9125\n",
      "Epoch 411/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2213 - acc: 0.9243 - val_loss: 0.2965 - val_acc: 0.8986\n",
      "Epoch 412/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2229 - acc: 0.9250 - val_loss: 0.2832 - val_acc: 0.9085\n",
      "Epoch 413/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2254 - acc: 0.9243 - val_loss: 0.2916 - val_acc: 0.9225\n",
      "Epoch 414/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2222 - acc: 0.9256 - val_loss: 0.2871 - val_acc: 0.9225\n",
      "Epoch 415/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.2233 - acc: 0.9290 - val_loss: 0.3072 - val_acc: 0.8887\n",
      "Epoch 416/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2280 - acc: 0.9230 - val_loss: 0.2803 - val_acc: 0.9145\n",
      "Epoch 417/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.2190 - acc: 0.9250 - val_loss: 0.2862 - val_acc: 0.9046\n",
      "Epoch 418/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.2255 - acc: 0.9236 - val_loss: 0.3083 - val_acc: 0.8887\n",
      "Epoch 419/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2295 - acc: 0.9210 - val_loss: 0.2856 - val_acc: 0.9105\n",
      "Epoch 420/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2221 - acc: 0.9230 - val_loss: 0.2832 - val_acc: 0.8986\n",
      "Epoch 421/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.2194 - acc: 0.9256 - val_loss: 0.2994 - val_acc: 0.8926\n",
      "Epoch 422/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2350 - acc: 0.9210 - val_loss: 0.2856 - val_acc: 0.9046\n",
      "Epoch 423/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2201 - acc: 0.9243 - val_loss: 0.2891 - val_acc: 0.9026\n",
      "Epoch 424/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2238 - acc: 0.9256 - val_loss: 0.2866 - val_acc: 0.9046\n",
      "Epoch 425/3000\n",
      "1506/1506 [==============================] - 0s 47us/step - loss: 0.2183 - acc: 0.9263 - val_loss: 0.2887 - val_acc: 0.8946\n",
      "Epoch 426/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2216 - acc: 0.9236 - val_loss: 0.2856 - val_acc: 0.9105\n",
      "Epoch 427/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2187 - acc: 0.9263 - val_loss: 0.2857 - val_acc: 0.8926\n",
      "Epoch 428/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2182 - acc: 0.9256 - val_loss: 0.2832 - val_acc: 0.8986\n",
      "Epoch 429/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2189 - acc: 0.9250 - val_loss: 0.2862 - val_acc: 0.8986\n",
      "Epoch 430/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2254 - acc: 0.9303 - val_loss: 0.2831 - val_acc: 0.9145\n",
      "Epoch 431/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.2228 - acc: 0.9309 - val_loss: 0.2798 - val_acc: 0.9026\n",
      "Epoch 432/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2269 - acc: 0.9250 - val_loss: 0.2940 - val_acc: 0.9026\n",
      "Epoch 433/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2213 - acc: 0.9223 - val_loss: 0.2753 - val_acc: 0.9165\n",
      "Epoch 434/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2185 - acc: 0.9296 - val_loss: 0.2728 - val_acc: 0.9125\n",
      "Epoch 435/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2181 - acc: 0.9263 - val_loss: 0.2782 - val_acc: 0.9066\n",
      "Epoch 436/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2212 - acc: 0.9263 - val_loss: 0.2766 - val_acc: 0.9006\n",
      "Epoch 437/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2172 - acc: 0.9283 - val_loss: 0.2857 - val_acc: 0.8966\n",
      "Epoch 438/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2175 - acc: 0.9250 - val_loss: 0.2844 - val_acc: 0.9105\n",
      "Epoch 439/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2164 - acc: 0.9270 - val_loss: 0.2858 - val_acc: 0.8986\n",
      "Epoch 440/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2142 - acc: 0.9290 - val_loss: 0.2703 - val_acc: 0.9085\n",
      "Epoch 441/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2158 - acc: 0.9263 - val_loss: 0.2756 - val_acc: 0.9066\n",
      "Epoch 442/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2147 - acc: 0.9263 - val_loss: 0.2782 - val_acc: 0.9046\n",
      "Epoch 443/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2202 - acc: 0.9276 - val_loss: 0.2757 - val_acc: 0.9125\n",
      "Epoch 444/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2172 - acc: 0.9316 - val_loss: 0.2810 - val_acc: 0.9066\n",
      "Epoch 445/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2178 - acc: 0.9243 - val_loss: 0.2800 - val_acc: 0.9284\n",
      "Epoch 446/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2393 - acc: 0.9223 - val_loss: 0.2787 - val_acc: 0.9085\n",
      "Epoch 447/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2163 - acc: 0.9303 - val_loss: 0.2753 - val_acc: 0.9085\n",
      "Epoch 448/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2207 - acc: 0.9296 - val_loss: 0.2724 - val_acc: 0.9165\n",
      "Epoch 449/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2190 - acc: 0.9276 - val_loss: 0.2886 - val_acc: 0.8986\n",
      "Epoch 450/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2144 - acc: 0.9283 - val_loss: 0.2763 - val_acc: 0.9105\n",
      "Epoch 451/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2181 - acc: 0.9270 - val_loss: 0.2825 - val_acc: 0.9145\n",
      "Epoch 452/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2133 - acc: 0.9296 - val_loss: 0.2841 - val_acc: 0.8986\n",
      "Epoch 453/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2128 - acc: 0.9356 - val_loss: 0.2859 - val_acc: 0.9085\n",
      "Epoch 454/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2135 - acc: 0.9256 - val_loss: 0.2813 - val_acc: 0.8966\n",
      "Epoch 455/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2139 - acc: 0.9303 - val_loss: 0.2785 - val_acc: 0.9046\n",
      "Epoch 456/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2114 - acc: 0.9276 - val_loss: 0.2735 - val_acc: 0.9105\n",
      "Epoch 457/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2148 - acc: 0.9256 - val_loss: 0.2740 - val_acc: 0.9046\n",
      "Epoch 458/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2132 - acc: 0.9256 - val_loss: 0.2776 - val_acc: 0.9085\n",
      "Epoch 459/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.2106 - acc: 0.9283 - val_loss: 0.2739 - val_acc: 0.9205\n",
      "Epoch 460/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2132 - acc: 0.9276 - val_loss: 0.2875 - val_acc: 0.9085\n",
      "Epoch 00460: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adam,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist1=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2634b4ae748>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26349df9b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFJRSSnn8P6XNvFLEIWrDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634b4df9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist1.history['loss'])\n",
    "plt.plot(hist1.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'], prop={'size': 15})\n",
    "#plt.savefig('loss with adam.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('loss with adam.eps', format='eps', dpi=1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (p.u)')\n",
    "plt.plot(hist1.history['acc'])\n",
    "plt.plot(hist1.history['val_acc'])\n",
    "#plt.savefig('accuracy with adam.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('accuracy with adam.eps', format='eps', dpi=1000)\n",
    "plt.legend(['Training', 'Validation'], loc='lower right', prop={'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/1500\n",
      "1506/1506 [==============================] - 1s 439us/step - loss: 2.2871 - acc: 0.1746 - val_loss: 2.2633 - val_acc: 0.2048\n",
      "Epoch 2/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 2.2535 - acc: 0.2151 - val_loss: 2.2207 - val_acc: 0.2366\n",
      "Epoch 3/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 2.2194 - acc: 0.2151 - val_loss: 2.1816 - val_acc: 0.2366\n",
      "Epoch 4/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 2.1933 - acc: 0.2151 - val_loss: 2.1547 - val_acc: 0.2366\n",
      "Epoch 5/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 2.1751 - acc: 0.2151 - val_loss: 2.1335 - val_acc: 0.2366\n",
      "Epoch 6/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 2.1623 - acc: 0.2151 - val_loss: 2.1209 - val_acc: 0.2366\n",
      "Epoch 7/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 2.1459 - acc: 0.2151 - val_loss: 2.0987 - val_acc: 0.2366\n",
      "Epoch 8/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 2.1206 - acc: 0.2331 - val_loss: 2.0667 - val_acc: 0.2584\n",
      "Epoch 9/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 2.0814 - acc: 0.2988 - val_loss: 2.0209 - val_acc: 0.4334\n",
      "Epoch 10/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 2.0269 - acc: 0.3938 - val_loss: 1.9625 - val_acc: 0.4414\n",
      "Epoch 11/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.9596 - acc: 0.3958 - val_loss: 1.8947 - val_acc: 0.4414\n",
      "Epoch 12/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.8883 - acc: 0.3958 - val_loss: 1.8295 - val_acc: 0.4414\n",
      "Epoch 13/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.8233 - acc: 0.3958 - val_loss: 1.7721 - val_acc: 0.4414\n",
      "Epoch 14/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 1.7674 - acc: 0.3958 - val_loss: 1.7246 - val_acc: 0.4414\n",
      "Epoch 15/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.7215 - acc: 0.3958 - val_loss: 1.6874 - val_acc: 0.4414\n",
      "Epoch 16/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 1.6849 - acc: 0.3958 - val_loss: 1.6585 - val_acc: 0.4414\n",
      "Epoch 17/1500\n",
      "1506/1506 [==============================] - 0s 43us/step - loss: 1.6551 - acc: 0.3958 - val_loss: 1.6362 - val_acc: 0.4414\n",
      "Epoch 18/1500\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 1.6298 - acc: 0.3958 - val_loss: 1.6154 - val_acc: 0.4414\n",
      "Epoch 19/1500\n",
      "1506/1506 [==============================] - 0s 44us/step - loss: 1.6079 - acc: 0.3958 - val_loss: 1.5986 - val_acc: 0.4414\n",
      "Epoch 20/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.5888 - acc: 0.3958 - val_loss: 1.5833 - val_acc: 0.4414\n",
      "Epoch 21/1500\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 1.5715 - acc: 0.4004 - val_loss: 1.5693 - val_acc: 0.4453\n",
      "Epoch 22/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.5559 - acc: 0.4024 - val_loss: 1.5541 - val_acc: 0.4453\n",
      "Epoch 23/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 1.5423 - acc: 0.4070 - val_loss: 1.5411 - val_acc: 0.4513\n",
      "Epoch 24/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.5284 - acc: 0.4137 - val_loss: 1.5287 - val_acc: 0.4592\n",
      "Epoch 25/1500\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 1.5144 - acc: 0.4250 - val_loss: 1.5192 - val_acc: 0.4652\n",
      "Epoch 26/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 1.5019 - acc: 0.4290 - val_loss: 1.5058 - val_acc: 0.4672\n",
      "Epoch 27/1500\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 1.4907 - acc: 0.4276 - val_loss: 1.4946 - val_acc: 0.4692\n",
      "Epoch 28/1500\n",
      "1506/1506 [==============================] - 0s 49us/step - loss: 1.4791 - acc: 0.4349 - val_loss: 1.4844 - val_acc: 0.4712\n",
      "Epoch 29/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.4676 - acc: 0.4402 - val_loss: 1.4748 - val_acc: 0.4771\n",
      "Epoch 30/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.4562 - acc: 0.4416 - val_loss: 1.4636 - val_acc: 0.4791\n",
      "Epoch 31/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.4453 - acc: 0.4429 - val_loss: 1.4540 - val_acc: 0.4831\n",
      "Epoch 32/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.4348 - acc: 0.4515 - val_loss: 1.4470 - val_acc: 0.4831\n",
      "Epoch 33/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 1.4242 - acc: 0.4542 - val_loss: 1.4371 - val_acc: 0.4851\n",
      "Epoch 34/1500\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 1.4150 - acc: 0.4535 - val_loss: 1.4302 - val_acc: 0.4950\n",
      "Epoch 35/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 1.4056 - acc: 0.4641 - val_loss: 1.4201 - val_acc: 0.4930\n",
      "Epoch 36/1500\n",
      "1506/1506 [==============================] - 0s 47us/step - loss: 1.3955 - acc: 0.4608 - val_loss: 1.4112 - val_acc: 0.4970\n",
      "Epoch 37/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.3875 - acc: 0.4542 - val_loss: 1.4012 - val_acc: 0.4930\n",
      "Epoch 38/1500\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 1.3772 - acc: 0.4721 - val_loss: 1.3948 - val_acc: 0.4990\n",
      "Epoch 39/1500\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 1.3696 - acc: 0.4734 - val_loss: 1.3848 - val_acc: 0.5030\n",
      "Epoch 40/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.3602 - acc: 0.4728 - val_loss: 1.3756 - val_acc: 0.5149\n",
      "Epoch 41/1500\n",
      "1506/1506 [==============================] - 0s 44us/step - loss: 1.3512 - acc: 0.4854 - val_loss: 1.3679 - val_acc: 0.5169\n",
      "Epoch 42/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.3433 - acc: 0.4788 - val_loss: 1.3596 - val_acc: 0.5169\n",
      "Epoch 43/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 1.3349 - acc: 0.5027 - val_loss: 1.3530 - val_acc: 0.5229\n",
      "Epoch 44/1500\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 1.3261 - acc: 0.5046 - val_loss: 1.3443 - val_acc: 0.5288\n",
      "Epoch 45/1500\n",
      "1506/1506 [==============================] - 0s 49us/step - loss: 1.3174 - acc: 0.5060 - val_loss: 1.3353 - val_acc: 0.5249\n",
      "Epoch 46/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.3093 - acc: 0.5046 - val_loss: 1.3276 - val_acc: 0.5229\n",
      "Epoch 47/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.3012 - acc: 0.5046 - val_loss: 1.3206 - val_acc: 0.5388\n",
      "Epoch 48/1500\n",
      "1506/1506 [==============================] - 0s 41us/step - loss: 1.2938 - acc: 0.5272 - val_loss: 1.3110 - val_acc: 0.5368\n",
      "Epoch 49/1500\n",
      "1506/1506 [==============================] - 0s 49us/step - loss: 1.2856 - acc: 0.5292 - val_loss: 1.3076 - val_acc: 0.5547\n",
      "Epoch 50/1500\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 1.2777 - acc: 0.5498 - val_loss: 1.2963 - val_acc: 0.5467\n",
      "Epoch 51/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 1.2694 - acc: 0.5345 - val_loss: 1.2900 - val_acc: 0.5606\n",
      "Epoch 52/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 1.2625 - acc: 0.5505 - val_loss: 1.2807 - val_acc: 0.5567\n",
      "Epoch 53/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.2540 - acc: 0.5551 - val_loss: 1.2760 - val_acc: 0.5865\n",
      "Epoch 54/1500\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 1.2454 - acc: 0.5677 - val_loss: 1.2661 - val_acc: 0.5706\n",
      "Epoch 55/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.2383 - acc: 0.5618 - val_loss: 1.2616 - val_acc: 0.6004\n",
      "Epoch 56/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 1.2311 - acc: 0.5697 - val_loss: 1.2530 - val_acc: 0.5905\n",
      "Epoch 57/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 1.2222 - acc: 0.5664 - val_loss: 1.2463 - val_acc: 0.5984\n",
      "Epoch 58/1500\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 1.2142 - acc: 0.5710 - val_loss: 1.2401 - val_acc: 0.5964\n",
      "Epoch 59/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 1.2064 - acc: 0.5983 - val_loss: 1.2292 - val_acc: 0.5944\n",
      "Epoch 60/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 58us/step - loss: 1.1985 - acc: 0.5691 - val_loss: 1.2239 - val_acc: 0.6183\n",
      "Epoch 61/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 1.1902 - acc: 0.6282 - val_loss: 1.2156 - val_acc: 0.6103\n",
      "Epoch 62/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 1.1820 - acc: 0.6102 - val_loss: 1.2050 - val_acc: 0.6064\n",
      "Epoch 63/1500\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 1.1740 - acc: 0.6082 - val_loss: 1.1977 - val_acc: 0.6203\n",
      "Epoch 64/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 1.1655 - acc: 0.6169 - val_loss: 1.1891 - val_acc: 0.6143\n",
      "Epoch 65/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 1.1576 - acc: 0.6248 - val_loss: 1.1826 - val_acc: 0.6223\n",
      "Epoch 66/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 1.1500 - acc: 0.6242 - val_loss: 1.1755 - val_acc: 0.6203\n",
      "Epoch 67/1500\n",
      "1506/1506 [==============================] - 0s 94us/step - loss: 1.1417 - acc: 0.6301 - val_loss: 1.1673 - val_acc: 0.6183\n",
      "Epoch 68/1500\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 1.1331 - acc: 0.6514 - val_loss: 1.1564 - val_acc: 0.6203\n",
      "Epoch 69/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.1255 - acc: 0.6301 - val_loss: 1.1511 - val_acc: 0.6203\n",
      "Epoch 70/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.1175 - acc: 0.6408 - val_loss: 1.1422 - val_acc: 0.6322\n",
      "Epoch 71/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.1074 - acc: 0.6454 - val_loss: 1.1317 - val_acc: 0.6402\n",
      "Epoch 72/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 1.0982 - acc: 0.6527 - val_loss: 1.1227 - val_acc: 0.6322\n",
      "Epoch 73/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.0897 - acc: 0.6547 - val_loss: 1.1156 - val_acc: 0.6402\n",
      "Epoch 74/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 1.0813 - acc: 0.6653 - val_loss: 1.1058 - val_acc: 0.6421\n",
      "Epoch 75/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 1.0732 - acc: 0.6527 - val_loss: 1.0973 - val_acc: 0.6501\n",
      "Epoch 76/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 1.0658 - acc: 0.6507 - val_loss: 1.0909 - val_acc: 0.6421\n",
      "Epoch 77/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 1.0569 - acc: 0.6633 - val_loss: 1.0836 - val_acc: 0.6441\n",
      "Epoch 78/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.0500 - acc: 0.6640 - val_loss: 1.0740 - val_acc: 0.6521\n",
      "Epoch 79/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.0406 - acc: 0.6673 - val_loss: 1.0656 - val_acc: 0.6461\n",
      "Epoch 80/1500\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 1.0342 - acc: 0.6587 - val_loss: 1.0603 - val_acc: 0.6481\n",
      "Epoch 81/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.0261 - acc: 0.6680 - val_loss: 1.0506 - val_acc: 0.6481\n",
      "Epoch 82/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.0173 - acc: 0.6700 - val_loss: 1.0448 - val_acc: 0.6541\n",
      "Epoch 83/1500\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 1.0108 - acc: 0.6667 - val_loss: 1.0366 - val_acc: 0.6481\n",
      "Epoch 84/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 1.0027 - acc: 0.6740 - val_loss: 1.0297 - val_acc: 0.6561\n",
      "Epoch 85/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.9974 - acc: 0.6799 - val_loss: 1.0219 - val_acc: 0.6680\n",
      "Epoch 86/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.9900 - acc: 0.6673 - val_loss: 1.0134 - val_acc: 0.6600\n",
      "Epoch 87/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.9826 - acc: 0.6859 - val_loss: 1.0067 - val_acc: 0.6700\n",
      "Epoch 88/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.9756 - acc: 0.6793 - val_loss: 1.0011 - val_acc: 0.6740\n",
      "Epoch 89/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.9695 - acc: 0.6813 - val_loss: 0.9953 - val_acc: 0.6720\n",
      "Epoch 90/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.9638 - acc: 0.6912 - val_loss: 0.9890 - val_acc: 0.6799\n",
      "Epoch 91/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.9568 - acc: 0.6912 - val_loss: 0.9817 - val_acc: 0.6759\n",
      "Epoch 92/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.9501 - acc: 0.6892 - val_loss: 0.9740 - val_acc: 0.6899\n",
      "Epoch 93/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9446 - acc: 0.6992 - val_loss: 0.9699 - val_acc: 0.6839\n",
      "Epoch 94/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.9395 - acc: 0.6926 - val_loss: 0.9665 - val_acc: 0.6839\n",
      "Epoch 95/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9328 - acc: 0.6979 - val_loss: 0.9601 - val_acc: 0.6799\n",
      "Epoch 96/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9268 - acc: 0.7045 - val_loss: 0.9522 - val_acc: 0.6859\n",
      "Epoch 97/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9236 - acc: 0.7005 - val_loss: 0.9457 - val_acc: 0.6918\n",
      "Epoch 98/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.9154 - acc: 0.7138 - val_loss: 0.9385 - val_acc: 0.6958\n",
      "Epoch 99/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.9104 - acc: 0.7105 - val_loss: 0.9341 - val_acc: 0.6938\n",
      "Epoch 100/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.9050 - acc: 0.7145 - val_loss: 0.9283 - val_acc: 0.7018\n",
      "Epoch 101/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8988 - acc: 0.7158 - val_loss: 0.9227 - val_acc: 0.7078\n",
      "Epoch 102/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.8931 - acc: 0.7311 - val_loss: 0.9154 - val_acc: 0.6998\n",
      "Epoch 103/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.8875 - acc: 0.7224 - val_loss: 0.9122 - val_acc: 0.7336\n",
      "Epoch 104/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.8810 - acc: 0.7311 - val_loss: 0.9040 - val_acc: 0.7296\n",
      "Epoch 105/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.8752 - acc: 0.7244 - val_loss: 0.8984 - val_acc: 0.7376\n",
      "Epoch 106/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8690 - acc: 0.7390 - val_loss: 0.8904 - val_acc: 0.7336\n",
      "Epoch 107/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.8637 - acc: 0.7364 - val_loss: 0.8849 - val_acc: 0.7316\n",
      "Epoch 108/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8579 - acc: 0.7450 - val_loss: 0.8806 - val_acc: 0.7515\n",
      "Epoch 109/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8518 - acc: 0.7497 - val_loss: 0.8742 - val_acc: 0.7495\n",
      "Epoch 110/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.8472 - acc: 0.7583 - val_loss: 0.8692 - val_acc: 0.7555\n",
      "Epoch 111/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8400 - acc: 0.7636 - val_loss: 0.8621 - val_acc: 0.7535\n",
      "Epoch 112/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.8349 - acc: 0.7623 - val_loss: 0.8567 - val_acc: 0.7535\n",
      "Epoch 113/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.8305 - acc: 0.7570 - val_loss: 0.8517 - val_acc: 0.7575\n",
      "Epoch 114/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.8239 - acc: 0.7663 - val_loss: 0.8463 - val_acc: 0.7515\n",
      "Epoch 115/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.8206 - acc: 0.7676 - val_loss: 0.8425 - val_acc: 0.7555\n",
      "Epoch 116/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8160 - acc: 0.7629 - val_loss: 0.8376 - val_acc: 0.7555\n",
      "Epoch 117/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.8090 - acc: 0.7689 - val_loss: 0.8301 - val_acc: 0.7575\n",
      "Epoch 118/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.8042 - acc: 0.7669 - val_loss: 0.8257 - val_acc: 0.7594\n",
      "Epoch 119/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7989 - acc: 0.7709 - val_loss: 0.8211 - val_acc: 0.7614\n",
      "Epoch 120/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7955 - acc: 0.7749 - val_loss: 0.8166 - val_acc: 0.7535\n",
      "Epoch 121/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7905 - acc: 0.7716 - val_loss: 0.8111 - val_acc: 0.7594\n",
      "Epoch 122/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.7865 - acc: 0.7722 - val_loss: 0.8067 - val_acc: 0.7614\n",
      "Epoch 123/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.7817 - acc: 0.7762 - val_loss: 0.8017 - val_acc: 0.7634\n",
      "Epoch 124/1500\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.7770 - acc: 0.7802 - val_loss: 0.7989 - val_acc: 0.7694\n",
      "Epoch 125/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.7726 - acc: 0.7795 - val_loss: 0.7949 - val_acc: 0.7634\n",
      "Epoch 126/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7692 - acc: 0.7722 - val_loss: 0.7904 - val_acc: 0.7753\n",
      "Epoch 127/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7641 - acc: 0.7789 - val_loss: 0.7861 - val_acc: 0.7753\n",
      "Epoch 128/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.7607 - acc: 0.7849 - val_loss: 0.7816 - val_acc: 0.7674\n",
      "Epoch 129/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.7562 - acc: 0.7849 - val_loss: 0.7785 - val_acc: 0.7893\n",
      "Epoch 130/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7517 - acc: 0.7842 - val_loss: 0.7733 - val_acc: 0.7753\n",
      "Epoch 131/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7478 - acc: 0.7882 - val_loss: 0.7708 - val_acc: 0.7853\n",
      "Epoch 132/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.7439 - acc: 0.7875 - val_loss: 0.7646 - val_acc: 0.7813\n",
      "Epoch 133/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.7417 - acc: 0.7809 - val_loss: 0.7623 - val_acc: 0.7833\n",
      "Epoch 134/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.7356 - acc: 0.7908 - val_loss: 0.7586 - val_acc: 0.7753\n",
      "Epoch 135/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.7329 - acc: 0.7869 - val_loss: 0.7542 - val_acc: 0.7873\n",
      "Epoch 136/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.7285 - acc: 0.7875 - val_loss: 0.7505 - val_acc: 0.7873\n",
      "Epoch 137/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.7250 - acc: 0.7862 - val_loss: 0.7456 - val_acc: 0.7873\n",
      "Epoch 138/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7221 - acc: 0.7875 - val_loss: 0.7414 - val_acc: 0.7833\n",
      "Epoch 139/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.7175 - acc: 0.7888 - val_loss: 0.7386 - val_acc: 0.7853\n",
      "Epoch 140/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7135 - acc: 0.7948 - val_loss: 0.7353 - val_acc: 0.7893\n",
      "Epoch 141/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7140 - acc: 0.7875 - val_loss: 0.7316 - val_acc: 0.7873\n",
      "Epoch 142/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.7070 - acc: 0.7961 - val_loss: 0.7291 - val_acc: 0.7893\n",
      "Epoch 143/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.7024 - acc: 0.8008 - val_loss: 0.7244 - val_acc: 0.7893\n",
      "Epoch 144/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6985 - acc: 0.7955 - val_loss: 0.7226 - val_acc: 0.7873\n",
      "Epoch 145/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6961 - acc: 0.8074 - val_loss: 0.7174 - val_acc: 0.7873\n",
      "Epoch 146/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6922 - acc: 0.8035 - val_loss: 0.7133 - val_acc: 0.7893\n",
      "Epoch 147/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6886 - acc: 0.8127 - val_loss: 0.7115 - val_acc: 0.7952\n",
      "Epoch 148/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.6848 - acc: 0.8121 - val_loss: 0.7078 - val_acc: 0.7972\n",
      "Epoch 149/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.6814 - acc: 0.8201 - val_loss: 0.7056 - val_acc: 0.8032\n",
      "Epoch 150/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6787 - acc: 0.8220 - val_loss: 0.7030 - val_acc: 0.8111\n",
      "Epoch 151/1500\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.6753 - acc: 0.8240 - val_loss: 0.6991 - val_acc: 0.8111\n",
      "Epoch 152/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.6726 - acc: 0.8280 - val_loss: 0.6956 - val_acc: 0.8131\n",
      "Epoch 153/1500\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.6694 - acc: 0.8254 - val_loss: 0.6932 - val_acc: 0.8072\n",
      "Epoch 154/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.6663 - acc: 0.8254 - val_loss: 0.6899 - val_acc: 0.8052\n",
      "Epoch 155/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.6630 - acc: 0.8254 - val_loss: 0.6868 - val_acc: 0.8052\n",
      "Epoch 156/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.6601 - acc: 0.8307 - val_loss: 0.6843 - val_acc: 0.8032\n",
      "Epoch 157/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.6561 - acc: 0.8293 - val_loss: 0.6814 - val_acc: 0.8091\n",
      "Epoch 158/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.6544 - acc: 0.8274 - val_loss: 0.6803 - val_acc: 0.8151\n",
      "Epoch 159/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.6511 - acc: 0.8293 - val_loss: 0.6760 - val_acc: 0.8231\n",
      "Epoch 160/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6496 - acc: 0.8267 - val_loss: 0.6732 - val_acc: 0.8072\n",
      "Epoch 161/1500\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 0.6455 - acc: 0.8333 - val_loss: 0.6712 - val_acc: 0.8072\n",
      "Epoch 162/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.6431 - acc: 0.8293 - val_loss: 0.6693 - val_acc: 0.8250\n",
      "Epoch 163/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.6409 - acc: 0.8313 - val_loss: 0.6652 - val_acc: 0.8171\n",
      "Epoch 164/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6373 - acc: 0.8307 - val_loss: 0.6646 - val_acc: 0.8171\n",
      "Epoch 165/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.6348 - acc: 0.8347 - val_loss: 0.6614 - val_acc: 0.8171\n",
      "Epoch 166/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6319 - acc: 0.8313 - val_loss: 0.6594 - val_acc: 0.8290\n",
      "Epoch 167/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6299 - acc: 0.8360 - val_loss: 0.6567 - val_acc: 0.8231\n",
      "Epoch 168/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6279 - acc: 0.8347 - val_loss: 0.6545 - val_acc: 0.8231\n",
      "Epoch 169/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6246 - acc: 0.8479 - val_loss: 0.6519 - val_acc: 0.8191\n",
      "Epoch 170/1500\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.6222 - acc: 0.8386 - val_loss: 0.6511 - val_acc: 0.8270\n",
      "Epoch 171/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.6211 - acc: 0.8360 - val_loss: 0.6477 - val_acc: 0.8290\n",
      "Epoch 172/1500\n",
      "1506/1506 [==============================] - 0s 45us/step - loss: 0.6179 - acc: 0.8373 - val_loss: 0.6453 - val_acc: 0.8231\n",
      "Epoch 173/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6151 - acc: 0.8479 - val_loss: 0.6422 - val_acc: 0.8211\n",
      "Epoch 174/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6129 - acc: 0.8380 - val_loss: 0.6406 - val_acc: 0.8290\n",
      "Epoch 175/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6115 - acc: 0.8413 - val_loss: 0.6379 - val_acc: 0.8250\n",
      "Epoch 176/1500\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 0.6085 - acc: 0.8426 - val_loss: 0.6378 - val_acc: 0.8310\n",
      "Epoch 177/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.6072 - acc: 0.8373 - val_loss: 0.6352 - val_acc: 0.8231\n",
      "Epoch 178/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 49us/step - loss: 0.6040 - acc: 0.8426 - val_loss: 0.6343 - val_acc: 0.8191\n",
      "Epoch 179/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.6012 - acc: 0.8446 - val_loss: 0.6312 - val_acc: 0.8310\n",
      "Epoch 180/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.6002 - acc: 0.8413 - val_loss: 0.6294 - val_acc: 0.8250\n",
      "Epoch 181/1500\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 0.5998 - acc: 0.8466 - val_loss: 0.6270 - val_acc: 0.8211\n",
      "Epoch 182/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.5960 - acc: 0.8420 - val_loss: 0.6279 - val_acc: 0.8250\n",
      "Epoch 183/1500\n",
      "1506/1506 [==============================] - 0s 44us/step - loss: 0.5971 - acc: 0.8446 - val_loss: 0.6223 - val_acc: 0.8290\n",
      "Epoch 184/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.5908 - acc: 0.8446 - val_loss: 0.6221 - val_acc: 0.8290\n",
      "Epoch 185/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.5890 - acc: 0.8426 - val_loss: 0.6190 - val_acc: 0.8310\n",
      "Epoch 186/1500\n",
      "1506/1506 [==============================] - 0s 46us/step - loss: 0.5878 - acc: 0.8386 - val_loss: 0.6173 - val_acc: 0.8270\n",
      "Epoch 187/1500\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.5848 - acc: 0.8440 - val_loss: 0.6141 - val_acc: 0.8310\n",
      "Epoch 188/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.5842 - acc: 0.8426 - val_loss: 0.6159 - val_acc: 0.8310\n",
      "Epoch 189/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.5814 - acc: 0.8413 - val_loss: 0.6127 - val_acc: 0.8330\n",
      "Epoch 190/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.5791 - acc: 0.8400 - val_loss: 0.6119 - val_acc: 0.8290\n",
      "Epoch 191/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.5780 - acc: 0.8459 - val_loss: 0.6095 - val_acc: 0.8290\n",
      "Epoch 192/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.5756 - acc: 0.8393 - val_loss: 0.6085 - val_acc: 0.8330\n",
      "Epoch 193/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.5729 - acc: 0.8420 - val_loss: 0.6063 - val_acc: 0.8290\n",
      "Epoch 194/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.5709 - acc: 0.8453 - val_loss: 0.6042 - val_acc: 0.8330\n",
      "Epoch 195/1500\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.5704 - acc: 0.8446 - val_loss: 0.6037 - val_acc: 0.8330\n",
      "Epoch 196/1500\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.5677 - acc: 0.8466 - val_loss: 0.6015 - val_acc: 0.8330\n",
      "Epoch 197/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5664 - acc: 0.8506 - val_loss: 0.6005 - val_acc: 0.8290\n",
      "Epoch 198/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.5663 - acc: 0.8446 - val_loss: 0.5974 - val_acc: 0.8270\n",
      "Epoch 199/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5622 - acc: 0.8440 - val_loss: 0.5961 - val_acc: 0.8390\n",
      "Epoch 200/1500\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.5599 - acc: 0.8533 - val_loss: 0.5936 - val_acc: 0.8290\n",
      "Epoch 201/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.5592 - acc: 0.8513 - val_loss: 0.5922 - val_acc: 0.8290\n",
      "Epoch 202/1500\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 0.5566 - acc: 0.8486 - val_loss: 0.5897 - val_acc: 0.8390\n",
      "Epoch 203/1500\n",
      "1506/1506 [==============================] - 0s 97us/step - loss: 0.5557 - acc: 0.8493 - val_loss: 0.5882 - val_acc: 0.8370\n",
      "Epoch 204/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5530 - acc: 0.8479 - val_loss: 0.5873 - val_acc: 0.8290\n",
      "Epoch 205/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5510 - acc: 0.8453 - val_loss: 0.5854 - val_acc: 0.8330\n",
      "Epoch 206/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5491 - acc: 0.8473 - val_loss: 0.5837 - val_acc: 0.8410\n",
      "Epoch 207/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5481 - acc: 0.8546 - val_loss: 0.5817 - val_acc: 0.8330\n",
      "Epoch 208/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5455 - acc: 0.8486 - val_loss: 0.5812 - val_acc: 0.8330\n",
      "Epoch 209/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5431 - acc: 0.8526 - val_loss: 0.5787 - val_acc: 0.8350\n",
      "Epoch 210/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5409 - acc: 0.8552 - val_loss: 0.5761 - val_acc: 0.8449\n",
      "Epoch 211/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5397 - acc: 0.8546 - val_loss: 0.5738 - val_acc: 0.8370\n",
      "Epoch 212/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5363 - acc: 0.8539 - val_loss: 0.5718 - val_acc: 0.8429\n",
      "Epoch 213/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5343 - acc: 0.8546 - val_loss: 0.5705 - val_acc: 0.8449\n",
      "Epoch 214/1500\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.5382 - acc: 0.8513 - val_loss: 0.5675 - val_acc: 0.8390\n",
      "Epoch 215/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5316 - acc: 0.8546 - val_loss: 0.5660 - val_acc: 0.8390\n",
      "Epoch 216/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5293 - acc: 0.8592 - val_loss: 0.5642 - val_acc: 0.8429\n",
      "Epoch 217/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5267 - acc: 0.8579 - val_loss: 0.5625 - val_acc: 0.8350\n",
      "Epoch 218/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5260 - acc: 0.8533 - val_loss: 0.5624 - val_acc: 0.8231\n",
      "Epoch 219/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5238 - acc: 0.8513 - val_loss: 0.5606 - val_acc: 0.8429\n",
      "Epoch 220/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5217 - acc: 0.8586 - val_loss: 0.5584 - val_acc: 0.8370\n",
      "Epoch 221/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.5192 - acc: 0.8566 - val_loss: 0.5568 - val_acc: 0.8370\n",
      "Epoch 222/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5167 - acc: 0.8526 - val_loss: 0.5553 - val_acc: 0.8410\n",
      "Epoch 223/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5150 - acc: 0.8579 - val_loss: 0.5533 - val_acc: 0.8370\n",
      "Epoch 224/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5137 - acc: 0.8586 - val_loss: 0.5512 - val_acc: 0.8370\n",
      "Epoch 225/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5119 - acc: 0.8579 - val_loss: 0.5499 - val_acc: 0.8449\n",
      "Epoch 226/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5098 - acc: 0.8599 - val_loss: 0.5474 - val_acc: 0.8469\n",
      "Epoch 227/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5084 - acc: 0.8599 - val_loss: 0.5455 - val_acc: 0.8410\n",
      "Epoch 228/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5071 - acc: 0.8599 - val_loss: 0.5442 - val_acc: 0.8410\n",
      "Epoch 229/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5048 - acc: 0.8612 - val_loss: 0.5455 - val_acc: 0.8330\n",
      "Epoch 230/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5025 - acc: 0.8552 - val_loss: 0.5407 - val_acc: 0.8449\n",
      "Epoch 231/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5000 - acc: 0.8619 - val_loss: 0.5392 - val_acc: 0.8429\n",
      "Epoch 232/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4981 - acc: 0.8546 - val_loss: 0.5374 - val_acc: 0.8449\n",
      "Epoch 233/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4963 - acc: 0.8592 - val_loss: 0.5350 - val_acc: 0.8469\n",
      "Epoch 234/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4939 - acc: 0.8606 - val_loss: 0.5333 - val_acc: 0.8489\n",
      "Epoch 235/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4919 - acc: 0.8599 - val_loss: 0.5317 - val_acc: 0.8429\n",
      "Epoch 236/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4894 - acc: 0.8566 - val_loss: 0.5301 - val_acc: 0.8449\n",
      "Epoch 237/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4968 - acc: 0.864 - 0s 64us/step - loss: 0.4877 - acc: 0.8612 - val_loss: 0.5287 - val_acc: 0.8410\n",
      "Epoch 238/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4852 - acc: 0.8606 - val_loss: 0.5269 - val_acc: 0.8449\n",
      "Epoch 239/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4837 - acc: 0.8606 - val_loss: 0.5242 - val_acc: 0.8429\n",
      "Epoch 240/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4808 - acc: 0.8586 - val_loss: 0.5232 - val_acc: 0.8410\n",
      "Epoch 241/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4770 - acc: 0.8606 - val_loss: 0.5200 - val_acc: 0.8429\n",
      "Epoch 242/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.4758 - acc: 0.8625 - val_loss: 0.5179 - val_acc: 0.8429\n",
      "Epoch 243/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4726 - acc: 0.8619 - val_loss: 0.5161 - val_acc: 0.8390\n",
      "Epoch 244/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4694 - acc: 0.8672 - val_loss: 0.5151 - val_acc: 0.8390\n",
      "Epoch 245/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4664 - acc: 0.8632 - val_loss: 0.5129 - val_acc: 0.8410\n",
      "Epoch 246/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4642 - acc: 0.8659 - val_loss: 0.5112 - val_acc: 0.8429\n",
      "Epoch 247/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4608 - acc: 0.8679 - val_loss: 0.5128 - val_acc: 0.8410\n",
      "Epoch 248/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4594 - acc: 0.8632 - val_loss: 0.5098 - val_acc: 0.8390\n",
      "Epoch 249/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4570 - acc: 0.8692 - val_loss: 0.5100 - val_acc: 0.8429\n",
      "Epoch 250/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4554 - acc: 0.8659 - val_loss: 0.5085 - val_acc: 0.8429\n",
      "Epoch 251/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4536 - acc: 0.8679 - val_loss: 0.5086 - val_acc: 0.8390\n",
      "Epoch 252/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4524 - acc: 0.8672 - val_loss: 0.5062 - val_acc: 0.8429\n",
      "Epoch 253/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4509 - acc: 0.8672 - val_loss: 0.5051 - val_acc: 0.8410\n",
      "Epoch 254/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4488 - acc: 0.8685 - val_loss: 0.5046 - val_acc: 0.8410\n",
      "Epoch 255/1500\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.4472 - acc: 0.8705 - val_loss: 0.5033 - val_acc: 0.8429\n",
      "Epoch 256/1500\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.4465 - acc: 0.8712 - val_loss: 0.5039 - val_acc: 0.8449\n",
      "Epoch 257/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4443 - acc: 0.8665 - val_loss: 0.5031 - val_acc: 0.8429\n",
      "Epoch 258/1500\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4612 - acc: 0.867 - 0s 61us/step - loss: 0.4439 - acc: 0.8712 - val_loss: 0.5022 - val_acc: 0.8410\n",
      "Epoch 259/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4421 - acc: 0.8718 - val_loss: 0.5001 - val_acc: 0.8429\n",
      "Epoch 260/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4414 - acc: 0.8712 - val_loss: 0.5033 - val_acc: 0.8429\n",
      "Epoch 261/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4402 - acc: 0.8712 - val_loss: 0.4993 - val_acc: 0.8410\n",
      "Epoch 262/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4390 - acc: 0.8718 - val_loss: 0.4995 - val_acc: 0.8410\n",
      "Epoch 263/1500\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.4371 - acc: 0.8725 - val_loss: 0.4988 - val_acc: 0.8410\n",
      "Epoch 264/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.4360 - acc: 0.8725 - val_loss: 0.4963 - val_acc: 0.8449\n",
      "Epoch 265/1500\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.4396 - acc: 0.8745 - val_loss: 0.5000 - val_acc: 0.8390\n",
      "Epoch 266/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4356 - acc: 0.8745 - val_loss: 0.4974 - val_acc: 0.8390\n",
      "Epoch 267/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4331 - acc: 0.8745 - val_loss: 0.4969 - val_acc: 0.8449\n",
      "Epoch 268/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4323 - acc: 0.8705 - val_loss: 0.4951 - val_acc: 0.8449\n",
      "Epoch 269/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4305 - acc: 0.8752 - val_loss: 0.4958 - val_acc: 0.8469\n",
      "Epoch 270/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4307 - acc: 0.8712 - val_loss: 0.4944 - val_acc: 0.8429\n",
      "Epoch 271/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4289 - acc: 0.8738 - val_loss: 0.4923 - val_acc: 0.8410\n",
      "Epoch 272/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4280 - acc: 0.8738 - val_loss: 0.4934 - val_acc: 0.8449\n",
      "Epoch 273/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4274 - acc: 0.8738 - val_loss: 0.4921 - val_acc: 0.8449\n",
      "Epoch 274/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4265 - acc: 0.8752 - val_loss: 0.4914 - val_acc: 0.8429\n",
      "Epoch 275/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4249 - acc: 0.8732 - val_loss: 0.4902 - val_acc: 0.8410\n",
      "Epoch 276/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4240 - acc: 0.8745 - val_loss: 0.4901 - val_acc: 0.8410\n",
      "Epoch 277/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4241 - acc: 0.8738 - val_loss: 0.4892 - val_acc: 0.8449\n",
      "Epoch 278/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4215 - acc: 0.8745 - val_loss: 0.4895 - val_acc: 0.8469\n",
      "Epoch 279/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4209 - acc: 0.8725 - val_loss: 0.4873 - val_acc: 0.8469\n",
      "Epoch 280/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4213 - acc: 0.8758 - val_loss: 0.4873 - val_acc: 0.8469\n",
      "Epoch 281/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4201 - acc: 0.8745 - val_loss: 0.4868 - val_acc: 0.8489\n",
      "Epoch 282/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4184 - acc: 0.8758 - val_loss: 0.4883 - val_acc: 0.8410\n",
      "Epoch 283/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4190 - acc: 0.8745 - val_loss: 0.4873 - val_acc: 0.8429\n",
      "Epoch 284/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4166 - acc: 0.8765 - val_loss: 0.4862 - val_acc: 0.8449\n",
      "Epoch 285/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4156 - acc: 0.8745 - val_loss: 0.4852 - val_acc: 0.8429\n",
      "Epoch 286/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4179 - acc: 0.8725 - val_loss: 0.4905 - val_acc: 0.8429\n",
      "Epoch 287/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4148 - acc: 0.8725 - val_loss: 0.4863 - val_acc: 0.8429\n",
      "Epoch 288/1500\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3948 - acc: 0.881 - 0s 66us/step - loss: 0.4138 - acc: 0.8732 - val_loss: 0.4866 - val_acc: 0.8429\n",
      "Epoch 289/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4128 - acc: 0.8725 - val_loss: 0.4843 - val_acc: 0.8469\n",
      "Epoch 290/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4127 - acc: 0.8745 - val_loss: 0.4836 - val_acc: 0.8449\n",
      "Epoch 291/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4111 - acc: 0.8745 - val_loss: 0.4845 - val_acc: 0.8449\n",
      "Epoch 292/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4106 - acc: 0.8778 - val_loss: 0.4824 - val_acc: 0.8469\n",
      "Epoch 293/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4098 - acc: 0.8752 - val_loss: 0.4845 - val_acc: 0.8449\n",
      "Epoch 294/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4092 - acc: 0.8745 - val_loss: 0.4818 - val_acc: 0.8489\n",
      "Epoch 295/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4086 - acc: 0.8772 - val_loss: 0.4813 - val_acc: 0.8469\n",
      "Epoch 296/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4084 - acc: 0.8772 - val_loss: 0.4802 - val_acc: 0.8429\n",
      "Epoch 297/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4080 - acc: 0.8772 - val_loss: 0.4801 - val_acc: 0.8429\n",
      "Epoch 298/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4070 - acc: 0.8765 - val_loss: 0.4793 - val_acc: 0.8449\n",
      "Epoch 299/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4063 - acc: 0.8752 - val_loss: 0.4781 - val_acc: 0.8449\n",
      "Epoch 300/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4050 - acc: 0.8778 - val_loss: 0.4785 - val_acc: 0.8429\n",
      "Epoch 301/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4042 - acc: 0.8772 - val_loss: 0.4781 - val_acc: 0.8429\n",
      "Epoch 302/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4034 - acc: 0.8758 - val_loss: 0.4774 - val_acc: 0.8429\n",
      "Epoch 303/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.4029 - acc: 0.8765 - val_loss: 0.4768 - val_acc: 0.8429\n",
      "Epoch 304/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4038 - acc: 0.8792 - val_loss: 0.4778 - val_acc: 0.8429\n",
      "Epoch 305/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.4025 - acc: 0.8765 - val_loss: 0.4778 - val_acc: 0.8429\n",
      "Epoch 306/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4016 - acc: 0.8772 - val_loss: 0.4772 - val_acc: 0.8469\n",
      "Epoch 307/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4003 - acc: 0.8778 - val_loss: 0.4769 - val_acc: 0.8449\n",
      "Epoch 308/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4011 - acc: 0.8798 - val_loss: 0.4769 - val_acc: 0.8449\n",
      "Epoch 309/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4002 - acc: 0.8785 - val_loss: 0.4753 - val_acc: 0.8429\n",
      "Epoch 310/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3996 - acc: 0.8798 - val_loss: 0.4768 - val_acc: 0.8469\n",
      "Epoch 311/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3995 - acc: 0.8811 - val_loss: 0.4748 - val_acc: 0.8429\n",
      "Epoch 312/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3988 - acc: 0.8792 - val_loss: 0.4747 - val_acc: 0.8449\n",
      "Epoch 313/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3968 - acc: 0.8792 - val_loss: 0.4732 - val_acc: 0.8429\n",
      "Epoch 314/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3965 - acc: 0.8798 - val_loss: 0.4725 - val_acc: 0.8429\n",
      "Epoch 315/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3956 - acc: 0.8811 - val_loss: 0.4728 - val_acc: 0.8449\n",
      "Epoch 316/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3953 - acc: 0.8825 - val_loss: 0.4726 - val_acc: 0.8429\n",
      "Epoch 317/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3951 - acc: 0.8792 - val_loss: 0.4733 - val_acc: 0.8429\n",
      "Epoch 318/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3946 - acc: 0.8818 - val_loss: 0.4714 - val_acc: 0.8449\n",
      "Epoch 319/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3939 - acc: 0.8792 - val_loss: 0.4716 - val_acc: 0.8449\n",
      "Epoch 320/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3927 - acc: 0.8792 - val_loss: 0.4717 - val_acc: 0.8429\n",
      "Epoch 321/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3927 - acc: 0.8798 - val_loss: 0.4724 - val_acc: 0.8429\n",
      "Epoch 322/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3956 - acc: 0.8805 - val_loss: 0.4721 - val_acc: 0.8410\n",
      "Epoch 323/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3915 - acc: 0.8798 - val_loss: 0.4720 - val_acc: 0.8449\n",
      "Epoch 324/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3923 - acc: 0.8831 - val_loss: 0.4702 - val_acc: 0.8449\n",
      "Epoch 325/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3912 - acc: 0.8798 - val_loss: 0.4683 - val_acc: 0.8449\n",
      "Epoch 326/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3894 - acc: 0.8825 - val_loss: 0.4722 - val_acc: 0.8489\n",
      "Epoch 327/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3898 - acc: 0.8798 - val_loss: 0.4690 - val_acc: 0.8449\n",
      "Epoch 328/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3896 - acc: 0.8811 - val_loss: 0.4691 - val_acc: 0.8469\n",
      "Epoch 329/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3889 - acc: 0.8811 - val_loss: 0.4688 - val_acc: 0.8469\n",
      "Epoch 330/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3888 - acc: 0.8818 - val_loss: 0.4662 - val_acc: 0.8469\n",
      "Epoch 331/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3879 - acc: 0.8825 - val_loss: 0.4662 - val_acc: 0.8429\n",
      "Epoch 332/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3869 - acc: 0.8831 - val_loss: 0.4682 - val_acc: 0.8429\n",
      "Epoch 333/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3863 - acc: 0.8838 - val_loss: 0.4651 - val_acc: 0.8469\n",
      "Epoch 334/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3861 - acc: 0.8825 - val_loss: 0.4656 - val_acc: 0.8469\n",
      "Epoch 335/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3855 - acc: 0.8818 - val_loss: 0.4688 - val_acc: 0.8449\n",
      "Epoch 336/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3858 - acc: 0.8858 - val_loss: 0.4663 - val_acc: 0.8449\n",
      "Epoch 337/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3844 - acc: 0.8845 - val_loss: 0.4646 - val_acc: 0.8469\n",
      "Epoch 338/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3844 - acc: 0.8818 - val_loss: 0.4659 - val_acc: 0.8489\n",
      "Epoch 339/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3841 - acc: 0.8851 - val_loss: 0.4636 - val_acc: 0.8449\n",
      "Epoch 340/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3831 - acc: 0.8845 - val_loss: 0.4642 - val_acc: 0.8469\n",
      "Epoch 341/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3844 - acc: 0.8825 - val_loss: 0.4644 - val_acc: 0.8449\n",
      "Epoch 342/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3830 - acc: 0.8851 - val_loss: 0.4638 - val_acc: 0.8449\n",
      "Epoch 343/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3822 - acc: 0.8858 - val_loss: 0.4630 - val_acc: 0.8449\n",
      "Epoch 344/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3817 - acc: 0.8851 - val_loss: 0.4617 - val_acc: 0.8449\n",
      "Epoch 345/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3827 - acc: 0.8845 - val_loss: 0.4611 - val_acc: 0.8429\n",
      "Epoch 346/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3819 - acc: 0.8838 - val_loss: 0.4605 - val_acc: 0.8429\n",
      "Epoch 347/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3813 - acc: 0.8851 - val_loss: 0.4609 - val_acc: 0.8469\n",
      "Epoch 348/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3803 - acc: 0.8845 - val_loss: 0.4607 - val_acc: 0.8449\n",
      "Epoch 349/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3803 - acc: 0.8845 - val_loss: 0.4604 - val_acc: 0.8449\n",
      "Epoch 350/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3800 - acc: 0.8831 - val_loss: 0.4596 - val_acc: 0.8449\n",
      "Epoch 351/1500\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3792 - acc: 0.8851 - val_loss: 0.4592 - val_acc: 0.8529\n",
      "Epoch 352/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3795 - acc: 0.8858 - val_loss: 0.4620 - val_acc: 0.8449\n",
      "Epoch 353/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3784 - acc: 0.8845 - val_loss: 0.4584 - val_acc: 0.8429\n",
      "Epoch 354/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3786 - acc: 0.8851 - val_loss: 0.4598 - val_acc: 0.8449\n",
      "Epoch 355/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3776 - acc: 0.8878 - val_loss: 0.4579 - val_acc: 0.8449\n",
      "Epoch 356/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3769 - acc: 0.8871 - val_loss: 0.4588 - val_acc: 0.8449\n",
      "Epoch 357/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3764 - acc: 0.8845 - val_loss: 0.4590 - val_acc: 0.8449\n",
      "Epoch 358/1500\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.3755 - acc: 0.8884 - val_loss: 0.4574 - val_acc: 0.8469\n",
      "Epoch 359/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3761 - acc: 0.8865 - val_loss: 0.4573 - val_acc: 0.8449\n",
      "Epoch 360/1500\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3753 - acc: 0.8865 - val_loss: 0.4573 - val_acc: 0.8469\n",
      "Epoch 361/1500\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3757 - acc: 0.8878 - val_loss: 0.4568 - val_acc: 0.8469\n",
      "Epoch 362/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3763 - acc: 0.8884 - val_loss: 0.4578 - val_acc: 0.8509\n",
      "Epoch 363/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3745 - acc: 0.8884 - val_loss: 0.4560 - val_acc: 0.8429\n",
      "Epoch 364/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3752 - acc: 0.8845 - val_loss: 0.4541 - val_acc: 0.8429\n",
      "Epoch 365/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3744 - acc: 0.8878 - val_loss: 0.4580 - val_acc: 0.8529\n",
      "Epoch 366/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3732 - acc: 0.8911 - val_loss: 0.4542 - val_acc: 0.8449\n",
      "Epoch 367/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3736 - acc: 0.8871 - val_loss: 0.4546 - val_acc: 0.8449\n",
      "Epoch 368/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3727 - acc: 0.8871 - val_loss: 0.4550 - val_acc: 0.8449\n",
      "Epoch 369/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3724 - acc: 0.8898 - val_loss: 0.4531 - val_acc: 0.8449\n",
      "Epoch 370/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3716 - acc: 0.8871 - val_loss: 0.4548 - val_acc: 0.8489\n",
      "Epoch 371/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3718 - acc: 0.8865 - val_loss: 0.4545 - val_acc: 0.8449\n",
      "Epoch 372/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3712 - acc: 0.8898 - val_loss: 0.4523 - val_acc: 0.8469\n",
      "Epoch 373/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3727 - acc: 0.8858 - val_loss: 0.4524 - val_acc: 0.8489\n",
      "Epoch 374/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3707 - acc: 0.8884 - val_loss: 0.4514 - val_acc: 0.8449\n",
      "Epoch 375/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3704 - acc: 0.8891 - val_loss: 0.4521 - val_acc: 0.8509\n",
      "Epoch 376/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3695 - acc: 0.8865 - val_loss: 0.4536 - val_acc: 0.8469\n",
      "Epoch 377/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3693 - acc: 0.8898 - val_loss: 0.4519 - val_acc: 0.8469\n",
      "Epoch 378/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3688 - acc: 0.8878 - val_loss: 0.4498 - val_acc: 0.8489\n",
      "Epoch 379/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3691 - acc: 0.8891 - val_loss: 0.4523 - val_acc: 0.8469\n",
      "Epoch 380/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3696 - acc: 0.8858 - val_loss: 0.4508 - val_acc: 0.8469\n",
      "Epoch 381/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3690 - acc: 0.8845 - val_loss: 0.4509 - val_acc: 0.8449\n",
      "Epoch 382/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3694 - acc: 0.8858 - val_loss: 0.4527 - val_acc: 0.8469\n",
      "Epoch 383/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3711 - acc: 0.8891 - val_loss: 0.4487 - val_acc: 0.8489\n",
      "Epoch 384/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3674 - acc: 0.8871 - val_loss: 0.4490 - val_acc: 0.8529\n",
      "Epoch 385/1500\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3690 - acc: 0.884 - 0s 66us/step - loss: 0.3665 - acc: 0.8878 - val_loss: 0.4510 - val_acc: 0.8489\n",
      "Epoch 386/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3662 - acc: 0.8911 - val_loss: 0.4486 - val_acc: 0.8549\n",
      "Epoch 387/1500\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3709 - acc: 0.881 - 0s 66us/step - loss: 0.3663 - acc: 0.8858 - val_loss: 0.4482 - val_acc: 0.8449\n",
      "Epoch 388/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3672 - acc: 0.8884 - val_loss: 0.4499 - val_acc: 0.8489\n",
      "Epoch 389/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3661 - acc: 0.8878 - val_loss: 0.4481 - val_acc: 0.8469\n",
      "Epoch 390/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3656 - acc: 0.8898 - val_loss: 0.4492 - val_acc: 0.8469\n",
      "Epoch 391/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3658 - acc: 0.8904 - val_loss: 0.4478 - val_acc: 0.8489\n",
      "Epoch 392/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3647 - acc: 0.8898 - val_loss: 0.4462 - val_acc: 0.8489\n",
      "Epoch 393/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3648 - acc: 0.8878 - val_loss: 0.4471 - val_acc: 0.8469\n",
      "Epoch 394/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3640 - acc: 0.8911 - val_loss: 0.4466 - val_acc: 0.8529\n",
      "Epoch 395/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3640 - acc: 0.8898 - val_loss: 0.4451 - val_acc: 0.8469\n",
      "Epoch 396/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3632 - acc: 0.8904 - val_loss: 0.4461 - val_acc: 0.8529\n",
      "Epoch 397/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3639 - acc: 0.8898 - val_loss: 0.4434 - val_acc: 0.8509\n",
      "Epoch 398/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3652 - acc: 0.8878 - val_loss: 0.4446 - val_acc: 0.8489\n",
      "Epoch 399/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3630 - acc: 0.8884 - val_loss: 0.4467 - val_acc: 0.8489\n",
      "Epoch 400/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3628 - acc: 0.8904 - val_loss: 0.4443 - val_acc: 0.8469\n",
      "Epoch 401/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3630 - acc: 0.8891 - val_loss: 0.4432 - val_acc: 0.8489\n",
      "Epoch 402/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3624 - acc: 0.8884 - val_loss: 0.4445 - val_acc: 0.8469\n",
      "Epoch 403/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3627 - acc: 0.8918 - val_loss: 0.4428 - val_acc: 0.8489\n",
      "Epoch 404/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3618 - acc: 0.8891 - val_loss: 0.4442 - val_acc: 0.8469\n",
      "Epoch 405/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3609 - acc: 0.8904 - val_loss: 0.4424 - val_acc: 0.8509\n",
      "Epoch 406/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3612 - acc: 0.8904 - val_loss: 0.4445 - val_acc: 0.8509\n",
      "Epoch 407/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3617 - acc: 0.8884 - val_loss: 0.4446 - val_acc: 0.8489\n",
      "Epoch 408/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3613 - acc: 0.8878 - val_loss: 0.4431 - val_acc: 0.8489\n",
      "Epoch 409/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3599 - acc: 0.8904 - val_loss: 0.4416 - val_acc: 0.8489\n",
      "Epoch 410/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3598 - acc: 0.8884 - val_loss: 0.4410 - val_acc: 0.8509\n",
      "Epoch 411/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3605 - acc: 0.8858 - val_loss: 0.4436 - val_acc: 0.8509\n",
      "Epoch 412/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3602 - acc: 0.8891 - val_loss: 0.4436 - val_acc: 0.8509\n",
      "Epoch 413/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3600 - acc: 0.8891 - val_loss: 0.4413 - val_acc: 0.8529\n",
      "Epoch 414/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3588 - acc: 0.8898 - val_loss: 0.4401 - val_acc: 0.8529\n",
      "Epoch 415/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3619 - acc: 0.8911 - val_loss: 0.4430 - val_acc: 0.8489\n",
      "Epoch 416/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3584 - acc: 0.8911 - val_loss: 0.4409 - val_acc: 0.8509\n",
      "Epoch 417/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3587 - acc: 0.8898 - val_loss: 0.4407 - val_acc: 0.8489\n",
      "Epoch 418/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3576 - acc: 0.8904 - val_loss: 0.4406 - val_acc: 0.8489\n",
      "Epoch 419/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3583 - acc: 0.8911 - val_loss: 0.4385 - val_acc: 0.8489\n",
      "Epoch 420/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3581 - acc: 0.8884 - val_loss: 0.4385 - val_acc: 0.8509\n",
      "Epoch 421/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.3580 - acc: 0.8911 - val_loss: 0.4382 - val_acc: 0.8529\n",
      "Epoch 422/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3568 - acc: 0.8884 - val_loss: 0.4378 - val_acc: 0.8489\n",
      "Epoch 423/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3566 - acc: 0.8898 - val_loss: 0.4383 - val_acc: 0.8529\n",
      "Epoch 424/1500\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3580 - acc: 0.8918 - val_loss: 0.4386 - val_acc: 0.8509\n",
      "Epoch 425/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3564 - acc: 0.8911 - val_loss: 0.4387 - val_acc: 0.8529\n",
      "Epoch 426/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3559 - acc: 0.8884 - val_loss: 0.4365 - val_acc: 0.8489\n",
      "Epoch 427/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3555 - acc: 0.8911 - val_loss: 0.4373 - val_acc: 0.8509\n",
      "Epoch 428/1500\n",
      "1506/1506 [==============================] - 0s 49us/step - loss: 0.3555 - acc: 0.8898 - val_loss: 0.4382 - val_acc: 0.8489\n",
      "Epoch 429/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3570 - acc: 0.8884 - val_loss: 0.4348 - val_acc: 0.8489\n",
      "Epoch 430/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.3612 - acc: 0.8871 - val_loss: 0.4412 - val_acc: 0.8489\n",
      "Epoch 431/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3568 - acc: 0.8878 - val_loss: 0.4373 - val_acc: 0.8509\n",
      "Epoch 432/1500\n",
      "1506/1506 [==============================] - 0s 48us/step - loss: 0.3552 - acc: 0.8904 - val_loss: 0.4357 - val_acc: 0.8509\n",
      "Epoch 433/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3554 - acc: 0.8898 - val_loss: 0.4346 - val_acc: 0.8509\n",
      "Epoch 434/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3548 - acc: 0.8898 - val_loss: 0.4343 - val_acc: 0.8489\n",
      "Epoch 435/1500\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 0.3539 - acc: 0.8898 - val_loss: 0.4350 - val_acc: 0.8489\n",
      "Epoch 436/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3544 - acc: 0.8911 - val_loss: 0.4356 - val_acc: 0.8509\n",
      "Epoch 437/1500\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3538 - acc: 0.8904 - val_loss: 0.4368 - val_acc: 0.8509\n",
      "Epoch 438/1500\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 0.3542 - acc: 0.8904 - val_loss: 0.4370 - val_acc: 0.8569\n",
      "Epoch 439/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3547 - acc: 0.8904 - val_loss: 0.4347 - val_acc: 0.8529\n",
      "Epoch 440/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3526 - acc: 0.8898 - val_loss: 0.4340 - val_acc: 0.8549\n",
      "Epoch 441/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3547 - acc: 0.8898 - val_loss: 0.4335 - val_acc: 0.8509\n",
      "Epoch 442/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3529 - acc: 0.8911 - val_loss: 0.4352 - val_acc: 0.8549\n",
      "Epoch 443/1500\n",
      "1506/1506 [==============================] - 0s 43us/step - loss: 0.3524 - acc: 0.8918 - val_loss: 0.4329 - val_acc: 0.8529\n",
      "Epoch 444/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3545 - acc: 0.8871 - val_loss: 0.4335 - val_acc: 0.8569\n",
      "Epoch 445/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3515 - acc: 0.8891 - val_loss: 0.4342 - val_acc: 0.8509\n",
      "Epoch 446/1500\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3516 - acc: 0.8904 - val_loss: 0.4318 - val_acc: 0.8489\n",
      "Epoch 447/1500\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3509 - acc: 0.8904 - val_loss: 0.4315 - val_acc: 0.8509\n",
      "Epoch 448/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.3501 - acc: 0.8891 - val_loss: 0.4324 - val_acc: 0.8489\n",
      "Epoch 449/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3500 - acc: 0.8898 - val_loss: 0.4317 - val_acc: 0.8529\n",
      "Epoch 450/1500\n",
      "1506/1506 [==============================] - 0s 45us/step - loss: 0.3499 - acc: 0.8918 - val_loss: 0.4307 - val_acc: 0.8489\n",
      "Epoch 451/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.3494 - acc: 0.8898 - val_loss: 0.4327 - val_acc: 0.8549\n",
      "Epoch 452/1500\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 0.3497 - acc: 0.8918 - val_loss: 0.4301 - val_acc: 0.8489\n",
      "Epoch 453/1500\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.3487 - acc: 0.8898 - val_loss: 0.4309 - val_acc: 0.8549\n",
      "Epoch 454/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3492 - acc: 0.8898 - val_loss: 0.4291 - val_acc: 0.8529\n",
      "Epoch 455/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3495 - acc: 0.8884 - val_loss: 0.4303 - val_acc: 0.8489\n",
      "Epoch 456/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3485 - acc: 0.8924 - val_loss: 0.4302 - val_acc: 0.8529\n",
      "Epoch 457/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3481 - acc: 0.8911 - val_loss: 0.4297 - val_acc: 0.8529\n",
      "Epoch 458/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3474 - acc: 0.8898 - val_loss: 0.4295 - val_acc: 0.8529\n",
      "Epoch 459/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3487 - acc: 0.8911 - val_loss: 0.4293 - val_acc: 0.8489\n",
      "Epoch 460/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3480 - acc: 0.8924 - val_loss: 0.4304 - val_acc: 0.8489\n",
      "Epoch 461/1500\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.3479 - acc: 0.8924 - val_loss: 0.4301 - val_acc: 0.8509\n",
      "Epoch 462/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3481 - acc: 0.8891 - val_loss: 0.4293 - val_acc: 0.8509\n",
      "Epoch 463/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3471 - acc: 0.8931 - val_loss: 0.4287 - val_acc: 0.8509\n",
      "Epoch 464/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3472 - acc: 0.8898 - val_loss: 0.4292 - val_acc: 0.8489\n",
      "Epoch 465/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3473 - acc: 0.8931 - val_loss: 0.4272 - val_acc: 0.8489\n",
      "Epoch 466/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3467 - acc: 0.8911 - val_loss: 0.4284 - val_acc: 0.8529\n",
      "Epoch 467/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3474 - acc: 0.8904 - val_loss: 0.4279 - val_acc: 0.8529\n",
      "Epoch 468/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3458 - acc: 0.8924 - val_loss: 0.4261 - val_acc: 0.8509\n",
      "Epoch 469/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3452 - acc: 0.8924 - val_loss: 0.4282 - val_acc: 0.8529\n",
      "Epoch 470/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3458 - acc: 0.8918 - val_loss: 0.4283 - val_acc: 0.8509\n",
      "Epoch 471/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3449 - acc: 0.8951 - val_loss: 0.4277 - val_acc: 0.8509\n",
      "Epoch 472/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3454 - acc: 0.8918 - val_loss: 0.4263 - val_acc: 0.8509\n",
      "Epoch 473/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3447 - acc: 0.8918 - val_loss: 0.4271 - val_acc: 0.8529\n",
      "Epoch 474/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3448 - acc: 0.8944 - val_loss: 0.4237 - val_acc: 0.8489\n",
      "Epoch 475/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3445 - acc: 0.8911 - val_loss: 0.4266 - val_acc: 0.8509\n",
      "Epoch 476/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3457 - acc: 0.8924 - val_loss: 0.4251 - val_acc: 0.8489\n",
      "Epoch 477/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3474 - acc: 0.8911 - val_loss: 0.4266 - val_acc: 0.8588\n",
      "Epoch 478/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3454 - acc: 0.8884 - val_loss: 0.4262 - val_acc: 0.8549\n",
      "Epoch 479/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3428 - acc: 0.8924 - val_loss: 0.4264 - val_acc: 0.8529\n",
      "Epoch 480/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3437 - acc: 0.8924 - val_loss: 0.4245 - val_acc: 0.8509\n",
      "Epoch 481/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3432 - acc: 0.8938 - val_loss: 0.4249 - val_acc: 0.8549\n",
      "Epoch 482/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3429 - acc: 0.8924 - val_loss: 0.4233 - val_acc: 0.8509\n",
      "Epoch 483/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3424 - acc: 0.8931 - val_loss: 0.4237 - val_acc: 0.8509\n",
      "Epoch 484/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3444 - acc: 0.8931 - val_loss: 0.4213 - val_acc: 0.8509\n",
      "Epoch 485/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3423 - acc: 0.8918 - val_loss: 0.4237 - val_acc: 0.8529\n",
      "Epoch 486/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3420 - acc: 0.8924 - val_loss: 0.4234 - val_acc: 0.8509\n",
      "Epoch 487/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3419 - acc: 0.8938 - val_loss: 0.4230 - val_acc: 0.8509\n",
      "Epoch 488/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3433 - acc: 0.8931 - val_loss: 0.4250 - val_acc: 0.8569\n",
      "Epoch 489/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3417 - acc: 0.8924 - val_loss: 0.4232 - val_acc: 0.8549\n",
      "Epoch 490/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3432 - acc: 0.8911 - val_loss: 0.4252 - val_acc: 0.8529\n",
      "Epoch 491/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3417 - acc: 0.8944 - val_loss: 0.4220 - val_acc: 0.8509\n",
      "Epoch 492/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3429 - acc: 0.8931 - val_loss: 0.4230 - val_acc: 0.8529\n",
      "Epoch 493/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3409 - acc: 0.8944 - val_loss: 0.4222 - val_acc: 0.8509\n",
      "Epoch 494/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3404 - acc: 0.8958 - val_loss: 0.4226 - val_acc: 0.8529\n",
      "Epoch 495/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3403 - acc: 0.8918 - val_loss: 0.4238 - val_acc: 0.8549\n",
      "Epoch 496/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3400 - acc: 0.8931 - val_loss: 0.4237 - val_acc: 0.8509\n",
      "Epoch 497/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3403 - acc: 0.8924 - val_loss: 0.4207 - val_acc: 0.8529\n",
      "Epoch 498/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3397 - acc: 0.8951 - val_loss: 0.4211 - val_acc: 0.8529\n",
      "Epoch 499/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3402 - acc: 0.8944 - val_loss: 0.4219 - val_acc: 0.8549\n",
      "Epoch 500/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3398 - acc: 0.8918 - val_loss: 0.4211 - val_acc: 0.8569\n",
      "Epoch 501/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3393 - acc: 0.8931 - val_loss: 0.4214 - val_acc: 0.8569\n",
      "Epoch 502/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3393 - acc: 0.8924 - val_loss: 0.4191 - val_acc: 0.8509\n",
      "Epoch 503/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3389 - acc: 0.8924 - val_loss: 0.4223 - val_acc: 0.8569\n",
      "Epoch 504/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3387 - acc: 0.8918 - val_loss: 0.4193 - val_acc: 0.8549\n",
      "Epoch 505/1500\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3395 - acc: 0.8938 - val_loss: 0.4197 - val_acc: 0.8569\n",
      "Epoch 506/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3389 - acc: 0.8938 - val_loss: 0.4200 - val_acc: 0.8549\n",
      "Epoch 507/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3383 - acc: 0.8924 - val_loss: 0.4200 - val_acc: 0.8509\n",
      "Epoch 508/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3388 - acc: 0.8931 - val_loss: 0.4198 - val_acc: 0.8549\n",
      "Epoch 509/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3379 - acc: 0.8938 - val_loss: 0.4201 - val_acc: 0.8529\n",
      "Epoch 510/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3383 - acc: 0.8931 - val_loss: 0.4187 - val_acc: 0.8549\n",
      "Epoch 511/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3374 - acc: 0.8924 - val_loss: 0.4189 - val_acc: 0.8549\n",
      "Epoch 512/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3377 - acc: 0.8944 - val_loss: 0.4206 - val_acc: 0.8608\n",
      "Epoch 513/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3376 - acc: 0.8918 - val_loss: 0.4197 - val_acc: 0.8588\n",
      "Epoch 514/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3369 - acc: 0.8944 - val_loss: 0.4178 - val_acc: 0.8549\n",
      "Epoch 515/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3365 - acc: 0.8951 - val_loss: 0.4184 - val_acc: 0.8549\n",
      "Epoch 516/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3370 - acc: 0.8944 - val_loss: 0.4178 - val_acc: 0.8529\n",
      "Epoch 517/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3364 - acc: 0.8918 - val_loss: 0.4187 - val_acc: 0.8549\n",
      "Epoch 518/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3369 - acc: 0.8931 - val_loss: 0.4210 - val_acc: 0.8588\n",
      "Epoch 519/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3363 - acc: 0.8951 - val_loss: 0.4170 - val_acc: 0.8588\n",
      "Epoch 520/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3358 - acc: 0.8931 - val_loss: 0.4163 - val_acc: 0.8549\n",
      "Epoch 521/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3360 - acc: 0.8938 - val_loss: 0.4174 - val_acc: 0.8569\n",
      "Epoch 522/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3354 - acc: 0.8951 - val_loss: 0.4171 - val_acc: 0.8549\n",
      "Epoch 523/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3369 - acc: 0.8951 - val_loss: 0.4157 - val_acc: 0.8569\n",
      "Epoch 524/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3351 - acc: 0.8944 - val_loss: 0.4162 - val_acc: 0.8588\n",
      "Epoch 525/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3361 - acc: 0.8951 - val_loss: 0.4193 - val_acc: 0.8588\n",
      "Epoch 526/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3360 - acc: 0.8924 - val_loss: 0.4151 - val_acc: 0.8529\n",
      "Epoch 527/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3375 - acc: 0.8924 - val_loss: 0.4168 - val_acc: 0.8569\n",
      "Epoch 528/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3370 - acc: 0.8951 - val_loss: 0.4195 - val_acc: 0.8588\n",
      "Epoch 529/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3357 - acc: 0.8918 - val_loss: 0.4171 - val_acc: 0.8569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 530/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3446 - acc: 0.8944 - val_loss: 0.4202 - val_acc: 0.8549\n",
      "Epoch 531/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3350 - acc: 0.8918 - val_loss: 0.4165 - val_acc: 0.8549\n",
      "Epoch 532/1500\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.3339 - acc: 0.8938 - val_loss: 0.4150 - val_acc: 0.8529\n",
      "Epoch 533/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3388 - acc: 0.8904 - val_loss: 0.4156 - val_acc: 0.8608\n",
      "Epoch 534/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3332 - acc: 0.8951 - val_loss: 0.4158 - val_acc: 0.8569\n",
      "Epoch 535/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3338 - acc: 0.8944 - val_loss: 0.4174 - val_acc: 0.8588\n",
      "Epoch 536/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3346 - acc: 0.8951 - val_loss: 0.4169 - val_acc: 0.8569\n",
      "Epoch 537/1500\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.3345 - acc: 0.8938 - val_loss: 0.4167 - val_acc: 0.8569\n",
      "Epoch 538/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3333 - acc: 0.8931 - val_loss: 0.4149 - val_acc: 0.8588\n",
      "Epoch 539/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3331 - acc: 0.8958 - val_loss: 0.4160 - val_acc: 0.8608\n",
      "Epoch 540/1500\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3332 - acc: 0.8924 - val_loss: 0.4145 - val_acc: 0.8588\n",
      "Epoch 541/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3324 - acc: 0.8964 - val_loss: 0.4141 - val_acc: 0.8549\n",
      "Epoch 542/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3331 - acc: 0.8924 - val_loss: 0.4155 - val_acc: 0.8588\n",
      "Epoch 543/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3334 - acc: 0.8944 - val_loss: 0.4146 - val_acc: 0.8569\n",
      "Epoch 544/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3326 - acc: 0.8944 - val_loss: 0.4150 - val_acc: 0.8569\n",
      "Epoch 545/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3328 - acc: 0.8938 - val_loss: 0.4137 - val_acc: 0.8549\n",
      "Epoch 546/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3320 - acc: 0.8958 - val_loss: 0.4148 - val_acc: 0.8588\n",
      "Epoch 547/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3319 - acc: 0.8951 - val_loss: 0.4132 - val_acc: 0.8588\n",
      "Epoch 548/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3318 - acc: 0.8958 - val_loss: 0.4142 - val_acc: 0.8549\n",
      "Epoch 549/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3313 - acc: 0.8944 - val_loss: 0.4137 - val_acc: 0.8608\n",
      "Epoch 550/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3344 - acc: 0.8931 - val_loss: 0.4116 - val_acc: 0.8569\n",
      "Epoch 551/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3314 - acc: 0.8964 - val_loss: 0.4133 - val_acc: 0.8569\n",
      "Epoch 552/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3313 - acc: 0.8958 - val_loss: 0.4124 - val_acc: 0.8569\n",
      "Epoch 553/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3310 - acc: 0.8958 - val_loss: 0.4127 - val_acc: 0.8608\n",
      "Epoch 554/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3305 - acc: 0.8958 - val_loss: 0.4101 - val_acc: 0.8549\n",
      "Epoch 555/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3318 - acc: 0.8911 - val_loss: 0.4123 - val_acc: 0.8569\n",
      "Epoch 556/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3313 - acc: 0.8958 - val_loss: 0.4121 - val_acc: 0.8588\n",
      "Epoch 557/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3304 - acc: 0.8944 - val_loss: 0.4124 - val_acc: 0.8608\n",
      "Epoch 558/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3302 - acc: 0.8964 - val_loss: 0.4116 - val_acc: 0.8588\n",
      "Epoch 559/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3309 - acc: 0.8964 - val_loss: 0.4132 - val_acc: 0.8588\n",
      "Epoch 560/1500\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3315 - acc: 0.8964 - val_loss: 0.4124 - val_acc: 0.8608\n",
      "Epoch 561/1500\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3297 - acc: 0.8944 - val_loss: 0.4122 - val_acc: 0.8588\n",
      "Epoch 562/1500\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3298 - acc: 0.8951 - val_loss: 0.4128 - val_acc: 0.8608\n",
      "Epoch 563/1500\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3305 - acc: 0.8958 - val_loss: 0.4123 - val_acc: 0.8588\n",
      "Epoch 564/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3293 - acc: 0.8958 - val_loss: 0.4108 - val_acc: 0.8509\n",
      "Epoch 565/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3305 - acc: 0.8964 - val_loss: 0.4112 - val_acc: 0.8588\n",
      "Epoch 566/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3288 - acc: 0.8951 - val_loss: 0.4124 - val_acc: 0.8588\n",
      "Epoch 567/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3297 - acc: 0.8951 - val_loss: 0.4104 - val_acc: 0.8588\n",
      "Epoch 568/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3318 - acc: 0.8964 - val_loss: 0.4125 - val_acc: 0.8608\n",
      "Epoch 569/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3310 - acc: 0.8958 - val_loss: 0.4122 - val_acc: 0.8628\n",
      "Epoch 570/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3293 - acc: 0.8964 - val_loss: 0.4094 - val_acc: 0.8588\n",
      "Epoch 571/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3296 - acc: 0.8944 - val_loss: 0.4115 - val_acc: 0.8628\n",
      "Epoch 572/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3282 - acc: 0.8958 - val_loss: 0.4108 - val_acc: 0.8608\n",
      "Epoch 573/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3289 - acc: 0.8977 - val_loss: 0.4105 - val_acc: 0.8588\n",
      "Epoch 574/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3283 - acc: 0.8971 - val_loss: 0.4125 - val_acc: 0.8628\n",
      "Epoch 575/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3281 - acc: 0.8944 - val_loss: 0.4113 - val_acc: 0.8628\n",
      "Epoch 576/1500\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3278 - acc: 0.8964 - val_loss: 0.4104 - val_acc: 0.8588\n",
      "Epoch 577/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3279 - acc: 0.8964 - val_loss: 0.4122 - val_acc: 0.8628\n",
      "Epoch 578/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3329 - acc: 0.8984 - val_loss: 0.4138 - val_acc: 0.8628\n",
      "Epoch 579/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3280 - acc: 0.8958 - val_loss: 0.4101 - val_acc: 0.8569\n",
      "Epoch 580/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3277 - acc: 0.8971 - val_loss: 0.4088 - val_acc: 0.8569\n",
      "Epoch 581/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3276 - acc: 0.8971 - val_loss: 0.4091 - val_acc: 0.8588\n",
      "Epoch 582/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3274 - acc: 0.8971 - val_loss: 0.4108 - val_acc: 0.8608\n",
      "Epoch 583/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3309 - acc: 0.8964 - val_loss: 0.4125 - val_acc: 0.8588\n",
      "Epoch 584/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3275 - acc: 0.8951 - val_loss: 0.4118 - val_acc: 0.8608\n",
      "Epoch 585/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3269 - acc: 0.8977 - val_loss: 0.4115 - val_acc: 0.8608\n",
      "Epoch 586/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3271 - acc: 0.8964 - val_loss: 0.4115 - val_acc: 0.8628\n",
      "Epoch 587/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3271 - acc: 0.8971 - val_loss: 0.4085 - val_acc: 0.8608\n",
      "Epoch 588/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3263 - acc: 0.8971 - val_loss: 0.4089 - val_acc: 0.8648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 589/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3268 - acc: 0.8964 - val_loss: 0.4081 - val_acc: 0.8608\n",
      "Epoch 590/1500\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3516 - acc: 0.886 - 0s 61us/step - loss: 0.3261 - acc: 0.8977 - val_loss: 0.4087 - val_acc: 0.8628\n",
      "Epoch 591/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3268 - acc: 0.8964 - val_loss: 0.4098 - val_acc: 0.8588\n",
      "Epoch 592/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3278 - acc: 0.8971 - val_loss: 0.4098 - val_acc: 0.8628\n",
      "Epoch 593/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3269 - acc: 0.8971 - val_loss: 0.4106 - val_acc: 0.8628\n",
      "Epoch 594/1500\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3263 - acc: 0.8958 - val_loss: 0.4088 - val_acc: 0.8608\n",
      "Epoch 595/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3258 - acc: 0.8977 - val_loss: 0.4076 - val_acc: 0.8608\n",
      "Epoch 596/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3260 - acc: 0.8951 - val_loss: 0.4095 - val_acc: 0.8608\n",
      "Epoch 597/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3275 - acc: 0.8977 - val_loss: 0.4097 - val_acc: 0.8608\n",
      "Epoch 598/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3262 - acc: 0.8964 - val_loss: 0.4086 - val_acc: 0.8608\n",
      "Epoch 599/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3267 - acc: 0.8971 - val_loss: 0.4056 - val_acc: 0.8588\n",
      "Epoch 600/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3259 - acc: 0.9004 - val_loss: 0.4085 - val_acc: 0.8628\n",
      "Epoch 601/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3249 - acc: 0.8977 - val_loss: 0.4084 - val_acc: 0.8588\n",
      "Epoch 602/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3260 - acc: 0.8964 - val_loss: 0.4062 - val_acc: 0.8608\n",
      "Epoch 603/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3243 - acc: 0.8958 - val_loss: 0.4081 - val_acc: 0.8588\n",
      "Epoch 604/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3242 - acc: 0.8964 - val_loss: 0.4103 - val_acc: 0.8608\n",
      "Epoch 605/1500\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3263 - acc: 0.8951 - val_loss: 0.4091 - val_acc: 0.8648\n",
      "Epoch 606/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3251 - acc: 0.8977 - val_loss: 0.4081 - val_acc: 0.8648\n",
      "Epoch 607/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3255 - acc: 0.8951 - val_loss: 0.4078 - val_acc: 0.8628\n",
      "Epoch 608/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3250 - acc: 0.8971 - val_loss: 0.4046 - val_acc: 0.8608\n",
      "Epoch 609/1500\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3248 - acc: 0.8971 - val_loss: 0.4057 - val_acc: 0.8588\n",
      "Epoch 610/1500\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3242 - acc: 0.8984 - val_loss: 0.4086 - val_acc: 0.8628\n",
      "Epoch 611/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3240 - acc: 0.8977 - val_loss: 0.4074 - val_acc: 0.8608\n",
      "Epoch 612/1500\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3254 - acc: 0.8984 - val_loss: 0.4050 - val_acc: 0.8569\n",
      "Epoch 613/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3274 - acc: 0.8964 - val_loss: 0.4051 - val_acc: 0.8628\n",
      "Epoch 614/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3244 - acc: 0.8997 - val_loss: 0.4071 - val_acc: 0.8608\n",
      "Epoch 615/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3246 - acc: 0.8958 - val_loss: 0.4055 - val_acc: 0.8608\n",
      "Epoch 616/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3240 - acc: 0.8991 - val_loss: 0.4086 - val_acc: 0.8628\n",
      "Epoch 617/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3314 - acc: 0.8964 - val_loss: 0.4093 - val_acc: 0.8588\n",
      "Epoch 618/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3253 - acc: 0.8958 - val_loss: 0.4047 - val_acc: 0.8628\n",
      "Epoch 619/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3229 - acc: 0.8984 - val_loss: 0.4067 - val_acc: 0.8648\n",
      "Epoch 620/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3239 - acc: 0.8958 - val_loss: 0.4067 - val_acc: 0.8628\n",
      "Epoch 621/1500\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3223 - acc: 0.8977 - val_loss: 0.4073 - val_acc: 0.8628\n",
      "Epoch 622/1500\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3221 - acc: 0.8958 - val_loss: 0.4044 - val_acc: 0.8588\n",
      "Epoch 623/1500\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3231 - acc: 0.8971 - val_loss: 0.4038 - val_acc: 0.8648\n",
      "Epoch 624/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3223 - acc: 0.8964 - val_loss: 0.4050 - val_acc: 0.8608\n",
      "Epoch 625/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3222 - acc: 0.8977 - val_loss: 0.4044 - val_acc: 0.8628\n",
      "Epoch 626/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3220 - acc: 0.8977 - val_loss: 0.4041 - val_acc: 0.8608\n",
      "Epoch 627/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3214 - acc: 0.8984 - val_loss: 0.4044 - val_acc: 0.8628\n",
      "Epoch 628/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3230 - acc: 0.8984 - val_loss: 0.4042 - val_acc: 0.8668\n",
      "Epoch 629/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3256 - acc: 0.8984 - val_loss: 0.4062 - val_acc: 0.8608\n",
      "Epoch 630/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3231 - acc: 0.8958 - val_loss: 0.4077 - val_acc: 0.8648\n",
      "Epoch 631/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3223 - acc: 0.8997 - val_loss: 0.4030 - val_acc: 0.8588\n",
      "Epoch 632/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3227 - acc: 0.8997 - val_loss: 0.4055 - val_acc: 0.8648\n",
      "Epoch 633/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3226 - acc: 0.8984 - val_loss: 0.4053 - val_acc: 0.8628\n",
      "Epoch 634/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3204 - acc: 0.8984 - val_loss: 0.4042 - val_acc: 0.8648\n",
      "Epoch 635/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3211 - acc: 0.8971 - val_loss: 0.4045 - val_acc: 0.8688\n",
      "Epoch 636/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3216 - acc: 0.8958 - val_loss: 0.4057 - val_acc: 0.8628\n",
      "Epoch 637/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3210 - acc: 0.8984 - val_loss: 0.4036 - val_acc: 0.8608\n",
      "Epoch 638/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3216 - acc: 0.8958 - val_loss: 0.4024 - val_acc: 0.8648\n",
      "Epoch 639/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3214 - acc: 0.8951 - val_loss: 0.4037 - val_acc: 0.8648\n",
      "Epoch 640/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3215 - acc: 0.8997 - val_loss: 0.4040 - val_acc: 0.8628\n",
      "Epoch 641/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3219 - acc: 0.8971 - val_loss: 0.4021 - val_acc: 0.8608\n",
      "Epoch 642/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3210 - acc: 0.8971 - val_loss: 0.4019 - val_acc: 0.8588\n",
      "Epoch 643/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3206 - acc: 0.8964 - val_loss: 0.4067 - val_acc: 0.8608\n",
      "Epoch 644/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3198 - acc: 0.8971 - val_loss: 0.4025 - val_acc: 0.8648\n",
      "Epoch 645/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3212 - acc: 0.8971 - val_loss: 0.4044 - val_acc: 0.8628\n",
      "Epoch 646/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3220 - acc: 0.9004 - val_loss: 0.4029 - val_acc: 0.8648\n",
      "Epoch 647/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3201 - acc: 0.8984 - val_loss: 0.4042 - val_acc: 0.8608\n",
      "Epoch 648/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3200 - acc: 0.8984 - val_loss: 0.4029 - val_acc: 0.8628\n",
      "Epoch 649/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3192 - acc: 0.8984 - val_loss: 0.4013 - val_acc: 0.8628\n",
      "Epoch 650/1500\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3190 - acc: 0.8991 - val_loss: 0.4022 - val_acc: 0.8628\n",
      "Epoch 651/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3216 - acc: 0.8984 - val_loss: 0.4036 - val_acc: 0.8648\n",
      "Epoch 652/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3190 - acc: 0.8991 - val_loss: 0.4009 - val_acc: 0.8628\n",
      "Epoch 653/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3204 - acc: 0.8971 - val_loss: 0.4025 - val_acc: 0.8628\n",
      "Epoch 654/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3200 - acc: 0.8984 - val_loss: 0.4039 - val_acc: 0.8608\n",
      "Epoch 655/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3199 - acc: 0.8991 - val_loss: 0.4034 - val_acc: 0.8608\n",
      "Epoch 656/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3187 - acc: 0.8984 - val_loss: 0.4020 - val_acc: 0.8628\n",
      "Epoch 657/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.3195 - acc: 0.8971 - val_loss: 0.4014 - val_acc: 0.8628\n",
      "Epoch 658/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3186 - acc: 0.8991 - val_loss: 0.4034 - val_acc: 0.8648\n",
      "Epoch 659/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3187 - acc: 0.8977 - val_loss: 0.4023 - val_acc: 0.8608\n",
      "Epoch 660/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3198 - acc: 0.8984 - val_loss: 0.4011 - val_acc: 0.8688\n",
      "Epoch 661/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3185 - acc: 0.8991 - val_loss: 0.4032 - val_acc: 0.8608\n",
      "Epoch 662/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3181 - acc: 0.9004 - val_loss: 0.4006 - val_acc: 0.8608\n",
      "Epoch 663/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3176 - acc: 0.8984 - val_loss: 0.3993 - val_acc: 0.8628\n",
      "Epoch 664/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3179 - acc: 0.8977 - val_loss: 0.4017 - val_acc: 0.8588\n",
      "Epoch 665/1500\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3180 - acc: 0.8971 - val_loss: 0.4021 - val_acc: 0.8648\n",
      "Epoch 666/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3183 - acc: 0.8997 - val_loss: 0.4006 - val_acc: 0.8628\n",
      "Epoch 667/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3172 - acc: 0.9004 - val_loss: 0.3997 - val_acc: 0.8628\n",
      "Epoch 668/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3174 - acc: 0.9004 - val_loss: 0.4004 - val_acc: 0.8628\n",
      "Epoch 669/1500\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.3179 - acc: 0.8984 - val_loss: 0.4010 - val_acc: 0.8608\n",
      "Epoch 670/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3181 - acc: 0.8984 - val_loss: 0.4020 - val_acc: 0.8628\n",
      "Epoch 671/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3178 - acc: 0.8971 - val_loss: 0.3999 - val_acc: 0.8628\n",
      "Epoch 672/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3181 - acc: 0.8991 - val_loss: 0.4000 - val_acc: 0.8628\n",
      "Epoch 673/1500\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3177 - acc: 0.9004 - val_loss: 0.4008 - val_acc: 0.8648\n",
      "Epoch 674/1500\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3180 - acc: 0.9017 - val_loss: 0.3997 - val_acc: 0.8648\n",
      "Epoch 675/1500\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3168 - acc: 0.9004 - val_loss: 0.3999 - val_acc: 0.8569\n",
      "Epoch 676/1500\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3165 - acc: 0.9011 - val_loss: 0.3991 - val_acc: 0.8648\n",
      "Epoch 677/1500\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3180 - acc: 0.8991 - val_loss: 0.4024 - val_acc: 0.8648\n",
      "Epoch 678/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3177 - acc: 0.8977 - val_loss: 0.4000 - val_acc: 0.8628\n",
      "Epoch 679/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3156 - acc: 0.9017 - val_loss: 0.4008 - val_acc: 0.8648\n",
      "Epoch 680/1500\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3169 - acc: 0.8991 - val_loss: 0.3969 - val_acc: 0.8708\n",
      "Epoch 681/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3181 - acc: 0.8997 - val_loss: 0.4022 - val_acc: 0.8668\n",
      "Epoch 682/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3161 - acc: 0.8984 - val_loss: 0.4009 - val_acc: 0.8668\n",
      "Epoch 683/1500\n",
      "1506/1506 [==============================] - 0s 50us/step - loss: 0.3192 - acc: 0.8997 - val_loss: 0.4007 - val_acc: 0.8668\n",
      "Epoch 684/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3168 - acc: 0.9004 - val_loss: 0.3999 - val_acc: 0.8648\n",
      "Epoch 685/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3156 - acc: 0.9017 - val_loss: 0.4023 - val_acc: 0.8668\n",
      "Epoch 686/1500\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3157 - acc: 0.9017 - val_loss: 0.3980 - val_acc: 0.8648\n",
      "Epoch 687/1500\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3160 - acc: 0.9017 - val_loss: 0.3988 - val_acc: 0.8648\n",
      "Epoch 688/1500\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3151 - acc: 0.9011 - val_loss: 0.3991 - val_acc: 0.8628\n",
      "Epoch 689/1500\n",
      "1506/1506 [==============================] - 0s 54us/step - loss: 0.3156 - acc: 0.8984 - val_loss: 0.3995 - val_acc: 0.8648\n",
      "Epoch 690/1500\n",
      "1506/1506 [==============================] - 0s 51us/step - loss: 0.3155 - acc: 0.8997 - val_loss: 0.3987 - val_acc: 0.8608\n",
      "Epoch 691/1500\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3245 - acc: 0.8997 - val_loss: 0.3979 - val_acc: 0.8648\n",
      "Epoch 692/1500\n",
      "1506/1506 [==============================] - 0s 42us/step - loss: 0.3144 - acc: 0.9011 - val_loss: 0.3984 - val_acc: 0.8628\n",
      "Epoch 693/1500\n",
      "1506/1506 [==============================] - 0s 45us/step - loss: 0.3146 - acc: 0.9011 - val_loss: 0.3979 - val_acc: 0.8668\n",
      "Epoch 694/1500\n",
      "1506/1506 [==============================] - 0s 45us/step - loss: 0.3142 - acc: 0.9031 - val_loss: 0.3985 - val_acc: 0.8668\n",
      "Epoch 695/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3158 - acc: 0.8984 - val_loss: 0.3981 - val_acc: 0.8628\n",
      "Epoch 696/1500\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3147 - acc: 0.8997 - val_loss: 0.3984 - val_acc: 0.8628\n",
      "Epoch 697/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3164 - acc: 0.9011 - val_loss: 0.3970 - val_acc: 0.8648\n",
      "Epoch 698/1500\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3147 - acc: 0.8984 - val_loss: 0.3977 - val_acc: 0.8668\n",
      "Epoch 699/1500\n",
      "1506/1506 [==============================] - 0s 47us/step - loss: 0.3139 - acc: 0.9024 - val_loss: 0.3981 - val_acc: 0.8608\n",
      "Epoch 700/1500\n",
      "1506/1506 [==============================] - 0s 52us/step - loss: 0.3141 - acc: 0.9004 - val_loss: 0.3980 - val_acc: 0.8608\n",
      "Epoch 00700: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(29, input_shape=(dims,)))\n",
    "m.add(Activation('softmax'))\n",
    "m.add(Dense(20))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adam,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1500, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/3000\n",
      "1506/1506 [==============================] - 1s 382us/step - loss: 2.2328 - acc: 0.1965 - val_loss: 2.0942 - val_acc: 0.2366\n",
      "Epoch 2/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 2.0881 - acc: 0.3347 - val_loss: 1.9732 - val_acc: 0.4414\n",
      "Epoch 3/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.9413 - acc: 0.3958 - val_loss: 1.7977 - val_acc: 0.4414\n",
      "Epoch 4/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 1.7553 - acc: 0.3958 - val_loss: 1.6457 - val_acc: 0.4414\n",
      "Epoch 5/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.6142 - acc: 0.3964 - val_loss: 1.5659 - val_acc: 0.4433\n",
      "Epoch 6/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.5161 - acc: 0.4031 - val_loss: 1.4767 - val_acc: 0.4533\n",
      "Epoch 7/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.4269 - acc: 0.4442 - val_loss: 1.4017 - val_acc: 0.5149\n",
      "Epoch 8/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.3416 - acc: 0.5465 - val_loss: 1.3206 - val_acc: 0.5765\n",
      "Epoch 9/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 1.2492 - acc: 0.5890 - val_loss: 1.2356 - val_acc: 0.6083\n",
      "Epoch 10/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 1.1627 - acc: 0.6308 - val_loss: 1.1563 - val_acc: 0.7296\n",
      "Epoch 11/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.0659 - acc: 0.6979 - val_loss: 1.0764 - val_acc: 0.7416\n",
      "Epoch 12/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.9824 - acc: 0.7297 - val_loss: 0.9964 - val_acc: 0.6481\n",
      "Epoch 13/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9028 - acc: 0.7517 - val_loss: 0.9121 - val_acc: 0.7714\n",
      "Epoch 14/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.8259 - acc: 0.7842 - val_loss: 0.8254 - val_acc: 0.7952\n",
      "Epoch 15/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7555 - acc: 0.8054 - val_loss: 0.7780 - val_acc: 0.7535\n",
      "Epoch 16/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7033 - acc: 0.8068 - val_loss: 0.7357 - val_acc: 0.7773\n",
      "Epoch 17/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6626 - acc: 0.8161 - val_loss: 0.7025 - val_acc: 0.8131\n",
      "Epoch 18/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.6299 - acc: 0.8373 - val_loss: 0.6812 - val_acc: 0.8231\n",
      "Epoch 19/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.6166 - acc: 0.8307 - val_loss: 0.6459 - val_acc: 0.8231\n",
      "Epoch 20/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5777 - acc: 0.8486 - val_loss: 0.6246 - val_acc: 0.8310\n",
      "Epoch 21/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5538 - acc: 0.8572 - val_loss: 0.6315 - val_acc: 0.8091\n",
      "Epoch 22/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5445 - acc: 0.8572 - val_loss: 0.5927 - val_acc: 0.8529\n",
      "Epoch 23/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5253 - acc: 0.8639 - val_loss: 0.5791 - val_acc: 0.8330\n",
      "Epoch 24/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5153 - acc: 0.8606 - val_loss: 0.5664 - val_acc: 0.8370\n",
      "Epoch 25/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4997 - acc: 0.8732 - val_loss: 0.6308 - val_acc: 0.8290\n",
      "Epoch 26/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.5123 - acc: 0.864 - 0s 64us/step - loss: 0.5160 - acc: 0.8679 - val_loss: 0.5541 - val_acc: 0.8390\n",
      "Epoch 27/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4890 - acc: 0.8725 - val_loss: 0.5558 - val_acc: 0.8410\n",
      "Epoch 28/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4779 - acc: 0.8732 - val_loss: 0.5718 - val_acc: 0.8350\n",
      "Epoch 29/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4682 - acc: 0.8725 - val_loss: 0.5366 - val_acc: 0.8529\n",
      "Epoch 30/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4654 - acc: 0.8772 - val_loss: 0.5580 - val_acc: 0.8310\n",
      "Epoch 31/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4643 - acc: 0.8738 - val_loss: 0.5167 - val_acc: 0.8529\n",
      "Epoch 32/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4484 - acc: 0.8798 - val_loss: 0.5541 - val_acc: 0.8529\n",
      "Epoch 33/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4528 - acc: 0.8785 - val_loss: 0.5075 - val_acc: 0.8628\n",
      "Epoch 34/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4447 - acc: 0.8778 - val_loss: 0.5178 - val_acc: 0.8608\n",
      "Epoch 35/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4317 - acc: 0.8831 - val_loss: 0.5928 - val_acc: 0.8171\n",
      "Epoch 36/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4427 - acc: 0.8752 - val_loss: 0.5260 - val_acc: 0.8370\n",
      "Epoch 37/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4301 - acc: 0.8805 - val_loss: 0.4908 - val_acc: 0.8509\n",
      "Epoch 38/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4275 - acc: 0.8811 - val_loss: 0.5181 - val_acc: 0.8390\n",
      "Epoch 39/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4198 - acc: 0.8805 - val_loss: 0.4934 - val_acc: 0.8529\n",
      "Epoch 40/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4134 - acc: 0.8904 - val_loss: 0.4854 - val_acc: 0.8588\n",
      "Epoch 41/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4101 - acc: 0.8845 - val_loss: 0.4841 - val_acc: 0.8549\n",
      "Epoch 42/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4036 - acc: 0.8891 - val_loss: 0.4726 - val_acc: 0.8489\n",
      "Epoch 43/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4074 - acc: 0.8811 - val_loss: 0.4694 - val_acc: 0.8608\n",
      "Epoch 44/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4288 - acc: 0.879 - 0s 64us/step - loss: 0.4065 - acc: 0.8865 - val_loss: 0.4946 - val_acc: 0.8588\n",
      "Epoch 45/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.4050 - acc: 0.8891 - val_loss: 0.4625 - val_acc: 0.8628\n",
      "Epoch 46/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3967 - acc: 0.8831 - val_loss: 0.4983 - val_acc: 0.8489\n",
      "Epoch 47/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4032 - acc: 0.8831 - val_loss: 0.4791 - val_acc: 0.8549\n",
      "Epoch 48/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3880 - acc: 0.8904 - val_loss: 0.5135 - val_acc: 0.8171\n",
      "Epoch 49/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4008 - acc: 0.8785 - val_loss: 0.5280 - val_acc: 0.8410\n",
      "Epoch 50/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4005 - acc: 0.8845 - val_loss: 0.4951 - val_acc: 0.8529\n",
      "Epoch 51/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4052 - acc: 0.8845 - val_loss: 0.4893 - val_acc: 0.8509\n",
      "Epoch 52/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3912 - acc: 0.8904 - val_loss: 0.4577 - val_acc: 0.8549\n",
      "Epoch 53/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4021 - acc: 0.883 - 0s 58us/step - loss: 0.3886 - acc: 0.8891 - val_loss: 0.4431 - val_acc: 0.8549\n",
      "Epoch 54/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3739 - acc: 0.8904 - val_loss: 0.5883 - val_acc: 0.8191\n",
      "Epoch 55/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4164 - acc: 0.8758 - val_loss: 0.4406 - val_acc: 0.8648\n",
      "Epoch 56/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3765 - acc: 0.8871 - val_loss: 0.4905 - val_acc: 0.8410\n",
      "Epoch 57/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3777 - acc: 0.8904 - val_loss: 0.4599 - val_acc: 0.8489\n",
      "Epoch 58/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3826 - acc: 0.8858 - val_loss: 0.4425 - val_acc: 0.8608\n",
      "Epoch 59/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3761 - acc: 0.8871 - val_loss: 0.4345 - val_acc: 0.8529\n",
      "Epoch 60/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3710 - acc: 0.8845 - val_loss: 0.4488 - val_acc: 0.8608\n",
      "Epoch 61/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3669 - acc: 0.8904 - val_loss: 0.6222 - val_acc: 0.8171\n",
      "Epoch 62/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3989 - acc: 0.8811 - val_loss: 0.4697 - val_acc: 0.8549\n",
      "Epoch 63/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3614 - acc: 0.8898 - val_loss: 0.4645 - val_acc: 0.8509\n",
      "Epoch 64/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3668 - acc: 0.8891 - val_loss: 0.4562 - val_acc: 0.8569\n",
      "Epoch 65/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3649 - acc: 0.8918 - val_loss: 0.4282 - val_acc: 0.8668\n",
      "Epoch 66/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3636 - acc: 0.8898 - val_loss: 0.4710 - val_acc: 0.8588\n",
      "Epoch 67/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3711 - acc: 0.8944 - val_loss: 0.4728 - val_acc: 0.8588\n",
      "Epoch 68/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3730 - acc: 0.8884 - val_loss: 0.4558 - val_acc: 0.8569\n",
      "Epoch 69/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3676 - acc: 0.8858 - val_loss: 0.4560 - val_acc: 0.8489\n",
      "Epoch 70/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3530 - acc: 0.8964 - val_loss: 0.4346 - val_acc: 0.8588\n",
      "Epoch 71/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3558 - acc: 0.8891 - val_loss: 0.4371 - val_acc: 0.8529\n",
      "Epoch 72/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3541 - acc: 0.8898 - val_loss: 0.4372 - val_acc: 0.8708\n",
      "Epoch 73/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3603 - acc: 0.8884 - val_loss: 0.4283 - val_acc: 0.8628\n",
      "Epoch 74/3000\n",
      "1506/1506 [==============================] - 0s 120us/step - loss: 0.3532 - acc: 0.8958 - val_loss: 0.4253 - val_acc: 0.8608\n",
      "Epoch 75/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3563 - acc: 0.8898 - val_loss: 0.8942 - val_acc: 0.6680\n",
      "Epoch 76/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.4487 - acc: 0.8758 - val_loss: 0.4400 - val_acc: 0.8549\n",
      "Epoch 77/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3598 - acc: 0.8898 - val_loss: 0.4776 - val_acc: 0.8469\n",
      "Epoch 78/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3601 - acc: 0.8904 - val_loss: 0.4451 - val_acc: 0.8728\n",
      "Epoch 79/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3600 - acc: 0.8931 - val_loss: 0.4256 - val_acc: 0.8628\n",
      "Epoch 80/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3556 - acc: 0.8964 - val_loss: 0.4263 - val_acc: 0.8767\n",
      "Epoch 81/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3531 - acc: 0.8924 - val_loss: 0.4343 - val_acc: 0.8628\n",
      "Epoch 82/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3533 - acc: 0.8944 - val_loss: 0.4566 - val_acc: 0.8668\n",
      "Epoch 83/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3452 - acc: 0.8911 - val_loss: 0.5265 - val_acc: 0.8449\n",
      "Epoch 84/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3748 - acc: 0.8825 - val_loss: 0.4251 - val_acc: 0.8549\n",
      "Epoch 85/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3448 - acc: 0.8918 - val_loss: 0.4304 - val_acc: 0.8569\n",
      "Epoch 86/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3418 - acc: 0.8911 - val_loss: 0.4260 - val_acc: 0.8608\n",
      "Epoch 87/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3454 - acc: 0.8951 - val_loss: 0.4288 - val_acc: 0.8688\n",
      "Epoch 88/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3442 - acc: 0.8964 - val_loss: 0.4207 - val_acc: 0.8648\n",
      "Epoch 89/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3386 - acc: 0.9011 - val_loss: 0.4348 - val_acc: 0.8688\n",
      "Epoch 90/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3354 - acc: 0.8971 - val_loss: 0.4509 - val_acc: 0.8668\n",
      "Epoch 91/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3488 - acc: 0.8958 - val_loss: 0.4007 - val_acc: 0.8608\n",
      "Epoch 92/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3383 - acc: 0.9004 - val_loss: 0.4002 - val_acc: 0.8588\n",
      "Epoch 93/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3297 - acc: 0.8971 - val_loss: 0.4633 - val_acc: 0.8708\n",
      "Epoch 94/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3405 - acc: 0.8938 - val_loss: 0.4426 - val_acc: 0.8608\n",
      "Epoch 95/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3403 - acc: 0.8997 - val_loss: 0.4090 - val_acc: 0.8628\n",
      "Epoch 96/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3387 - acc: 0.8951 - val_loss: 0.4255 - val_acc: 0.8569\n",
      "Epoch 97/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3340 - acc: 0.8984 - val_loss: 0.4324 - val_acc: 0.8688\n",
      "Epoch 98/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3365 - acc: 0.8997 - val_loss: 0.4237 - val_acc: 0.8588\n",
      "Epoch 99/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3309 - acc: 0.9017 - val_loss: 0.4170 - val_acc: 0.8628\n",
      "Epoch 100/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3322 - acc: 0.9011 - val_loss: 0.7053 - val_acc: 0.8091\n",
      "Epoch 101/3000\n",
      "1506/1506 [==============================] - 0s 127us/step - loss: 0.3683 - acc: 0.8838 - val_loss: 0.4060 - val_acc: 0.8728\n",
      "Epoch 102/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3405 - acc: 0.8977 - val_loss: 0.4051 - val_acc: 0.8728\n",
      "Epoch 103/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3219 - acc: 0.8991 - val_loss: 0.4103 - val_acc: 0.8608\n",
      "Epoch 104/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3338 - acc: 0.8958 - val_loss: 0.4029 - val_acc: 0.8728\n",
      "Epoch 105/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3220 - acc: 0.9017 - val_loss: 0.4157 - val_acc: 0.8588\n",
      "Epoch 106/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3259 - acc: 0.8977 - val_loss: 0.4470 - val_acc: 0.8728\n",
      "Epoch 107/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3448 - acc: 0.8984 - val_loss: 0.4132 - val_acc: 0.8668\n",
      "Epoch 108/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3262 - acc: 0.9044 - val_loss: 0.3775 - val_acc: 0.8787\n",
      "Epoch 109/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3165 - acc: 0.9037 - val_loss: 0.4061 - val_acc: 0.8588\n",
      "Epoch 110/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3277 - acc: 0.8997 - val_loss: 0.3852 - val_acc: 0.8748\n",
      "Epoch 111/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3254 - acc: 0.8997 - val_loss: 0.4081 - val_acc: 0.8728\n",
      "Epoch 112/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3202 - acc: 0.9031 - val_loss: 0.4523 - val_acc: 0.8648\n",
      "Epoch 113/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3160 - acc: 0.9057 - val_loss: 0.3962 - val_acc: 0.8748\n",
      "Epoch 114/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3162 - acc: 0.9031 - val_loss: 0.4153 - val_acc: 0.8628\n",
      "Epoch 115/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3125 - acc: 0.9064 - val_loss: 0.4256 - val_acc: 0.8728\n",
      "Epoch 116/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3357 - acc: 0.8971 - val_loss: 0.3748 - val_acc: 0.8767\n",
      "Epoch 117/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3100 - acc: 0.9044 - val_loss: 0.4069 - val_acc: 0.8668\n",
      "Epoch 118/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3564 - acc: 0.8878 - val_loss: 0.4611 - val_acc: 0.8847\n",
      "Epoch 119/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3276 - acc: 0.9011 - val_loss: 0.4683 - val_acc: 0.8668\n",
      "Epoch 120/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3353 - acc: 0.8964 - val_loss: 0.3981 - val_acc: 0.8847\n",
      "Epoch 121/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.3207 - acc: 0.9024 - val_loss: 0.3805 - val_acc: 0.8807\n",
      "Epoch 122/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3145 - acc: 0.9077 - val_loss: 0.4009 - val_acc: 0.8827\n",
      "Epoch 123/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3081 - acc: 0.9090 - val_loss: 0.4715 - val_acc: 0.8708\n",
      "Epoch 124/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3273 - acc: 0.9044 - val_loss: 0.3932 - val_acc: 0.8767\n",
      "Epoch 125/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3082 - acc: 0.9050 - val_loss: 0.3518 - val_acc: 0.8748\n",
      "Epoch 126/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3040 - acc: 0.9050 - val_loss: 0.4107 - val_acc: 0.8767\n",
      "Epoch 127/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3150 - acc: 0.9011 - val_loss: 0.4937 - val_acc: 0.8648\n",
      "Epoch 128/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3479 - acc: 0.8958 - val_loss: 0.3924 - val_acc: 0.8688\n",
      "Epoch 129/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3248 - acc: 0.8958 - val_loss: 0.3920 - val_acc: 0.8787\n",
      "Epoch 130/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3047 - acc: 0.9097 - val_loss: 0.4919 - val_acc: 0.8469\n",
      "Epoch 131/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3274 - acc: 0.8971 - val_loss: 0.3965 - val_acc: 0.8767\n",
      "Epoch 132/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3176 - acc: 0.9037 - val_loss: 0.3806 - val_acc: 0.8748\n",
      "Epoch 133/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3110 - acc: 0.9037 - val_loss: 0.3936 - val_acc: 0.8787\n",
      "Epoch 134/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3068 - acc: 0.9004 - val_loss: 0.3671 - val_acc: 0.8827\n",
      "Epoch 135/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.2996 - acc: 0.9057 - val_loss: 0.3615 - val_acc: 0.8787\n",
      "Epoch 136/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3072 - acc: 0.9064 - val_loss: 0.4227 - val_acc: 0.8549\n",
      "Epoch 137/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3049 - acc: 0.9057 - val_loss: 0.4117 - val_acc: 0.8728\n",
      "Epoch 138/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3044 - acc: 0.9104 - val_loss: 0.3845 - val_acc: 0.8748\n",
      "Epoch 139/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3090 - acc: 0.9070 - val_loss: 0.3735 - val_acc: 0.8847\n",
      "Epoch 140/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3035 - acc: 0.9077 - val_loss: 0.3697 - val_acc: 0.8728\n",
      "Epoch 141/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2982 - acc: 0.9084 - val_loss: 2.0747 - val_acc: 0.4155\n",
      "Epoch 142/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3785 - acc: 0.8951 - val_loss: 0.3981 - val_acc: 0.8688\n",
      "Epoch 143/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3009 - acc: 0.8984 - val_loss: 0.4143 - val_acc: 0.8728\n",
      "Epoch 144/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3025 - acc: 0.9070 - val_loss: 0.3678 - val_acc: 0.8728\n",
      "Epoch 145/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2958 - acc: 0.9031 - val_loss: 0.3567 - val_acc: 0.8887\n",
      "Epoch 00145: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist2=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/3000\n",
      "1506/1506 [==============================] - 1s 410us/step - loss: 2.1823 - acc: 0.2058 - val_loss: 2.0858 - val_acc: 0.2425\n",
      "Epoch 2/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 2.0323 - acc: 0.2258 - val_loss: 1.9500 - val_acc: 0.3181\n",
      "Epoch 3/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.8939 - acc: 0.3533 - val_loss: 1.8112 - val_acc: 0.4433\n",
      "Epoch 4/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.7585 - acc: 0.4031 - val_loss: 1.6870 - val_acc: 0.4513\n",
      "Epoch 5/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.6428 - acc: 0.4084 - val_loss: 1.5909 - val_acc: 0.4592\n",
      "Epoch 6/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.5464 - acc: 0.4150 - val_loss: 1.5232 - val_acc: 0.4891\n",
      "Epoch 7/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.4695 - acc: 0.4323 - val_loss: 1.4533 - val_acc: 0.4911\n",
      "Epoch 8/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.4021 - acc: 0.4548 - val_loss: 1.3870 - val_acc: 0.4911\n",
      "Epoch 9/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 1.3387 - acc: 0.5013 - val_loss: 1.3321 - val_acc: 0.5388\n",
      "Epoch 10/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.2835 - acc: 0.5438 - val_loss: 1.2930 - val_acc: 0.5885\n",
      "Epoch 11/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.2276 - acc: 0.5996 - val_loss: 1.2277 - val_acc: 0.6143\n",
      "Epoch 12/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.1719 - acc: 0.6255 - val_loss: 1.1773 - val_acc: 0.6103\n",
      "Epoch 13/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 1.1184 - acc: 0.6567 - val_loss: 1.1251 - val_acc: 0.6282\n",
      "Epoch 14/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.0651 - acc: 0.6753 - val_loss: 1.0782 - val_acc: 0.6720\n",
      "Epoch 15/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 1.0144 - acc: 0.6992 - val_loss: 1.0377 - val_acc: 0.7018\n",
      "Epoch 16/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9685 - acc: 0.7191 - val_loss: 0.9930 - val_acc: 0.7237\n",
      "Epoch 17/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9233 - acc: 0.7297 - val_loss: 0.9530 - val_acc: 0.7495\n",
      "Epoch 18/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.8798 - acc: 0.7623 - val_loss: 0.9130 - val_acc: 0.7018\n",
      "Epoch 19/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.8382 - acc: 0.7656 - val_loss: 0.8718 - val_acc: 0.8390\n",
      "Epoch 20/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7987 - acc: 0.8167 - val_loss: 0.8490 - val_acc: 0.8330\n",
      "Epoch 21/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7653 - acc: 0.8327 - val_loss: 0.8067 - val_acc: 0.8171\n",
      "Epoch 22/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7334 - acc: 0.8426 - val_loss: 0.7778 - val_acc: 0.8151\n",
      "Epoch 23/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.7035 - acc: 0.8539 - val_loss: 0.7429 - val_acc: 0.8410\n",
      "Epoch 24/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6751 - acc: 0.8546 - val_loss: 0.7273 - val_acc: 0.8390\n",
      "Epoch 25/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.6499 - acc: 0.8572 - val_loss: 0.7044 - val_acc: 0.8390\n",
      "Epoch 26/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.6281 - acc: 0.8606 - val_loss: 0.6790 - val_acc: 0.8350\n",
      "Epoch 27/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.6065 - acc: 0.8632 - val_loss: 0.6674 - val_acc: 0.8370\n",
      "Epoch 28/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5901 - acc: 0.8679 - val_loss: 0.6463 - val_acc: 0.8370\n",
      "Epoch 29/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5724 - acc: 0.8659 - val_loss: 0.6310 - val_acc: 0.8529\n",
      "Epoch 30/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5578 - acc: 0.8732 - val_loss: 0.6244 - val_acc: 0.8429\n",
      "Epoch 31/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5442 - acc: 0.8745 - val_loss: 0.6039 - val_acc: 0.8489\n",
      "Epoch 32/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5352 - acc: 0.8732 - val_loss: 0.5893 - val_acc: 0.8449\n",
      "Epoch 33/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5222 - acc: 0.8758 - val_loss: 0.5810 - val_acc: 0.8449\n",
      "Epoch 34/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5146 - acc: 0.8758 - val_loss: 0.5731 - val_acc: 0.8509\n",
      "Epoch 35/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5055 - acc: 0.8792 - val_loss: 0.5641 - val_acc: 0.8469\n",
      "Epoch 36/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4962 - acc: 0.8752 - val_loss: 0.5567 - val_acc: 0.8529\n",
      "Epoch 37/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4869 - acc: 0.8818 - val_loss: 0.5503 - val_acc: 0.8370\n",
      "Epoch 38/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4812 - acc: 0.8811 - val_loss: 0.5523 - val_acc: 0.8469\n",
      "Epoch 39/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4761 - acc: 0.8818 - val_loss: 0.5460 - val_acc: 0.8549\n",
      "Epoch 40/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4696 - acc: 0.8818 - val_loss: 0.5321 - val_acc: 0.8410\n",
      "Epoch 41/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4652 - acc: 0.8805 - val_loss: 0.5317 - val_acc: 0.8569\n",
      "Epoch 42/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4582 - acc: 0.8798 - val_loss: 0.5132 - val_acc: 0.8429\n",
      "Epoch 43/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4541 - acc: 0.8811 - val_loss: 0.5162 - val_acc: 0.8529\n",
      "Epoch 44/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4495 - acc: 0.8805 - val_loss: 0.5205 - val_acc: 0.8549\n",
      "Epoch 45/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4448 - acc: 0.8865 - val_loss: 0.5089 - val_acc: 0.8549\n",
      "Epoch 46/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4415 - acc: 0.8838 - val_loss: 0.5026 - val_acc: 0.8509\n",
      "Epoch 47/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4376 - acc: 0.8818 - val_loss: 0.4979 - val_acc: 0.8429\n",
      "Epoch 48/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4330 - acc: 0.8845 - val_loss: 0.5022 - val_acc: 0.8569\n",
      "Epoch 49/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4308 - acc: 0.8865 - val_loss: 0.4952 - val_acc: 0.8529\n",
      "Epoch 50/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4262 - acc: 0.8845 - val_loss: 0.4923 - val_acc: 0.8549\n",
      "Epoch 51/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4249 - acc: 0.8891 - val_loss: 0.4903 - val_acc: 0.8489\n",
      "Epoch 52/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4217 - acc: 0.8838 - val_loss: 0.4854 - val_acc: 0.8549\n",
      "Epoch 53/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4181 - acc: 0.8825 - val_loss: 0.4878 - val_acc: 0.8509\n",
      "Epoch 54/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4160 - acc: 0.8851 - val_loss: 0.4879 - val_acc: 0.8588\n",
      "Epoch 55/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4129 - acc: 0.8865 - val_loss: 0.4790 - val_acc: 0.8549\n",
      "Epoch 56/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4104 - acc: 0.8858 - val_loss: 0.4757 - val_acc: 0.8509\n",
      "Epoch 57/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4082 - acc: 0.8878 - val_loss: 0.4717 - val_acc: 0.8529\n",
      "Epoch 58/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4058 - acc: 0.8924 - val_loss: 0.4677 - val_acc: 0.8509\n",
      "Epoch 59/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4038 - acc: 0.8865 - val_loss: 0.4705 - val_acc: 0.8489\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4007 - acc: 0.8891 - val_loss: 0.4705 - val_acc: 0.8628\n",
      "Epoch 61/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3994 - acc: 0.8898 - val_loss: 0.4646 - val_acc: 0.8549\n",
      "Epoch 62/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3980 - acc: 0.8891 - val_loss: 0.4797 - val_acc: 0.8549\n",
      "Epoch 63/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3960 - acc: 0.8931 - val_loss: 0.4601 - val_acc: 0.8608\n",
      "Epoch 64/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.3925 - acc: 0.8878 - val_loss: 0.4646 - val_acc: 0.8648\n",
      "Epoch 65/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3916 - acc: 0.8911 - val_loss: 0.4644 - val_acc: 0.8588\n",
      "Epoch 66/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3910 - acc: 0.8904 - val_loss: 0.4557 - val_acc: 0.8608\n",
      "Epoch 67/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3874 - acc: 0.8924 - val_loss: 0.4567 - val_acc: 0.8688\n",
      "Epoch 68/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3872 - acc: 0.8944 - val_loss: 0.4509 - val_acc: 0.8588\n",
      "Epoch 69/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3863 - acc: 0.8918 - val_loss: 0.4528 - val_acc: 0.8588\n",
      "Epoch 70/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3842 - acc: 0.8924 - val_loss: 0.4587 - val_acc: 0.8668\n",
      "Epoch 71/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3818 - acc: 0.8918 - val_loss: 0.4455 - val_acc: 0.8628\n",
      "Epoch 72/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3798 - acc: 0.8884 - val_loss: 0.4742 - val_acc: 0.8708\n",
      "Epoch 73/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3803 - acc: 0.8931 - val_loss: 0.4462 - val_acc: 0.8648\n",
      "Epoch 74/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3773 - acc: 0.8931 - val_loss: 0.4469 - val_acc: 0.8628\n",
      "Epoch 75/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3766 - acc: 0.8924 - val_loss: 0.4591 - val_acc: 0.8608\n",
      "Epoch 76/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3759 - acc: 0.8904 - val_loss: 0.4474 - val_acc: 0.8608\n",
      "Epoch 77/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3734 - acc: 0.8924 - val_loss: 0.4455 - val_acc: 0.8628\n",
      "Epoch 78/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3713 - acc: 0.8944 - val_loss: 0.4432 - val_acc: 0.8588\n",
      "Epoch 79/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3703 - acc: 0.8951 - val_loss: 0.4413 - val_acc: 0.8628\n",
      "Epoch 80/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3697 - acc: 0.8911 - val_loss: 0.4447 - val_acc: 0.8708\n",
      "Epoch 81/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3710 - acc: 0.8958 - val_loss: 0.4375 - val_acc: 0.8668\n",
      "Epoch 82/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3676 - acc: 0.8977 - val_loss: 0.4618 - val_acc: 0.8648\n",
      "Epoch 83/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3678 - acc: 0.8944 - val_loss: 0.4660 - val_acc: 0.8648\n",
      "Epoch 84/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3644 - acc: 0.9004 - val_loss: 0.4441 - val_acc: 0.8648\n",
      "Epoch 85/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.3663 - acc: 0.8971 - val_loss: 0.4350 - val_acc: 0.8628\n",
      "Epoch 86/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3624 - acc: 0.8944 - val_loss: 0.4432 - val_acc: 0.8708\n",
      "Epoch 87/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3616 - acc: 0.8984 - val_loss: 0.4324 - val_acc: 0.8708\n",
      "Epoch 88/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3612 - acc: 0.8971 - val_loss: 0.4404 - val_acc: 0.8588\n",
      "Epoch 89/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3602 - acc: 0.8991 - val_loss: 0.4280 - val_acc: 0.8648\n",
      "Epoch 90/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3574 - acc: 0.8938 - val_loss: 0.4326 - val_acc: 0.8648\n",
      "Epoch 91/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3579 - acc: 0.8984 - val_loss: 0.4334 - val_acc: 0.8628\n",
      "Epoch 92/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3567 - acc: 0.8971 - val_loss: 0.4361 - val_acc: 0.8688\n",
      "Epoch 93/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3538 - acc: 0.9004 - val_loss: 0.4282 - val_acc: 0.8668\n",
      "Epoch 94/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3532 - acc: 0.9004 - val_loss: 0.4345 - val_acc: 0.8608\n",
      "Epoch 95/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3540 - acc: 0.8991 - val_loss: 0.4293 - val_acc: 0.8767\n",
      "Epoch 96/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3524 - acc: 0.8991 - val_loss: 0.4236 - val_acc: 0.8668\n",
      "Epoch 97/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3515 - acc: 0.8958 - val_loss: 0.4208 - val_acc: 0.8708\n",
      "Epoch 98/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3491 - acc: 0.9011 - val_loss: 0.4255 - val_acc: 0.8708\n",
      "Epoch 99/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3488 - acc: 0.8971 - val_loss: 0.4237 - val_acc: 0.8628\n",
      "Epoch 100/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3482 - acc: 0.8958 - val_loss: 0.4218 - val_acc: 0.8648\n",
      "Epoch 101/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3477 - acc: 0.8971 - val_loss: 0.4309 - val_acc: 0.8648\n",
      "Epoch 102/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3469 - acc: 0.8951 - val_loss: 0.4187 - val_acc: 0.8608\n",
      "Epoch 103/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3480 - acc: 0.8984 - val_loss: 0.4230 - val_acc: 0.8668\n",
      "Epoch 104/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3436 - acc: 0.8971 - val_loss: 0.4156 - val_acc: 0.8708\n",
      "Epoch 105/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3424 - acc: 0.9004 - val_loss: 0.4242 - val_acc: 0.8708\n",
      "Epoch 106/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3436 - acc: 0.8977 - val_loss: 0.4210 - val_acc: 0.8688\n",
      "Epoch 107/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3403 - acc: 0.8964 - val_loss: 0.4250 - val_acc: 0.8688\n",
      "Epoch 108/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3435 - acc: 0.9011 - val_loss: 0.4132 - val_acc: 0.8728\n",
      "Epoch 109/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3401 - acc: 0.8977 - val_loss: 0.4243 - val_acc: 0.8648\n",
      "Epoch 110/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3425 - acc: 0.8977 - val_loss: 0.4145 - val_acc: 0.8728\n",
      "Epoch 111/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3391 - acc: 0.9031 - val_loss: 0.4118 - val_acc: 0.8708\n",
      "Epoch 112/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3383 - acc: 0.8991 - val_loss: 0.4167 - val_acc: 0.8728\n",
      "Epoch 113/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3370 - acc: 0.9017 - val_loss: 0.4149 - val_acc: 0.8628\n",
      "Epoch 114/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3371 - acc: 0.8991 - val_loss: 0.4105 - val_acc: 0.8688\n",
      "Epoch 115/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3373 - acc: 0.8997 - val_loss: 0.4102 - val_acc: 0.8688\n",
      "Epoch 116/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3376 - acc: 0.8991 - val_loss: 0.4035 - val_acc: 0.8668\n",
      "Epoch 117/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3345 - acc: 0.9011 - val_loss: 0.4148 - val_acc: 0.8688\n",
      "Epoch 118/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3363 - acc: 0.8971 - val_loss: 0.4029 - val_acc: 0.8688\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3333 - acc: 0.9004 - val_loss: 0.4114 - val_acc: 0.8688\n",
      "Epoch 120/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.3342 - acc: 0.8997 - val_loss: 0.4128 - val_acc: 0.8708\n",
      "Epoch 121/3000\n",
      "1506/1506 [==============================] - 0s 56us/step - loss: 0.3326 - acc: 0.8984 - val_loss: 0.4075 - val_acc: 0.8708\n",
      "Epoch 122/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3292 - acc: 0.9011 - val_loss: 0.4165 - val_acc: 0.8708\n",
      "Epoch 123/3000\n",
      "1506/1506 [==============================] - 0s 60us/step - loss: 0.3305 - acc: 0.9011 - val_loss: 0.4076 - val_acc: 0.8628\n",
      "Epoch 124/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3316 - acc: 0.8997 - val_loss: 0.4028 - val_acc: 0.8648\n",
      "Epoch 125/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3311 - acc: 0.8971 - val_loss: 0.4019 - val_acc: 0.8708\n",
      "Epoch 126/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3281 - acc: 0.9024 - val_loss: 0.4019 - val_acc: 0.8688\n",
      "Epoch 127/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3270 - acc: 0.9024 - val_loss: 0.4032 - val_acc: 0.8748\n",
      "Epoch 128/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3255 - acc: 0.9031 - val_loss: 0.4090 - val_acc: 0.8588\n",
      "Epoch 129/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3268 - acc: 0.9011 - val_loss: 0.3945 - val_acc: 0.8767\n",
      "Epoch 130/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3254 - acc: 0.9017 - val_loss: 0.4035 - val_acc: 0.8787\n",
      "Epoch 131/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3224 - acc: 0.9037 - val_loss: 0.4203 - val_acc: 0.8668\n",
      "Epoch 132/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3262 - acc: 0.9004 - val_loss: 0.4016 - val_acc: 0.8608\n",
      "Epoch 133/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3236 - acc: 0.8991 - val_loss: 0.3946 - val_acc: 0.8708\n",
      "Epoch 134/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3230 - acc: 0.9011 - val_loss: 0.3978 - val_acc: 0.8708\n",
      "Epoch 135/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.3220 - acc: 0.9024 - val_loss: 0.3964 - val_acc: 0.8688\n",
      "Epoch 136/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3222 - acc: 0.9017 - val_loss: 0.3999 - val_acc: 0.8767\n",
      "Epoch 137/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3207 - acc: 0.9037 - val_loss: 0.3985 - val_acc: 0.8708\n",
      "Epoch 138/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3209 - acc: 0.9017 - val_loss: 0.3993 - val_acc: 0.8648\n",
      "Epoch 139/3000\n",
      "1506/1506 [==============================] - 0s 89us/step - loss: 0.3195 - acc: 0.9017 - val_loss: 0.3984 - val_acc: 0.8608\n",
      "Epoch 140/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3201 - acc: 0.8991 - val_loss: 0.3987 - val_acc: 0.8708\n",
      "Epoch 141/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3186 - acc: 0.9044 - val_loss: 0.3899 - val_acc: 0.8748\n",
      "Epoch 142/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3190 - acc: 0.9037 - val_loss: 0.4011 - val_acc: 0.8688\n",
      "Epoch 143/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3176 - acc: 0.9011 - val_loss: 0.3877 - val_acc: 0.8708\n",
      "Epoch 144/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3172 - acc: 0.8997 - val_loss: 0.3913 - val_acc: 0.8708\n",
      "Epoch 145/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3167 - acc: 0.9037 - val_loss: 0.3859 - val_acc: 0.8748\n",
      "Epoch 146/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3163 - acc: 0.9037 - val_loss: 0.3963 - val_acc: 0.8748\n",
      "Epoch 147/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3148 - acc: 0.9037 - val_loss: 0.3841 - val_acc: 0.8748\n",
      "Epoch 148/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3147 - acc: 0.9037 - val_loss: 0.3934 - val_acc: 0.8728\n",
      "Epoch 149/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3132 - acc: 0.9044 - val_loss: 0.3905 - val_acc: 0.8728\n",
      "Epoch 150/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3132 - acc: 0.9044 - val_loss: 0.3892 - val_acc: 0.8628\n",
      "Epoch 151/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3128 - acc: 0.9044 - val_loss: 0.3806 - val_acc: 0.8668\n",
      "Epoch 152/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3094 - acc: 0.9064 - val_loss: 0.3878 - val_acc: 0.8688\n",
      "Epoch 153/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3130 - acc: 0.9037 - val_loss: 0.3819 - val_acc: 0.8807\n",
      "Epoch 154/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3094 - acc: 0.9057 - val_loss: 0.3930 - val_acc: 0.8668\n",
      "Epoch 155/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3113 - acc: 0.9017 - val_loss: 0.4039 - val_acc: 0.8728\n",
      "Epoch 156/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3104 - acc: 0.9057 - val_loss: 0.3946 - val_acc: 0.8748\n",
      "Epoch 157/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3093 - acc: 0.9024 - val_loss: 0.3757 - val_acc: 0.8787\n",
      "Epoch 158/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3082 - acc: 0.9044 - val_loss: 0.3830 - val_acc: 0.8767\n",
      "Epoch 159/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3243 - acc: 0.898 - 0s 66us/step - loss: 0.3055 - acc: 0.9037 - val_loss: 0.3876 - val_acc: 0.8748\n",
      "Epoch 160/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3055 - acc: 0.9024 - val_loss: 0.3781 - val_acc: 0.8827\n",
      "Epoch 161/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3077 - acc: 0.9064 - val_loss: 0.3854 - val_acc: 0.8767\n",
      "Epoch 162/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3026 - acc: 0.9070 - val_loss: 0.3830 - val_acc: 0.8708\n",
      "Epoch 163/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3054 - acc: 0.9064 - val_loss: 0.3980 - val_acc: 0.8628\n",
      "Epoch 164/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3071 - acc: 0.9050 - val_loss: 0.3775 - val_acc: 0.8748\n",
      "Epoch 165/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3055 - acc: 0.9070 - val_loss: 0.4026 - val_acc: 0.8748\n",
      "Epoch 166/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3065 - acc: 0.9057 - val_loss: 0.3816 - val_acc: 0.8728\n",
      "Epoch 167/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3033 - acc: 0.9077 - val_loss: 0.3818 - val_acc: 0.8708\n",
      "Epoch 168/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3041 - acc: 0.9070 - val_loss: 0.3784 - val_acc: 0.8748\n",
      "Epoch 169/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3009 - acc: 0.9057 - val_loss: 0.3757 - val_acc: 0.8748\n",
      "Epoch 170/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3009 - acc: 0.9050 - val_loss: 0.4125 - val_acc: 0.8807\n",
      "Epoch 171/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3026 - acc: 0.9090 - val_loss: 0.3800 - val_acc: 0.8767\n",
      "Epoch 172/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3002 - acc: 0.9064 - val_loss: 0.3735 - val_acc: 0.8748\n",
      "Epoch 173/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3002 - acc: 0.9057 - val_loss: 0.3784 - val_acc: 0.8767\n",
      "Epoch 174/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2993 - acc: 0.9050 - val_loss: 0.3774 - val_acc: 0.8728\n",
      "Epoch 175/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3000 - acc: 0.9044 - val_loss: 0.3754 - val_acc: 0.8708\n",
      "Epoch 176/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2973 - acc: 0.9084 - val_loss: 0.3822 - val_acc: 0.8728\n",
      "Epoch 177/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2970 - acc: 0.9064 - val_loss: 0.3713 - val_acc: 0.8767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2969 - acc: 0.9097 - val_loss: 0.3649 - val_acc: 0.8827\n",
      "Epoch 179/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2974 - acc: 0.9077 - val_loss: 0.3656 - val_acc: 0.8708\n",
      "Epoch 180/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2947 - acc: 0.9070 - val_loss: 0.3785 - val_acc: 0.8787\n",
      "Epoch 181/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2966 - acc: 0.9064 - val_loss: 0.3734 - val_acc: 0.8708\n",
      "Epoch 182/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2963 - acc: 0.9090 - val_loss: 0.3680 - val_acc: 0.8787\n",
      "Epoch 183/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2949 - acc: 0.9097 - val_loss: 0.3737 - val_acc: 0.8787\n",
      "Epoch 184/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2939 - acc: 0.9117 - val_loss: 0.3645 - val_acc: 0.8787\n",
      "Epoch 185/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2939 - acc: 0.9057 - val_loss: 0.3691 - val_acc: 0.8767\n",
      "Epoch 186/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.2941 - acc: 0.9077 - val_loss: 0.3664 - val_acc: 0.8708\n",
      "Epoch 187/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2909 - acc: 0.9077 - val_loss: 0.3654 - val_acc: 0.8787\n",
      "Epoch 188/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2918 - acc: 0.9137 - val_loss: 0.3779 - val_acc: 0.8767\n",
      "Epoch 189/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2948 - acc: 0.9084 - val_loss: 0.3626 - val_acc: 0.8767\n",
      "Epoch 190/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2901 - acc: 0.9117 - val_loss: 0.3707 - val_acc: 0.8748\n",
      "Epoch 191/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2898 - acc: 0.9110 - val_loss: 0.3599 - val_acc: 0.8748\n",
      "Epoch 192/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2908 - acc: 0.9090 - val_loss: 0.3636 - val_acc: 0.8807\n",
      "Epoch 193/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2892 - acc: 0.9097 - val_loss: 0.3733 - val_acc: 0.8767\n",
      "Epoch 194/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2911 - acc: 0.9070 - val_loss: 0.3814 - val_acc: 0.8648\n",
      "Epoch 195/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2882 - acc: 0.9104 - val_loss: 0.3615 - val_acc: 0.8807\n",
      "Epoch 196/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2863 - acc: 0.9077 - val_loss: 0.3796 - val_acc: 0.8648\n",
      "Epoch 197/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2866 - acc: 0.9090 - val_loss: 0.3684 - val_acc: 0.8728\n",
      "Epoch 198/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2872 - acc: 0.9050 - val_loss: 0.3581 - val_acc: 0.8827\n",
      "Epoch 199/3000\n",
      "1506/1506 [==============================] - 0s 117us/step - loss: 0.2878 - acc: 0.9117 - val_loss: 0.3531 - val_acc: 0.8827\n",
      "Epoch 200/3000\n",
      "1506/1506 [==============================] - 0s 133us/step - loss: 0.2855 - acc: 0.9104 - val_loss: 0.3766 - val_acc: 0.8767\n",
      "Epoch 201/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.2858 - acc: 0.9097 - val_loss: 0.3668 - val_acc: 0.8887\n",
      "Epoch 202/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.2839 - acc: 0.9117 - val_loss: 0.3598 - val_acc: 0.8787\n",
      "Epoch 203/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2848 - acc: 0.9110 - val_loss: 0.3623 - val_acc: 0.8728\n",
      "Epoch 204/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2831 - acc: 0.9097 - val_loss: 0.3830 - val_acc: 0.8708\n",
      "Epoch 205/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2849 - acc: 0.9104 - val_loss: 0.3593 - val_acc: 0.8787\n",
      "Epoch 206/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2797 - acc: 0.9097 - val_loss: 0.3491 - val_acc: 0.8847\n",
      "Epoch 207/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2825 - acc: 0.9077 - val_loss: 0.3472 - val_acc: 0.8827\n",
      "Epoch 208/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2819 - acc: 0.9070 - val_loss: 0.3537 - val_acc: 0.8748\n",
      "Epoch 209/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2824 - acc: 0.9110 - val_loss: 0.3585 - val_acc: 0.8807\n",
      "Epoch 210/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2827 - acc: 0.9104 - val_loss: 0.3664 - val_acc: 0.8827\n",
      "Epoch 211/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2806 - acc: 0.9104 - val_loss: 0.3521 - val_acc: 0.8847\n",
      "Epoch 212/3000\n",
      "1506/1506 [==============================] - 0s 57us/step - loss: 0.2817 - acc: 0.9077 - val_loss: 0.3562 - val_acc: 0.8767\n",
      "Epoch 213/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2811 - acc: 0.9090 - val_loss: 0.3465 - val_acc: 0.8867\n",
      "Epoch 214/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2794 - acc: 0.9104 - val_loss: 0.3874 - val_acc: 0.9006\n",
      "Epoch 215/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2830 - acc: 0.9117 - val_loss: 0.3634 - val_acc: 0.8748\n",
      "Epoch 216/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2795 - acc: 0.9097 - val_loss: 0.3516 - val_acc: 0.8787\n",
      "Epoch 217/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2767 - acc: 0.9130 - val_loss: 0.3619 - val_acc: 0.8767\n",
      "Epoch 218/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2805 - acc: 0.9110 - val_loss: 0.3474 - val_acc: 0.8748\n",
      "Epoch 219/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2777 - acc: 0.9084 - val_loss: 0.3386 - val_acc: 0.8827\n",
      "Epoch 220/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2769 - acc: 0.9130 - val_loss: 0.3692 - val_acc: 0.8807\n",
      "Epoch 221/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2780 - acc: 0.9104 - val_loss: 0.3450 - val_acc: 0.8787\n",
      "Epoch 222/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2758 - acc: 0.9110 - val_loss: 0.3396 - val_acc: 0.8867\n",
      "Epoch 223/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2757 - acc: 0.9097 - val_loss: 0.3456 - val_acc: 0.8847\n",
      "Epoch 224/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2757 - acc: 0.9130 - val_loss: 0.3590 - val_acc: 0.8748\n",
      "Epoch 225/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2748 - acc: 0.9130 - val_loss: 0.3447 - val_acc: 0.8748\n",
      "Epoch 226/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2749 - acc: 0.9077 - val_loss: 0.3539 - val_acc: 0.8767\n",
      "Epoch 227/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2758 - acc: 0.9130 - val_loss: 0.3579 - val_acc: 0.8907\n",
      "Epoch 228/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2749 - acc: 0.9137 - val_loss: 0.3371 - val_acc: 0.8827\n",
      "Epoch 229/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2732 - acc: 0.9124 - val_loss: 0.3535 - val_acc: 0.8787\n",
      "Epoch 230/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2726 - acc: 0.9117 - val_loss: 0.3397 - val_acc: 0.8827\n",
      "Epoch 231/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2733 - acc: 0.9117 - val_loss: 0.3306 - val_acc: 0.8847\n",
      "Epoch 232/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2725 - acc: 0.9124 - val_loss: 0.3569 - val_acc: 0.8748\n",
      "Epoch 233/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2742 - acc: 0.9157 - val_loss: 0.3367 - val_acc: 0.8847\n",
      "Epoch 234/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2710 - acc: 0.9137 - val_loss: 0.3323 - val_acc: 0.8867\n",
      "Epoch 235/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2717 - acc: 0.9104 - val_loss: 0.3506 - val_acc: 0.8867\n",
      "Epoch 236/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2716 - acc: 0.9137 - val_loss: 0.3592 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2701 - acc: 0.9150 - val_loss: 0.3371 - val_acc: 0.8827\n",
      "Epoch 238/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2719 - acc: 0.9124 - val_loss: 0.3438 - val_acc: 0.8966\n",
      "Epoch 239/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2686 - acc: 0.9130 - val_loss: 0.3504 - val_acc: 0.8787\n",
      "Epoch 240/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2697 - acc: 0.9124 - val_loss: 0.3325 - val_acc: 0.8926\n",
      "Epoch 241/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2681 - acc: 0.9104 - val_loss: 0.3461 - val_acc: 0.8787\n",
      "Epoch 242/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2671 - acc: 0.9110 - val_loss: 0.3314 - val_acc: 0.8807\n",
      "Epoch 243/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.2672 - acc: 0.9117 - val_loss: 0.3291 - val_acc: 0.8887\n",
      "Epoch 244/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2686 - acc: 0.9150 - val_loss: 0.3295 - val_acc: 0.8907\n",
      "Epoch 245/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2666 - acc: 0.9117 - val_loss: 0.3521 - val_acc: 0.8887\n",
      "Epoch 246/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2664 - acc: 0.9163 - val_loss: 0.3529 - val_acc: 0.8748\n",
      "Epoch 247/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.2689 - acc: 0.9124 - val_loss: 0.3216 - val_acc: 0.8887\n",
      "Epoch 248/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2651 - acc: 0.9130 - val_loss: 0.3293 - val_acc: 0.8926\n",
      "Epoch 249/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2665 - acc: 0.9150 - val_loss: 0.3354 - val_acc: 0.8847\n",
      "Epoch 250/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2643 - acc: 0.9143 - val_loss: 0.3342 - val_acc: 0.8787\n",
      "Epoch 251/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2672 - acc: 0.9157 - val_loss: 0.3528 - val_acc: 0.8787\n",
      "Epoch 252/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.2611 - acc: 0.9143 - val_loss: 0.3359 - val_acc: 0.8867\n",
      "Epoch 253/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2629 - acc: 0.9183 - val_loss: 0.3228 - val_acc: 0.8827\n",
      "Epoch 254/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2643 - acc: 0.9157 - val_loss: 0.3239 - val_acc: 0.8907\n",
      "Epoch 255/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2640 - acc: 0.9163 - val_loss: 0.3242 - val_acc: 0.8827\n",
      "Epoch 256/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2615 - acc: 0.9157 - val_loss: 0.3191 - val_acc: 0.8867\n",
      "Epoch 257/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.2616 - acc: 0.9150 - val_loss: 0.3530 - val_acc: 0.8708\n",
      "Epoch 258/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2635 - acc: 0.9130 - val_loss: 0.3474 - val_acc: 0.8787\n",
      "Epoch 259/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2623 - acc: 0.9130 - val_loss: 0.3400 - val_acc: 0.8827\n",
      "Epoch 260/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2631 - acc: 0.9150 - val_loss: 0.3302 - val_acc: 0.9046\n",
      "Epoch 261/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2631 - acc: 0.9170 - val_loss: 0.3248 - val_acc: 0.8847\n",
      "Epoch 262/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2599 - acc: 0.9130 - val_loss: 0.3152 - val_acc: 0.8926\n",
      "Epoch 263/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2601 - acc: 0.9170 - val_loss: 0.3153 - val_acc: 0.8847\n",
      "Epoch 264/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2574 - acc: 0.9150 - val_loss: 0.3337 - val_acc: 0.8907\n",
      "Epoch 265/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2605 - acc: 0.9137 - val_loss: 0.3212 - val_acc: 0.8926\n",
      "Epoch 266/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2593 - acc: 0.9157 - val_loss: 0.3575 - val_acc: 0.8708\n",
      "Epoch 267/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2598 - acc: 0.9183 - val_loss: 0.3193 - val_acc: 0.8827\n",
      "Epoch 268/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2598 - acc: 0.9183 - val_loss: 0.3263 - val_acc: 0.8827\n",
      "Epoch 269/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2600 - acc: 0.9150 - val_loss: 0.3220 - val_acc: 0.8926\n",
      "Epoch 270/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.2589 - acc: 0.9150 - val_loss: 0.3339 - val_acc: 0.8807\n",
      "Epoch 271/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2585 - acc: 0.9130 - val_loss: 0.3391 - val_acc: 0.8767\n",
      "Epoch 272/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2568 - acc: 0.9183 - val_loss: 0.3370 - val_acc: 0.8887\n",
      "Epoch 273/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2565 - acc: 0.9177 - val_loss: 0.3487 - val_acc: 0.8767\n",
      "Epoch 274/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2562 - acc: 0.9110 - val_loss: 0.3208 - val_acc: 0.8907\n",
      "Epoch 275/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2573 - acc: 0.9170 - val_loss: 0.3189 - val_acc: 0.8887\n",
      "Epoch 276/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2561 - acc: 0.9197 - val_loss: 0.3211 - val_acc: 0.8887\n",
      "Epoch 277/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2561 - acc: 0.9177 - val_loss: 0.3231 - val_acc: 0.8847\n",
      "Epoch 278/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.2555 - acc: 0.9197 - val_loss: 0.3652 - val_acc: 0.9046\n",
      "Epoch 279/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2544 - acc: 0.9216 - val_loss: 0.3187 - val_acc: 0.8907\n",
      "Epoch 280/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2568 - acc: 0.9163 - val_loss: 0.3113 - val_acc: 0.8946\n",
      "Epoch 281/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2564 - acc: 0.9163 - val_loss: 0.3061 - val_acc: 0.9006\n",
      "Epoch 282/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2545 - acc: 0.9197 - val_loss: 0.3306 - val_acc: 0.8827\n",
      "Epoch 283/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2539 - acc: 0.9157 - val_loss: 0.3175 - val_acc: 0.8926\n",
      "Epoch 284/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2550 - acc: 0.9177 - val_loss: 0.3152 - val_acc: 0.8907\n",
      "Epoch 285/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2534 - acc: 0.9170 - val_loss: 0.3068 - val_acc: 0.8946\n",
      "Epoch 286/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2528 - acc: 0.9137 - val_loss: 0.3171 - val_acc: 0.8966\n",
      "Epoch 287/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.2537 - acc: 0.9210 - val_loss: 0.3463 - val_acc: 0.8748\n",
      "Epoch 288/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2543 - acc: 0.9197 - val_loss: 0.3270 - val_acc: 0.8847\n",
      "Epoch 289/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2522 - acc: 0.9177 - val_loss: 0.3223 - val_acc: 0.8907\n",
      "Epoch 290/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2517 - acc: 0.9183 - val_loss: 0.3154 - val_acc: 0.8887\n",
      "Epoch 291/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2507 - acc: 0.9190 - val_loss: 0.3338 - val_acc: 0.8847\n",
      "Epoch 292/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2514 - acc: 0.9183 - val_loss: 0.3209 - val_acc: 0.8867\n",
      "Epoch 293/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2529 - acc: 0.9190 - val_loss: 0.3080 - val_acc: 0.8986\n",
      "Epoch 294/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2500 - acc: 0.9177 - val_loss: 0.3166 - val_acc: 0.8867\n",
      "Epoch 295/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2496 - acc: 0.9170 - val_loss: 0.3083 - val_acc: 0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2498 - acc: 0.9163 - val_loss: 0.3440 - val_acc: 0.8807\n",
      "Epoch 297/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.2507 - acc: 0.9197 - val_loss: 0.3183 - val_acc: 0.8847\n",
      "Epoch 298/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.2486 - acc: 0.9203 - val_loss: 0.3132 - val_acc: 0.8887\n",
      "Epoch 299/3000\n",
      "1506/1506 [==============================] - 0s 58us/step - loss: 0.2517 - acc: 0.9197 - val_loss: 0.3063 - val_acc: 0.8946\n",
      "Epoch 300/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2470 - acc: 0.9216 - val_loss: 0.3100 - val_acc: 0.8926\n",
      "Epoch 301/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.2490 - acc: 0.9170 - val_loss: 0.3110 - val_acc: 0.8926\n",
      "Epoch 00301: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=rmsp,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist3=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/3000\n",
      "1506/1506 [==============================] - 1s 530us/step - loss: 2.0409 - acc: 0.2756 - val_loss: 1.8970 - val_acc: 0.3797\n",
      "Epoch 2/3000\n",
      "1506/1506 [==============================] - 0s 97us/step - loss: 1.8557 - acc: 0.3798 - val_loss: 1.7694 - val_acc: 0.4513\n",
      "Epoch 3/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 1.7277 - acc: 0.4031 - val_loss: 1.6642 - val_acc: 0.4712\n",
      "Epoch 4/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.6084 - acc: 0.4117 - val_loss: 1.5428 - val_acc: 0.4533\n",
      "Epoch 5/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 1.5092 - acc: 0.4137 - val_loss: 1.4707 - val_acc: 0.4732\n",
      "Epoch 6/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.4283 - acc: 0.4290 - val_loss: 1.4039 - val_acc: 0.4771\n",
      "Epoch 7/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 1.3619 - acc: 0.4728 - val_loss: 1.3460 - val_acc: 0.5388\n",
      "Epoch 8/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.3027 - acc: 0.5544 - val_loss: 1.2978 - val_acc: 0.5686\n",
      "Epoch 9/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.2520 - acc: 0.5830 - val_loss: 1.2522 - val_acc: 0.5905\n",
      "Epoch 10/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 1.2045 - acc: 0.6268 - val_loss: 1.2091 - val_acc: 0.6103\n",
      "Epoch 11/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.1614 - acc: 0.6434 - val_loss: 1.1780 - val_acc: 0.6103\n",
      "Epoch 12/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.1225 - acc: 0.6474 - val_loss: 1.1539 - val_acc: 0.6998\n",
      "Epoch 13/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 1.0847 - acc: 0.6806 - val_loss: 1.1087 - val_acc: 0.6541\n",
      "Epoch 14/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 1.0511 - acc: 0.6992 - val_loss: 1.0844 - val_acc: 0.6640\n",
      "Epoch 15/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 1.0203 - acc: 0.6992 - val_loss: 1.0628 - val_acc: 0.7594\n",
      "Epoch 16/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.9927 - acc: 0.7510 - val_loss: 1.0282 - val_acc: 0.7276\n",
      "Epoch 17/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.9665 - acc: 0.7470 - val_loss: 1.0104 - val_acc: 0.7555\n",
      "Epoch 18/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.9426 - acc: 0.7663 - val_loss: 0.9882 - val_acc: 0.7694\n",
      "Epoch 19/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.9196 - acc: 0.7689 - val_loss: 0.9633 - val_acc: 0.7773\n",
      "Epoch 20/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.8982 - acc: 0.7862 - val_loss: 0.9454 - val_acc: 0.7674\n",
      "Epoch 21/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.8800 - acc: 0.7875 - val_loss: 0.9274 - val_acc: 0.7793\n",
      "Epoch 22/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.8614 - acc: 0.7988 - val_loss: 0.9179 - val_acc: 0.8052\n",
      "Epoch 23/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.8438 - acc: 0.8081 - val_loss: 0.8956 - val_acc: 0.7992\n",
      "Epoch 24/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.8278 - acc: 0.8201 - val_loss: 0.8826 - val_acc: 0.8072\n",
      "Epoch 25/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.8127 - acc: 0.8201 - val_loss: 0.8682 - val_acc: 0.7972\n",
      "Epoch 26/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7987 - acc: 0.8214 - val_loss: 0.8559 - val_acc: 0.8191\n",
      "Epoch 27/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7855 - acc: 0.8274 - val_loss: 0.8407 - val_acc: 0.8270\n",
      "Epoch 28/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.7725 - acc: 0.8333 - val_loss: 0.8291 - val_acc: 0.8191\n",
      "Epoch 29/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.7605 - acc: 0.8307 - val_loss: 0.8198 - val_acc: 0.8250\n",
      "Epoch 30/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7493 - acc: 0.8360 - val_loss: 0.8142 - val_acc: 0.8290\n",
      "Epoch 31/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7386 - acc: 0.8433 - val_loss: 0.8010 - val_acc: 0.8330\n",
      "Epoch 32/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.7286 - acc: 0.8453 - val_loss: 0.7904 - val_acc: 0.8370\n",
      "Epoch 33/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.7183 - acc: 0.8486 - val_loss: 0.7826 - val_acc: 0.8231\n",
      "Epoch 34/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.7095 - acc: 0.8433 - val_loss: 0.7816 - val_acc: 0.8310\n",
      "Epoch 35/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.7018 - acc: 0.8546 - val_loss: 0.7647 - val_acc: 0.8350\n",
      "Epoch 36/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.6927 - acc: 0.8519 - val_loss: 0.7568 - val_acc: 0.8350\n",
      "Epoch 37/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.6846 - acc: 0.8513 - val_loss: 0.7469 - val_acc: 0.8429\n",
      "Epoch 38/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.6774 - acc: 0.8579 - val_loss: 0.7391 - val_acc: 0.8429\n",
      "Epoch 39/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.6696 - acc: 0.8599 - val_loss: 0.7354 - val_acc: 0.8429\n",
      "Epoch 40/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6637 - acc: 0.8639 - val_loss: 0.7278 - val_acc: 0.8410\n",
      "Epoch 41/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6563 - acc: 0.8559 - val_loss: 0.7216 - val_acc: 0.8410\n",
      "Epoch 42/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6501 - acc: 0.8566 - val_loss: 0.7162 - val_acc: 0.8390\n",
      "Epoch 43/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6443 - acc: 0.8606 - val_loss: 0.7090 - val_acc: 0.8429\n",
      "Epoch 44/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.6379 - acc: 0.8612 - val_loss: 0.7044 - val_acc: 0.8390\n",
      "Epoch 45/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6322 - acc: 0.8619 - val_loss: 0.7005 - val_acc: 0.8429\n",
      "Epoch 46/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.6275 - acc: 0.8612 - val_loss: 0.6941 - val_acc: 0.8449\n",
      "Epoch 47/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6219 - acc: 0.8659 - val_loss: 0.6913 - val_acc: 0.8410\n",
      "Epoch 48/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6172 - acc: 0.8645 - val_loss: 0.6865 - val_acc: 0.8370\n",
      "Epoch 49/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6122 - acc: 0.8632 - val_loss: 0.6792 - val_acc: 0.8449\n",
      "Epoch 50/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6074 - acc: 0.8665 - val_loss: 0.6753 - val_acc: 0.8410\n",
      "Epoch 51/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6032 - acc: 0.8692 - val_loss: 0.6717 - val_acc: 0.8410\n",
      "Epoch 52/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5986 - acc: 0.8659 - val_loss: 0.6663 - val_acc: 0.8449\n",
      "Epoch 53/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5945 - acc: 0.8645 - val_loss: 0.6624 - val_acc: 0.8410\n",
      "Epoch 54/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5904 - acc: 0.8659 - val_loss: 0.6635 - val_acc: 0.8469\n",
      "Epoch 55/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5873 - acc: 0.8718 - val_loss: 0.6556 - val_acc: 0.8449\n",
      "Epoch 56/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5830 - acc: 0.8712 - val_loss: 0.6524 - val_acc: 0.8469\n",
      "Epoch 57/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5800 - acc: 0.8718 - val_loss: 0.6470 - val_acc: 0.8429\n",
      "Epoch 58/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5757 - acc: 0.8699 - val_loss: 0.6437 - val_acc: 0.8410\n",
      "Epoch 59/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.5720 - acc: 0.8685 - val_loss: 0.6402 - val_acc: 0.8489\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.5688 - acc: 0.8692 - val_loss: 0.6360 - val_acc: 0.8469\n",
      "Epoch 61/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5652 - acc: 0.8712 - val_loss: 0.6336 - val_acc: 0.8469\n",
      "Epoch 62/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5619 - acc: 0.8712 - val_loss: 0.6313 - val_acc: 0.8449\n",
      "Epoch 63/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.5586 - acc: 0.8712 - val_loss: 0.6324 - val_acc: 0.8469\n",
      "Epoch 64/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.5562 - acc: 0.8765 - val_loss: 0.6268 - val_acc: 0.8469\n",
      "Epoch 65/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5529 - acc: 0.8718 - val_loss: 0.6243 - val_acc: 0.8449\n",
      "Epoch 66/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5499 - acc: 0.8738 - val_loss: 0.6197 - val_acc: 0.8489\n",
      "Epoch 67/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.5468 - acc: 0.8718 - val_loss: 0.6168 - val_acc: 0.8449\n",
      "Epoch 68/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5443 - acc: 0.8725 - val_loss: 0.6131 - val_acc: 0.8429\n",
      "Epoch 69/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5418 - acc: 0.8738 - val_loss: 0.6149 - val_acc: 0.8410\n",
      "Epoch 70/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.5402 - acc: 0.8732 - val_loss: 0.6088 - val_acc: 0.8489\n",
      "Epoch 71/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5367 - acc: 0.8732 - val_loss: 0.6062 - val_acc: 0.8469\n",
      "Epoch 72/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.5342 - acc: 0.8758 - val_loss: 0.6029 - val_acc: 0.8489\n",
      "Epoch 73/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5317 - acc: 0.8752 - val_loss: 0.6025 - val_acc: 0.8489\n",
      "Epoch 74/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5295 - acc: 0.8772 - val_loss: 0.6000 - val_acc: 0.8529\n",
      "Epoch 75/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5271 - acc: 0.8778 - val_loss: 0.5973 - val_acc: 0.8469\n",
      "Epoch 76/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5249 - acc: 0.8772 - val_loss: 0.5944 - val_acc: 0.8489\n",
      "Epoch 77/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5225 - acc: 0.8798 - val_loss: 0.5932 - val_acc: 0.8489\n",
      "Epoch 78/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5207 - acc: 0.8765 - val_loss: 0.5909 - val_acc: 0.8489\n",
      "Epoch 79/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5184 - acc: 0.8765 - val_loss: 0.5902 - val_acc: 0.8489\n",
      "Epoch 80/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5166 - acc: 0.8778 - val_loss: 0.5870 - val_acc: 0.8469\n",
      "Epoch 81/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5140 - acc: 0.8792 - val_loss: 0.5844 - val_acc: 0.8489\n",
      "Epoch 82/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5124 - acc: 0.8792 - val_loss: 0.5829 - val_acc: 0.8489\n",
      "Epoch 83/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5107 - acc: 0.8792 - val_loss: 0.5805 - val_acc: 0.8469\n",
      "Epoch 84/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.5089 - acc: 0.8785 - val_loss: 0.5794 - val_acc: 0.8489\n",
      "Epoch 85/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5065 - acc: 0.8805 - val_loss: 0.5773 - val_acc: 0.8469\n",
      "Epoch 86/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.5049 - acc: 0.8798 - val_loss: 0.5758 - val_acc: 0.8489\n",
      "Epoch 87/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5030 - acc: 0.8798 - val_loss: 0.5745 - val_acc: 0.8509\n",
      "Epoch 88/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5016 - acc: 0.8798 - val_loss: 0.5732 - val_acc: 0.8529\n",
      "Epoch 89/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4999 - acc: 0.8818 - val_loss: 0.5713 - val_acc: 0.8529\n",
      "Epoch 90/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4983 - acc: 0.8805 - val_loss: 0.5700 - val_acc: 0.8509\n",
      "Epoch 91/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4965 - acc: 0.8818 - val_loss: 0.5689 - val_acc: 0.8509\n",
      "Epoch 92/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4950 - acc: 0.8805 - val_loss: 0.5682 - val_acc: 0.8509\n",
      "Epoch 93/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4936 - acc: 0.8805 - val_loss: 0.5657 - val_acc: 0.8469\n",
      "Epoch 94/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.4923 - acc: 0.8838 - val_loss: 0.5636 - val_acc: 0.8489\n",
      "Epoch 95/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4904 - acc: 0.8798 - val_loss: 0.5627 - val_acc: 0.8489\n",
      "Epoch 96/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4890 - acc: 0.8805 - val_loss: 0.5605 - val_acc: 0.8469\n",
      "Epoch 97/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4878 - acc: 0.8825 - val_loss: 0.5592 - val_acc: 0.8469\n",
      "Epoch 98/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4866 - acc: 0.8811 - val_loss: 0.5591 - val_acc: 0.8529\n",
      "Epoch 99/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4849 - acc: 0.8818 - val_loss: 0.5579 - val_acc: 0.8569\n",
      "Epoch 100/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4835 - acc: 0.8811 - val_loss: 0.5560 - val_acc: 0.8549\n",
      "Epoch 101/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4820 - acc: 0.8825 - val_loss: 0.5554 - val_acc: 0.8549\n",
      "Epoch 102/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4808 - acc: 0.8825 - val_loss: 0.5536 - val_acc: 0.8549\n",
      "Epoch 103/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4793 - acc: 0.8845 - val_loss: 0.5522 - val_acc: 0.8549\n",
      "Epoch 104/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4785 - acc: 0.8838 - val_loss: 0.5508 - val_acc: 0.8529\n",
      "Epoch 105/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4773 - acc: 0.8805 - val_loss: 0.5499 - val_acc: 0.8549\n",
      "Epoch 106/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4759 - acc: 0.8838 - val_loss: 0.5494 - val_acc: 0.8509\n",
      "Epoch 107/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4748 - acc: 0.8845 - val_loss: 0.5490 - val_acc: 0.8569\n",
      "Epoch 108/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4524 - acc: 0.889 - 0s 64us/step - loss: 0.4738 - acc: 0.8818 - val_loss: 0.5476 - val_acc: 0.8569\n",
      "Epoch 109/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4730 - acc: 0.8838 - val_loss: 0.5466 - val_acc: 0.8549\n",
      "Epoch 110/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4717 - acc: 0.8838 - val_loss: 0.5449 - val_acc: 0.8588\n",
      "Epoch 111/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4703 - acc: 0.8831 - val_loss: 0.5506 - val_acc: 0.8489\n",
      "Epoch 112/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4721 - acc: 0.8851 - val_loss: 0.5435 - val_acc: 0.8549\n",
      "Epoch 113/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4689 - acc: 0.8838 - val_loss: 0.5409 - val_acc: 0.8549\n",
      "Epoch 114/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4671 - acc: 0.8851 - val_loss: 0.5406 - val_acc: 0.8549\n",
      "Epoch 115/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4661 - acc: 0.8851 - val_loss: 0.5401 - val_acc: 0.8549\n",
      "Epoch 116/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4655 - acc: 0.8851 - val_loss: 0.5384 - val_acc: 0.8569\n",
      "Epoch 117/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.4639 - acc: 0.8858 - val_loss: 0.5381 - val_acc: 0.8549\n",
      "Epoch 118/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.4633 - acc: 0.8878 - val_loss: 0.5369 - val_acc: 0.8549\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.4623 - acc: 0.8838 - val_loss: 0.5355 - val_acc: 0.8549\n",
      "Epoch 120/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.4613 - acc: 0.8838 - val_loss: 0.5349 - val_acc: 0.8569\n",
      "Epoch 121/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4608 - acc: 0.8845 - val_loss: 0.5339 - val_acc: 0.8549\n",
      "Epoch 122/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4599 - acc: 0.8878 - val_loss: 0.5328 - val_acc: 0.8549\n",
      "Epoch 123/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4587 - acc: 0.8838 - val_loss: 0.5317 - val_acc: 0.8549\n",
      "Epoch 124/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.4579 - acc: 0.8871 - val_loss: 0.5311 - val_acc: 0.8569\n",
      "Epoch 125/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4568 - acc: 0.8858 - val_loss: 0.5307 - val_acc: 0.8588\n",
      "Epoch 126/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4560 - acc: 0.8865 - val_loss: 0.5301 - val_acc: 0.8569\n",
      "Epoch 127/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4554 - acc: 0.8865 - val_loss: 0.5291 - val_acc: 0.8588\n",
      "Epoch 128/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4544 - acc: 0.8851 - val_loss: 0.5285 - val_acc: 0.8588\n",
      "Epoch 129/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4536 - acc: 0.8858 - val_loss: 0.5287 - val_acc: 0.8608\n",
      "Epoch 130/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4528 - acc: 0.8845 - val_loss: 0.5271 - val_acc: 0.8588\n",
      "Epoch 131/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4521 - acc: 0.8871 - val_loss: 0.5307 - val_acc: 0.8529\n",
      "Epoch 132/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4536 - acc: 0.8871 - val_loss: 0.5241 - val_acc: 0.8549\n",
      "Epoch 133/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4508 - acc: 0.8858 - val_loss: 0.5236 - val_acc: 0.8569\n",
      "Epoch 134/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4497 - acc: 0.8865 - val_loss: 0.5227 - val_acc: 0.8588\n",
      "Epoch 135/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4492 - acc: 0.8851 - val_loss: 0.5223 - val_acc: 0.8588\n",
      "Epoch 136/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.4481 - acc: 0.8858 - val_loss: 0.5217 - val_acc: 0.8588\n",
      "Epoch 137/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4475 - acc: 0.8858 - val_loss: 0.5212 - val_acc: 0.8608\n",
      "Epoch 138/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.4466 - acc: 0.8865 - val_loss: 0.5211 - val_acc: 0.8569\n",
      "Epoch 139/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4462 - acc: 0.8865 - val_loss: 0.5202 - val_acc: 0.8588\n",
      "Epoch 140/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4451 - acc: 0.8884 - val_loss: 0.5189 - val_acc: 0.8588\n",
      "Epoch 141/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4449 - acc: 0.8871 - val_loss: 0.5183 - val_acc: 0.8569\n",
      "Epoch 142/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4441 - acc: 0.8871 - val_loss: 0.5176 - val_acc: 0.8588\n",
      "Epoch 143/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4434 - acc: 0.8865 - val_loss: 0.5166 - val_acc: 0.8608\n",
      "Epoch 144/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4425 - acc: 0.8858 - val_loss: 0.5163 - val_acc: 0.8608\n",
      "Epoch 145/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4421 - acc: 0.8865 - val_loss: 0.5156 - val_acc: 0.8608\n",
      "Epoch 146/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4411 - acc: 0.8871 - val_loss: 0.5152 - val_acc: 0.8608\n",
      "Epoch 147/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4405 - acc: 0.8871 - val_loss: 0.5143 - val_acc: 0.8588\n",
      "Epoch 148/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4401 - acc: 0.8865 - val_loss: 0.5147 - val_acc: 0.8588\n",
      "Epoch 149/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4392 - acc: 0.8865 - val_loss: 0.5139 - val_acc: 0.8608\n",
      "Epoch 150/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4387 - acc: 0.8851 - val_loss: 0.5132 - val_acc: 0.8608\n",
      "Epoch 151/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4381 - acc: 0.8865 - val_loss: 0.5130 - val_acc: 0.8608\n",
      "Epoch 152/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4376 - acc: 0.8871 - val_loss: 0.5125 - val_acc: 0.8529\n",
      "Epoch 153/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4370 - acc: 0.8878 - val_loss: 0.5117 - val_acc: 0.8588\n",
      "Epoch 154/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4363 - acc: 0.8871 - val_loss: 0.5119 - val_acc: 0.8549\n",
      "Epoch 155/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4358 - acc: 0.8871 - val_loss: 0.5116 - val_acc: 0.8569\n",
      "Epoch 156/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4353 - acc: 0.8871 - val_loss: 0.5101 - val_acc: 0.8588\n",
      "Epoch 157/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4345 - acc: 0.8878 - val_loss: 0.5096 - val_acc: 0.8588\n",
      "Epoch 158/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4340 - acc: 0.8871 - val_loss: 0.5090 - val_acc: 0.8608\n",
      "Epoch 159/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4333 - acc: 0.8865 - val_loss: 0.5084 - val_acc: 0.8588\n",
      "Epoch 160/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4330 - acc: 0.8878 - val_loss: 0.5080 - val_acc: 0.8569\n",
      "Epoch 161/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4323 - acc: 0.8878 - val_loss: 0.5074 - val_acc: 0.8569\n",
      "Epoch 162/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4318 - acc: 0.8865 - val_loss: 0.5065 - val_acc: 0.8588\n",
      "Epoch 163/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4311 - acc: 0.8865 - val_loss: 0.5066 - val_acc: 0.8569\n",
      "Epoch 164/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4307 - acc: 0.8865 - val_loss: 0.5062 - val_acc: 0.8549\n",
      "Epoch 165/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4303 - acc: 0.8884 - val_loss: 0.5054 - val_acc: 0.8608\n",
      "Epoch 166/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4294 - acc: 0.8871 - val_loss: 0.5050 - val_acc: 0.8588\n",
      "Epoch 167/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4290 - acc: 0.8878 - val_loss: 0.5056 - val_acc: 0.8569\n",
      "Epoch 168/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4293 - acc: 0.8884 - val_loss: 0.5050 - val_acc: 0.8569\n",
      "Epoch 169/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4281 - acc: 0.8878 - val_loss: 0.5044 - val_acc: 0.8588\n",
      "Epoch 170/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4274 - acc: 0.8891 - val_loss: 0.5044 - val_acc: 0.8569\n",
      "Epoch 171/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4276 - acc: 0.8891 - val_loss: 0.5030 - val_acc: 0.8549\n",
      "Epoch 172/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4267 - acc: 0.8878 - val_loss: 0.5025 - val_acc: 0.8588\n",
      "Epoch 173/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4260 - acc: 0.8884 - val_loss: 0.5020 - val_acc: 0.8569\n",
      "Epoch 174/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4257 - acc: 0.8884 - val_loss: 0.5022 - val_acc: 0.8549\n",
      "Epoch 175/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4252 - acc: 0.8898 - val_loss: 0.5017 - val_acc: 0.8569\n",
      "Epoch 176/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4246 - acc: 0.8904 - val_loss: 0.5013 - val_acc: 0.8588\n",
      "Epoch 177/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4240 - acc: 0.8884 - val_loss: 0.5024 - val_acc: 0.8549\n",
      "Epoch 178/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4241 - acc: 0.8891 - val_loss: 0.5020 - val_acc: 0.8529\n",
      "Epoch 179/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4240 - acc: 0.8904 - val_loss: 0.5007 - val_acc: 0.8549\n",
      "Epoch 180/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4233 - acc: 0.8884 - val_loss: 0.4996 - val_acc: 0.8549\n",
      "Epoch 181/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4223 - acc: 0.8878 - val_loss: 0.5036 - val_acc: 0.8549\n",
      "Epoch 182/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.4237 - acc: 0.8871 - val_loss: 0.4995 - val_acc: 0.8509\n",
      "Epoch 183/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4218 - acc: 0.8878 - val_loss: 0.4974 - val_acc: 0.8529\n",
      "Epoch 184/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4213 - acc: 0.8878 - val_loss: 0.4980 - val_acc: 0.8509\n",
      "Epoch 185/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.4212 - acc: 0.8891 - val_loss: 0.4968 - val_acc: 0.8569\n",
      "Epoch 186/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4202 - acc: 0.8891 - val_loss: 0.4961 - val_acc: 0.8549\n",
      "Epoch 187/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4197 - acc: 0.8884 - val_loss: 0.4959 - val_acc: 0.8549\n",
      "Epoch 188/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4192 - acc: 0.8891 - val_loss: 0.4980 - val_acc: 0.8569\n",
      "Epoch 189/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4194 - acc: 0.8918 - val_loss: 0.4967 - val_acc: 0.8608\n",
      "Epoch 190/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.4185 - acc: 0.8898 - val_loss: 0.4959 - val_acc: 0.8588\n",
      "Epoch 191/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4184 - acc: 0.8891 - val_loss: 0.4947 - val_acc: 0.8549\n",
      "Epoch 192/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4176 - acc: 0.8898 - val_loss: 0.4945 - val_acc: 0.8569\n",
      "Epoch 193/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4171 - acc: 0.8878 - val_loss: 0.4941 - val_acc: 0.8588\n",
      "Epoch 194/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4168 - acc: 0.8878 - val_loss: 0.4938 - val_acc: 0.8569\n",
      "Epoch 195/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.4164 - acc: 0.8891 - val_loss: 0.4936 - val_acc: 0.8588\n",
      "Epoch 196/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4159 - acc: 0.8898 - val_loss: 0.4927 - val_acc: 0.8569\n",
      "Epoch 197/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4158 - acc: 0.8898 - val_loss: 0.4929 - val_acc: 0.8588\n",
      "Epoch 198/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4153 - acc: 0.8891 - val_loss: 0.4918 - val_acc: 0.8628\n",
      "Epoch 199/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4150 - acc: 0.8891 - val_loss: 0.4914 - val_acc: 0.8588\n",
      "Epoch 200/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4145 - acc: 0.8878 - val_loss: 0.4913 - val_acc: 0.8588\n",
      "Epoch 201/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.4142 - acc: 0.8884 - val_loss: 0.4942 - val_acc: 0.8608\n",
      "Epoch 202/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4146 - acc: 0.8865 - val_loss: 0.4920 - val_acc: 0.8608\n",
      "Epoch 203/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4135 - acc: 0.8884 - val_loss: 0.4914 - val_acc: 0.8509\n",
      "Epoch 204/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4131 - acc: 0.8878 - val_loss: 0.4903 - val_acc: 0.8549\n",
      "Epoch 205/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4125 - acc: 0.8904 - val_loss: 0.4915 - val_acc: 0.8588\n",
      "Epoch 206/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4127 - acc: 0.8865 - val_loss: 0.4912 - val_acc: 0.8588\n",
      "Epoch 207/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4127 - acc: 0.8884 - val_loss: 0.4892 - val_acc: 0.8569\n",
      "Epoch 208/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4116 - acc: 0.8898 - val_loss: 0.4893 - val_acc: 0.8509\n",
      "Epoch 209/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4113 - acc: 0.8891 - val_loss: 0.4890 - val_acc: 0.8549\n",
      "Epoch 210/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4109 - acc: 0.8884 - val_loss: 0.4890 - val_acc: 0.8529\n",
      "Epoch 211/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4107 - acc: 0.8884 - val_loss: 0.4888 - val_acc: 0.8529\n",
      "Epoch 212/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4103 - acc: 0.8904 - val_loss: 0.4880 - val_acc: 0.8569\n",
      "Epoch 213/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4096 - acc: 0.8898 - val_loss: 0.4876 - val_acc: 0.8608\n",
      "Epoch 214/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4093 - acc: 0.8904 - val_loss: 0.4867 - val_acc: 0.8549\n",
      "Epoch 215/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4091 - acc: 0.8898 - val_loss: 0.4867 - val_acc: 0.8549\n",
      "Epoch 216/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4087 - acc: 0.8898 - val_loss: 0.4862 - val_acc: 0.8549\n",
      "Epoch 217/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4083 - acc: 0.8891 - val_loss: 0.4846 - val_acc: 0.8509\n",
      "Epoch 218/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4084 - acc: 0.8898 - val_loss: 0.4846 - val_acc: 0.8489\n",
      "Epoch 219/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4080 - acc: 0.8884 - val_loss: 0.4846 - val_acc: 0.8549\n",
      "Epoch 220/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4074 - acc: 0.8891 - val_loss: 0.4846 - val_acc: 0.8569\n",
      "Epoch 221/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4070 - acc: 0.8891 - val_loss: 0.4840 - val_acc: 0.8549\n",
      "Epoch 222/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4067 - acc: 0.8891 - val_loss: 0.4840 - val_acc: 0.8608\n",
      "Epoch 223/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4063 - acc: 0.8904 - val_loss: 0.4840 - val_acc: 0.8588\n",
      "Epoch 224/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4061 - acc: 0.8891 - val_loss: 0.4838 - val_acc: 0.8588\n",
      "Epoch 225/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4057 - acc: 0.8891 - val_loss: 0.4836 - val_acc: 0.8569\n",
      "Epoch 226/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4054 - acc: 0.8891 - val_loss: 0.4835 - val_acc: 0.8569\n",
      "Epoch 227/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4053 - acc: 0.8884 - val_loss: 0.4833 - val_acc: 0.8608\n",
      "Epoch 228/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4047 - acc: 0.8884 - val_loss: 0.4823 - val_acc: 0.8569\n",
      "Epoch 229/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4045 - acc: 0.8898 - val_loss: 0.4821 - val_acc: 0.8569\n",
      "Epoch 230/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4041 - acc: 0.8884 - val_loss: 0.4823 - val_acc: 0.8529\n",
      "Epoch 231/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4039 - acc: 0.8904 - val_loss: 0.4825 - val_acc: 0.8549\n",
      "Epoch 232/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4036 - acc: 0.8904 - val_loss: 0.4820 - val_acc: 0.8569\n",
      "Epoch 233/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.4034 - acc: 0.8904 - val_loss: 0.4815 - val_acc: 0.8588\n",
      "Epoch 234/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4029 - acc: 0.8891 - val_loss: 0.4812 - val_acc: 0.8608\n",
      "Epoch 235/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4027 - acc: 0.8891 - val_loss: 0.4815 - val_acc: 0.8549\n",
      "Epoch 236/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.4026 - acc: 0.8891 - val_loss: 0.4808 - val_acc: 0.8608\n",
      "Epoch 237/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4022 - acc: 0.8891 - val_loss: 0.4807 - val_acc: 0.8569\n",
      "Epoch 238/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.4018 - acc: 0.8904 - val_loss: 0.4813 - val_acc: 0.8569\n",
      "Epoch 239/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4016 - acc: 0.8911 - val_loss: 0.4808 - val_acc: 0.8549\n",
      "Epoch 240/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.4013 - acc: 0.8904 - val_loss: 0.4815 - val_acc: 0.8509\n",
      "Epoch 241/3000\n",
      "1506/1506 [==============================] - 0s 63us/step - loss: 0.4012 - acc: 0.8884 - val_loss: 0.4803 - val_acc: 0.8529\n",
      "Epoch 242/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4010 - acc: 0.8884 - val_loss: 0.4801 - val_acc: 0.8569\n",
      "Epoch 243/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.4008 - acc: 0.8891 - val_loss: 0.4796 - val_acc: 0.8569\n",
      "Epoch 244/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4003 - acc: 0.8878 - val_loss: 0.4788 - val_acc: 0.8549\n",
      "Epoch 245/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3998 - acc: 0.8911 - val_loss: 0.4787 - val_acc: 0.8588\n",
      "Epoch 246/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3996 - acc: 0.8884 - val_loss: 0.4783 - val_acc: 0.8569\n",
      "Epoch 247/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3992 - acc: 0.8911 - val_loss: 0.4780 - val_acc: 0.8549\n",
      "Epoch 248/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3989 - acc: 0.8918 - val_loss: 0.4774 - val_acc: 0.8529\n",
      "Epoch 249/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3990 - acc: 0.8891 - val_loss: 0.4771 - val_acc: 0.8569\n",
      "Epoch 250/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3985 - acc: 0.8891 - val_loss: 0.4774 - val_acc: 0.8529\n",
      "Epoch 251/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3984 - acc: 0.8904 - val_loss: 0.4770 - val_acc: 0.8569\n",
      "Epoch 252/3000\n",
      "1506/1506 [==============================] - 0s 125us/step - loss: 0.3980 - acc: 0.8904 - val_loss: 0.4767 - val_acc: 0.8569\n",
      "Epoch 253/3000\n",
      "1506/1506 [==============================] - 0s 92us/step - loss: 0.3976 - acc: 0.8891 - val_loss: 0.4771 - val_acc: 0.8569\n",
      "Epoch 254/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3975 - acc: 0.8904 - val_loss: 0.4767 - val_acc: 0.8569\n",
      "Epoch 255/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3971 - acc: 0.8911 - val_loss: 0.4764 - val_acc: 0.8569\n",
      "Epoch 256/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3969 - acc: 0.8904 - val_loss: 0.4780 - val_acc: 0.8588\n",
      "Epoch 257/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3969 - acc: 0.8931 - val_loss: 0.4762 - val_acc: 0.8529\n",
      "Epoch 258/3000\n",
      "1506/1506 [==============================] - 0s 134us/step - loss: 0.3965 - acc: 0.8911 - val_loss: 0.4755 - val_acc: 0.8529\n",
      "Epoch 259/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3963 - acc: 0.8911 - val_loss: 0.4750 - val_acc: 0.8529\n",
      "Epoch 260/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3958 - acc: 0.8904 - val_loss: 0.4747 - val_acc: 0.8569\n",
      "Epoch 261/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3957 - acc: 0.8911 - val_loss: 0.4742 - val_acc: 0.8588\n",
      "Epoch 262/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3954 - acc: 0.8911 - val_loss: 0.4739 - val_acc: 0.8569\n",
      "Epoch 263/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3950 - acc: 0.8904 - val_loss: 0.4737 - val_acc: 0.8569\n",
      "Epoch 264/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3948 - acc: 0.8898 - val_loss: 0.4736 - val_acc: 0.8569\n",
      "Epoch 265/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3946 - acc: 0.8904 - val_loss: 0.4733 - val_acc: 0.8569\n",
      "Epoch 266/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3942 - acc: 0.8911 - val_loss: 0.4734 - val_acc: 0.8549\n",
      "Epoch 267/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3941 - acc: 0.8891 - val_loss: 0.4731 - val_acc: 0.8569\n",
      "Epoch 268/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3939 - acc: 0.8911 - val_loss: 0.4731 - val_acc: 0.8529\n",
      "Epoch 269/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3935 - acc: 0.8898 - val_loss: 0.4727 - val_acc: 0.8549\n",
      "Epoch 270/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3933 - acc: 0.8911 - val_loss: 0.4725 - val_acc: 0.8549\n",
      "Epoch 271/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3932 - acc: 0.8891 - val_loss: 0.4736 - val_acc: 0.8509\n",
      "Epoch 272/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3932 - acc: 0.8898 - val_loss: 0.4726 - val_acc: 0.8569\n",
      "Epoch 273/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3927 - acc: 0.8904 - val_loss: 0.4735 - val_acc: 0.8588\n",
      "Epoch 274/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3931 - acc: 0.8898 - val_loss: 0.4724 - val_acc: 0.8569\n",
      "Epoch 275/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3923 - acc: 0.8904 - val_loss: 0.4720 - val_acc: 0.8569\n",
      "Epoch 276/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3919 - acc: 0.8924 - val_loss: 0.4719 - val_acc: 0.8569\n",
      "Epoch 277/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3918 - acc: 0.8918 - val_loss: 0.4717 - val_acc: 0.8588\n",
      "Epoch 278/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3915 - acc: 0.8911 - val_loss: 0.4716 - val_acc: 0.8569\n",
      "Epoch 279/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3913 - acc: 0.8904 - val_loss: 0.4713 - val_acc: 0.8569\n",
      "Epoch 280/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3912 - acc: 0.8911 - val_loss: 0.4706 - val_acc: 0.8569\n",
      "Epoch 281/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3909 - acc: 0.8911 - val_loss: 0.4705 - val_acc: 0.8588\n",
      "Epoch 282/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3909 - acc: 0.8884 - val_loss: 0.4703 - val_acc: 0.8569\n",
      "Epoch 283/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3903 - acc: 0.8898 - val_loss: 0.4700 - val_acc: 0.8529\n",
      "Epoch 284/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3902 - acc: 0.8911 - val_loss: 0.4695 - val_acc: 0.8549\n",
      "Epoch 285/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3898 - acc: 0.8898 - val_loss: 0.4694 - val_acc: 0.8549\n",
      "Epoch 286/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3897 - acc: 0.8898 - val_loss: 0.4694 - val_acc: 0.8529\n",
      "Epoch 287/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3894 - acc: 0.8891 - val_loss: 0.4693 - val_acc: 0.8529\n",
      "Epoch 288/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3892 - acc: 0.8884 - val_loss: 0.4692 - val_acc: 0.8529\n",
      "Epoch 289/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3891 - acc: 0.8878 - val_loss: 0.4695 - val_acc: 0.8549\n",
      "Epoch 290/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3889 - acc: 0.8898 - val_loss: 0.4689 - val_acc: 0.8569\n",
      "Epoch 291/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3886 - acc: 0.8911 - val_loss: 0.4686 - val_acc: 0.8549\n",
      "Epoch 292/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3884 - acc: 0.8911 - val_loss: 0.4685 - val_acc: 0.8569\n",
      "Epoch 293/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3882 - acc: 0.8924 - val_loss: 0.4680 - val_acc: 0.8569\n",
      "Epoch 294/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3881 - acc: 0.8898 - val_loss: 0.4679 - val_acc: 0.8569\n",
      "Epoch 295/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3876 - acc: 0.8904 - val_loss: 0.4680 - val_acc: 0.8569\n",
      "Epoch 296/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3877 - acc: 0.8904 - val_loss: 0.4675 - val_acc: 0.8569\n",
      "Epoch 297/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3873 - acc: 0.8904 - val_loss: 0.4669 - val_acc: 0.8569\n",
      "Epoch 298/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3885 - acc: 0.8898 - val_loss: 0.4654 - val_acc: 0.8588\n",
      "Epoch 299/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3873 - acc: 0.8898 - val_loss: 0.4654 - val_acc: 0.8549\n",
      "Epoch 300/3000\n",
      "1506/1506 [==============================] - 0s 92us/step - loss: 0.3869 - acc: 0.8904 - val_loss: 0.4654 - val_acc: 0.8569\n",
      "Epoch 301/3000\n",
      "1506/1506 [==============================] - 0s 151us/step - loss: 0.3867 - acc: 0.8898 - val_loss: 0.4655 - val_acc: 0.8549\n",
      "Epoch 302/3000\n",
      "1506/1506 [==============================] - 0s 127us/step - loss: 0.3864 - acc: 0.8904 - val_loss: 0.4646 - val_acc: 0.8569\n",
      "Epoch 303/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3863 - acc: 0.8904 - val_loss: 0.4645 - val_acc: 0.8588\n",
      "Epoch 304/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3860 - acc: 0.8891 - val_loss: 0.4646 - val_acc: 0.8588\n",
      "Epoch 305/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3858 - acc: 0.8898 - val_loss: 0.4648 - val_acc: 0.8588\n",
      "Epoch 306/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3854 - acc: 0.8911 - val_loss: 0.4647 - val_acc: 0.8569\n",
      "Epoch 307/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3853 - acc: 0.8911 - val_loss: 0.4651 - val_acc: 0.8588\n",
      "Epoch 308/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3851 - acc: 0.8898 - val_loss: 0.4652 - val_acc: 0.8529\n",
      "Epoch 309/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.3851 - acc: 0.8918 - val_loss: 0.4651 - val_acc: 0.8549\n",
      "Epoch 310/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3847 - acc: 0.8911 - val_loss: 0.4647 - val_acc: 0.8588\n",
      "Epoch 311/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3846 - acc: 0.8911 - val_loss: 0.4644 - val_acc: 0.8588\n",
      "Epoch 312/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3843 - acc: 0.8924 - val_loss: 0.4642 - val_acc: 0.8588\n",
      "Epoch 313/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3842 - acc: 0.8911 - val_loss: 0.4631 - val_acc: 0.8588\n",
      "Epoch 314/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3842 - acc: 0.8904 - val_loss: 0.4639 - val_acc: 0.8529\n",
      "Epoch 315/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3842 - acc: 0.8918 - val_loss: 0.4634 - val_acc: 0.8588\n",
      "Epoch 316/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3836 - acc: 0.8924 - val_loss: 0.4636 - val_acc: 0.8569\n",
      "Epoch 317/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3834 - acc: 0.8911 - val_loss: 0.4637 - val_acc: 0.8588\n",
      "Epoch 318/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3834 - acc: 0.8918 - val_loss: 0.4627 - val_acc: 0.8569\n",
      "Epoch 319/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3830 - acc: 0.8924 - val_loss: 0.4630 - val_acc: 0.8588\n",
      "Epoch 320/3000\n",
      "1506/1506 [==============================] - 0s 108us/step - loss: 0.3829 - acc: 0.8918 - val_loss: 0.4626 - val_acc: 0.8569\n",
      "Epoch 321/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3826 - acc: 0.8918 - val_loss: 0.4625 - val_acc: 0.8549\n",
      "Epoch 322/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3824 - acc: 0.8938 - val_loss: 0.4625 - val_acc: 0.8588\n",
      "Epoch 323/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3821 - acc: 0.8924 - val_loss: 0.4624 - val_acc: 0.8569\n",
      "Epoch 324/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3820 - acc: 0.8924 - val_loss: 0.4610 - val_acc: 0.8529\n",
      "Epoch 325/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3831 - acc: 0.8911 - val_loss: 0.4612 - val_acc: 0.8628\n",
      "Epoch 326/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3825 - acc: 0.8918 - val_loss: 0.4603 - val_acc: 0.8549\n",
      "Epoch 327/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3817 - acc: 0.8904 - val_loss: 0.4606 - val_acc: 0.8569\n",
      "Epoch 328/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3813 - acc: 0.8911 - val_loss: 0.4607 - val_acc: 0.8588\n",
      "Epoch 329/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3812 - acc: 0.8918 - val_loss: 0.4606 - val_acc: 0.8588\n",
      "Epoch 330/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3810 - acc: 0.8918 - val_loss: 0.4608 - val_acc: 0.8569\n",
      "Epoch 331/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3809 - acc: 0.8918 - val_loss: 0.4607 - val_acc: 0.8549\n",
      "Epoch 332/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3807 - acc: 0.8918 - val_loss: 0.4605 - val_acc: 0.8569\n",
      "Epoch 333/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3804 - acc: 0.8918 - val_loss: 0.4604 - val_acc: 0.8588\n",
      "Epoch 334/3000\n",
      "1506/1506 [==============================] - 0s 143us/step - loss: 0.3803 - acc: 0.8918 - val_loss: 0.4603 - val_acc: 0.8569\n",
      "Epoch 335/3000\n",
      "1506/1506 [==============================] - 0s 141us/step - loss: 0.3801 - acc: 0.8931 - val_loss: 0.4603 - val_acc: 0.8569\n",
      "Epoch 336/3000\n",
      "1506/1506 [==============================] - 0s 173us/step - loss: 0.3798 - acc: 0.8924 - val_loss: 0.4603 - val_acc: 0.8549\n",
      "Epoch 337/3000\n",
      "1506/1506 [==============================] - 0s 143us/step - loss: 0.3799 - acc: 0.8911 - val_loss: 0.4600 - val_acc: 0.8588\n",
      "Epoch 338/3000\n",
      "1506/1506 [==============================] - 0s 186us/step - loss: 0.3796 - acc: 0.8931 - val_loss: 0.4599 - val_acc: 0.8569\n",
      "Epoch 339/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3794 - acc: 0.8931 - val_loss: 0.4589 - val_acc: 0.8588\n",
      "Epoch 340/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3796 - acc: 0.8918 - val_loss: 0.4589 - val_acc: 0.8588\n",
      "Epoch 341/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3791 - acc: 0.8911 - val_loss: 0.4587 - val_acc: 0.8588\n",
      "Epoch 342/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3789 - acc: 0.8918 - val_loss: 0.4587 - val_acc: 0.8569\n",
      "Epoch 343/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3786 - acc: 0.8924 - val_loss: 0.4605 - val_acc: 0.8588\n",
      "Epoch 344/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3789 - acc: 0.8931 - val_loss: 0.4597 - val_acc: 0.8569\n",
      "Epoch 345/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3785 - acc: 0.8931 - val_loss: 0.4592 - val_acc: 0.8588\n",
      "Epoch 346/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3782 - acc: 0.8924 - val_loss: 0.4589 - val_acc: 0.8569\n",
      "Epoch 347/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3781 - acc: 0.8924 - val_loss: 0.4587 - val_acc: 0.8569\n",
      "Epoch 348/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3779 - acc: 0.8924 - val_loss: 0.4586 - val_acc: 0.8569\n",
      "Epoch 349/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3777 - acc: 0.8924 - val_loss: 0.4587 - val_acc: 0.8588\n",
      "Epoch 350/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3777 - acc: 0.8924 - val_loss: 0.4580 - val_acc: 0.8569\n",
      "Epoch 351/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3775 - acc: 0.8931 - val_loss: 0.4579 - val_acc: 0.8569\n",
      "Epoch 352/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3773 - acc: 0.8931 - val_loss: 0.4578 - val_acc: 0.8569\n",
      "Epoch 353/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3771 - acc: 0.8938 - val_loss: 0.4573 - val_acc: 0.8588\n",
      "Epoch 354/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3771 - acc: 0.8918 - val_loss: 0.4572 - val_acc: 0.8569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3768 - acc: 0.8931 - val_loss: 0.4572 - val_acc: 0.8588\n",
      "Epoch 356/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3766 - acc: 0.8924 - val_loss: 0.4572 - val_acc: 0.8569\n",
      "Epoch 357/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3764 - acc: 0.8938 - val_loss: 0.4572 - val_acc: 0.8588\n",
      "Epoch 358/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3764 - acc: 0.8918 - val_loss: 0.4569 - val_acc: 0.8608\n",
      "Epoch 359/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3761 - acc: 0.8924 - val_loss: 0.4567 - val_acc: 0.8569\n",
      "Epoch 360/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3760 - acc: 0.8924 - val_loss: 0.4565 - val_acc: 0.8588\n",
      "Epoch 361/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3758 - acc: 0.8931 - val_loss: 0.4565 - val_acc: 0.8569\n",
      "Epoch 362/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3757 - acc: 0.8938 - val_loss: 0.4561 - val_acc: 0.8569\n",
      "Epoch 363/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3755 - acc: 0.8918 - val_loss: 0.4561 - val_acc: 0.8588\n",
      "Epoch 364/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3753 - acc: 0.8938 - val_loss: 0.4560 - val_acc: 0.8588\n",
      "Epoch 365/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3753 - acc: 0.8924 - val_loss: 0.4559 - val_acc: 0.8608\n",
      "Epoch 366/3000\n",
      "1506/1506 [==============================] - 0s 123us/step - loss: 0.3752 - acc: 0.8931 - val_loss: 0.4559 - val_acc: 0.8588\n",
      "Epoch 367/3000\n",
      "1506/1506 [==============================] - 0s 143us/step - loss: 0.3750 - acc: 0.8938 - val_loss: 0.4560 - val_acc: 0.8588\n",
      "Epoch 368/3000\n",
      "1506/1506 [==============================] - 0s 146us/step - loss: 0.3748 - acc: 0.8931 - val_loss: 0.4556 - val_acc: 0.8588\n",
      "Epoch 369/3000\n",
      "1506/1506 [==============================] - 0s 186us/step - loss: 0.3746 - acc: 0.8938 - val_loss: 0.4560 - val_acc: 0.8628\n",
      "Epoch 370/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3746 - acc: 0.8951 - val_loss: 0.4555 - val_acc: 0.8569\n",
      "Epoch 371/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3743 - acc: 0.8931 - val_loss: 0.4554 - val_acc: 0.8588\n",
      "Epoch 372/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3743 - acc: 0.8931 - val_loss: 0.4551 - val_acc: 0.8588\n",
      "Epoch 373/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3740 - acc: 0.8938 - val_loss: 0.4550 - val_acc: 0.8588\n",
      "Epoch 374/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3739 - acc: 0.8944 - val_loss: 0.4548 - val_acc: 0.8588\n",
      "Epoch 375/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3737 - acc: 0.8931 - val_loss: 0.4547 - val_acc: 0.8569\n",
      "Epoch 376/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3735 - acc: 0.8924 - val_loss: 0.4547 - val_acc: 0.8588\n",
      "Epoch 377/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3735 - acc: 0.8938 - val_loss: 0.4546 - val_acc: 0.8588\n",
      "Epoch 378/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3734 - acc: 0.8938 - val_loss: 0.4543 - val_acc: 0.8588\n",
      "Epoch 379/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3731 - acc: 0.8951 - val_loss: 0.4541 - val_acc: 0.8569\n",
      "Epoch 380/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3730 - acc: 0.8951 - val_loss: 0.4548 - val_acc: 0.8569\n",
      "Epoch 381/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3732 - acc: 0.8924 - val_loss: 0.4549 - val_acc: 0.8608\n",
      "Epoch 382/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3729 - acc: 0.8944 - val_loss: 0.4545 - val_acc: 0.8608\n",
      "Epoch 383/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3726 - acc: 0.8944 - val_loss: 0.4542 - val_acc: 0.8628\n",
      "Epoch 384/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3725 - acc: 0.8931 - val_loss: 0.4540 - val_acc: 0.8608\n",
      "Epoch 385/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3723 - acc: 0.8938 - val_loss: 0.4540 - val_acc: 0.8608\n",
      "Epoch 386/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.3722 - acc: 0.8944 - val_loss: 0.4537 - val_acc: 0.8608\n",
      "Epoch 387/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3721 - acc: 0.8938 - val_loss: 0.4535 - val_acc: 0.8628\n",
      "Epoch 388/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3718 - acc: 0.8944 - val_loss: 0.4533 - val_acc: 0.8608\n",
      "Epoch 389/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3718 - acc: 0.8931 - val_loss: 0.4535 - val_acc: 0.8608\n",
      "Epoch 390/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3719 - acc: 0.8944 - val_loss: 0.4525 - val_acc: 0.8608\n",
      "Epoch 391/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3716 - acc: 0.8944 - val_loss: 0.4525 - val_acc: 0.8608\n",
      "Epoch 392/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3716 - acc: 0.8944 - val_loss: 0.4523 - val_acc: 0.8608\n",
      "Epoch 393/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3712 - acc: 0.8951 - val_loss: 0.4520 - val_acc: 0.8628\n",
      "Epoch 394/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3711 - acc: 0.8944 - val_loss: 0.4521 - val_acc: 0.8608\n",
      "Epoch 395/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3709 - acc: 0.8944 - val_loss: 0.4519 - val_acc: 0.8628\n",
      "Epoch 396/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3707 - acc: 0.8938 - val_loss: 0.4535 - val_acc: 0.8608\n",
      "Epoch 397/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3709 - acc: 0.8944 - val_loss: 0.4523 - val_acc: 0.8628\n",
      "Epoch 398/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3705 - acc: 0.8944 - val_loss: 0.4523 - val_acc: 0.8628\n",
      "Epoch 399/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3704 - acc: 0.8944 - val_loss: 0.4520 - val_acc: 0.8608\n",
      "Epoch 400/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3703 - acc: 0.8938 - val_loss: 0.4518 - val_acc: 0.8608\n",
      "Epoch 401/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3701 - acc: 0.8944 - val_loss: 0.4516 - val_acc: 0.8628\n",
      "Epoch 402/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3700 - acc: 0.8951 - val_loss: 0.4513 - val_acc: 0.8608\n",
      "Epoch 403/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3698 - acc: 0.8938 - val_loss: 0.4508 - val_acc: 0.8628\n",
      "Epoch 404/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3697 - acc: 0.8938 - val_loss: 0.4504 - val_acc: 0.8628\n",
      "Epoch 405/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3696 - acc: 0.8944 - val_loss: 0.4502 - val_acc: 0.8608\n",
      "Epoch 406/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3695 - acc: 0.8951 - val_loss: 0.4501 - val_acc: 0.8628\n",
      "Epoch 407/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3694 - acc: 0.8944 - val_loss: 0.4501 - val_acc: 0.8628\n",
      "Epoch 408/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3691 - acc: 0.8958 - val_loss: 0.4502 - val_acc: 0.8628\n",
      "Epoch 409/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3690 - acc: 0.8951 - val_loss: 0.4500 - val_acc: 0.8628\n",
      "Epoch 410/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3689 - acc: 0.8951 - val_loss: 0.4499 - val_acc: 0.8608\n",
      "Epoch 411/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3688 - acc: 0.8951 - val_loss: 0.4497 - val_acc: 0.8628\n",
      "Epoch 412/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3686 - acc: 0.8958 - val_loss: 0.4498 - val_acc: 0.8608\n",
      "Epoch 413/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3685 - acc: 0.8964 - val_loss: 0.4493 - val_acc: 0.8628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3685 - acc: 0.8951 - val_loss: 0.4490 - val_acc: 0.8628\n",
      "Epoch 415/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3682 - acc: 0.8958 - val_loss: 0.4491 - val_acc: 0.8628\n",
      "Epoch 416/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3682 - acc: 0.8951 - val_loss: 0.4493 - val_acc: 0.8628\n",
      "Epoch 417/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3681 - acc: 0.8951 - val_loss: 0.4480 - val_acc: 0.8628\n",
      "Epoch 418/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3681 - acc: 0.8944 - val_loss: 0.4473 - val_acc: 0.8608\n",
      "Epoch 419/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3688 - acc: 0.8944 - val_loss: 0.4467 - val_acc: 0.8588\n",
      "Epoch 420/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3680 - acc: 0.8938 - val_loss: 0.4470 - val_acc: 0.8608\n",
      "Epoch 421/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3676 - acc: 0.8944 - val_loss: 0.4473 - val_acc: 0.8628\n",
      "Epoch 422/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3674 - acc: 0.8944 - val_loss: 0.4479 - val_acc: 0.8628\n",
      "Epoch 423/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3674 - acc: 0.8958 - val_loss: 0.4477 - val_acc: 0.8628\n",
      "Epoch 424/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3672 - acc: 0.8944 - val_loss: 0.4478 - val_acc: 0.8628\n",
      "Epoch 425/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3671 - acc: 0.8951 - val_loss: 0.4479 - val_acc: 0.8608\n",
      "Epoch 426/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3669 - acc: 0.8958 - val_loss: 0.4478 - val_acc: 0.8608\n",
      "Epoch 427/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3667 - acc: 0.8951 - val_loss: 0.4479 - val_acc: 0.8648\n",
      "Epoch 428/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3666 - acc: 0.8951 - val_loss: 0.4484 - val_acc: 0.8648\n",
      "Epoch 429/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3666 - acc: 0.8971 - val_loss: 0.4479 - val_acc: 0.8628\n",
      "Epoch 430/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3668 - acc: 0.8964 - val_loss: 0.4476 - val_acc: 0.8608\n",
      "Epoch 431/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3664 - acc: 0.8964 - val_loss: 0.4464 - val_acc: 0.8608\n",
      "Epoch 432/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3664 - acc: 0.8944 - val_loss: 0.4468 - val_acc: 0.8628\n",
      "Epoch 433/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3662 - acc: 0.8931 - val_loss: 0.4480 - val_acc: 0.8628\n",
      "Epoch 434/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3665 - acc: 0.8951 - val_loss: 0.4474 - val_acc: 0.8628\n",
      "Epoch 435/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3660 - acc: 0.8958 - val_loss: 0.4477 - val_acc: 0.8588\n",
      "Epoch 436/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3659 - acc: 0.8944 - val_loss: 0.4474 - val_acc: 0.8608\n",
      "Epoch 437/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3656 - acc: 0.8958 - val_loss: 0.4472 - val_acc: 0.8628\n",
      "Epoch 438/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3655 - acc: 0.8958 - val_loss: 0.4474 - val_acc: 0.8648\n",
      "Epoch 439/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3656 - acc: 0.8944 - val_loss: 0.4471 - val_acc: 0.8648\n",
      "Epoch 440/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3653 - acc: 0.8958 - val_loss: 0.4469 - val_acc: 0.8648\n",
      "Epoch 441/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3652 - acc: 0.8951 - val_loss: 0.4468 - val_acc: 0.8628\n",
      "Epoch 442/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3650 - acc: 0.8958 - val_loss: 0.4467 - val_acc: 0.8628\n",
      "Epoch 443/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3649 - acc: 0.8958 - val_loss: 0.4468 - val_acc: 0.8648\n",
      "Epoch 444/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3647 - acc: 0.8958 - val_loss: 0.4466 - val_acc: 0.8648\n",
      "Epoch 445/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3646 - acc: 0.8958 - val_loss: 0.4465 - val_acc: 0.8648\n",
      "Epoch 446/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3645 - acc: 0.8951 - val_loss: 0.4463 - val_acc: 0.8648\n",
      "Epoch 447/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3644 - acc: 0.8951 - val_loss: 0.4463 - val_acc: 0.8628\n",
      "Epoch 448/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3644 - acc: 0.8958 - val_loss: 0.4470 - val_acc: 0.8608\n",
      "Epoch 449/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3645 - acc: 0.8958 - val_loss: 0.4466 - val_acc: 0.8628\n",
      "Epoch 450/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3641 - acc: 0.8951 - val_loss: 0.4463 - val_acc: 0.8628\n",
      "Epoch 451/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3640 - acc: 0.8971 - val_loss: 0.4464 - val_acc: 0.8608\n",
      "Epoch 452/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3640 - acc: 0.8964 - val_loss: 0.4458 - val_acc: 0.8628\n",
      "Epoch 453/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3638 - acc: 0.8964 - val_loss: 0.4453 - val_acc: 0.8648\n",
      "Epoch 454/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3636 - acc: 0.8958 - val_loss: 0.4454 - val_acc: 0.8648\n",
      "Epoch 455/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3636 - acc: 0.8958 - val_loss: 0.4454 - val_acc: 0.8648\n",
      "Epoch 456/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3635 - acc: 0.8958 - val_loss: 0.4451 - val_acc: 0.8648\n",
      "Epoch 457/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3632 - acc: 0.8958 - val_loss: 0.4452 - val_acc: 0.8648\n",
      "Epoch 458/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3638 - acc: 0.8958 - val_loss: 0.4445 - val_acc: 0.8569\n",
      "Epoch 459/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3631 - acc: 0.8958 - val_loss: 0.4447 - val_acc: 0.8588\n",
      "Epoch 460/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3632 - acc: 0.8984 - val_loss: 0.4443 - val_acc: 0.8588\n",
      "Epoch 461/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3629 - acc: 0.8958 - val_loss: 0.4448 - val_acc: 0.8608\n",
      "Epoch 462/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3630 - acc: 0.8958 - val_loss: 0.4445 - val_acc: 0.8628\n",
      "Epoch 463/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3627 - acc: 0.8951 - val_loss: 0.4433 - val_acc: 0.8588\n",
      "Epoch 464/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3628 - acc: 0.8964 - val_loss: 0.4433 - val_acc: 0.8628\n",
      "Epoch 465/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3624 - acc: 0.8971 - val_loss: 0.4433 - val_acc: 0.8608\n",
      "Epoch 466/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3626 - acc: 0.8964 - val_loss: 0.4429 - val_acc: 0.8628\n",
      "Epoch 467/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3624 - acc: 0.8958 - val_loss: 0.4431 - val_acc: 0.8648\n",
      "Epoch 468/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3622 - acc: 0.8958 - val_loss: 0.4431 - val_acc: 0.8648\n",
      "Epoch 469/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3621 - acc: 0.8944 - val_loss: 0.4431 - val_acc: 0.8628\n",
      "Epoch 470/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3619 - acc: 0.8971 - val_loss: 0.4431 - val_acc: 0.8628\n",
      "Epoch 471/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3617 - acc: 0.8964 - val_loss: 0.4431 - val_acc: 0.8628\n",
      "Epoch 472/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3617 - acc: 0.8977 - val_loss: 0.4432 - val_acc: 0.8608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3616 - acc: 0.8951 - val_loss: 0.4434 - val_acc: 0.8588\n",
      "Epoch 474/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3615 - acc: 0.8958 - val_loss: 0.4433 - val_acc: 0.8628\n",
      "Epoch 475/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3614 - acc: 0.8964 - val_loss: 0.4431 - val_acc: 0.8628\n",
      "Epoch 476/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3612 - acc: 0.8977 - val_loss: 0.4431 - val_acc: 0.8648\n",
      "Epoch 477/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3612 - acc: 0.8964 - val_loss: 0.4430 - val_acc: 0.8648\n",
      "Epoch 478/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3611 - acc: 0.8964 - val_loss: 0.4431 - val_acc: 0.8628\n",
      "Epoch 479/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3609 - acc: 0.8964 - val_loss: 0.4428 - val_acc: 0.8648\n",
      "Epoch 480/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3609 - acc: 0.8964 - val_loss: 0.4426 - val_acc: 0.8628\n",
      "Epoch 481/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3608 - acc: 0.8964 - val_loss: 0.4417 - val_acc: 0.8628\n",
      "Epoch 482/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3609 - acc: 0.8964 - val_loss: 0.4417 - val_acc: 0.8628\n",
      "Epoch 483/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3607 - acc: 0.8964 - val_loss: 0.4419 - val_acc: 0.8608\n",
      "Epoch 484/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3605 - acc: 0.8964 - val_loss: 0.4418 - val_acc: 0.8628\n",
      "Epoch 485/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3604 - acc: 0.8964 - val_loss: 0.4421 - val_acc: 0.8668\n",
      "Epoch 486/3000\n",
      "1506/1506 [==============================] - 0s 91us/step - loss: 0.3604 - acc: 0.8964 - val_loss: 0.4421 - val_acc: 0.8608\n",
      "Epoch 487/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3602 - acc: 0.8984 - val_loss: 0.4418 - val_acc: 0.8628\n",
      "Epoch 488/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3599 - acc: 0.8964 - val_loss: 0.4417 - val_acc: 0.8628\n",
      "Epoch 489/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.3599 - acc: 0.8958 - val_loss: 0.4417 - val_acc: 0.8628\n",
      "Epoch 490/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3599 - acc: 0.8971 - val_loss: 0.4418 - val_acc: 0.8648\n",
      "Epoch 491/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3598 - acc: 0.8958 - val_loss: 0.4418 - val_acc: 0.8648\n",
      "Epoch 492/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3598 - acc: 0.8964 - val_loss: 0.4416 - val_acc: 0.8648\n",
      "Epoch 493/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3595 - acc: 0.8971 - val_loss: 0.4418 - val_acc: 0.8648\n",
      "Epoch 494/3000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3732 - acc: 0.893 - 0s 62us/step - loss: 0.3594 - acc: 0.8964 - val_loss: 0.4414 - val_acc: 0.8668\n",
      "Epoch 495/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3593 - acc: 0.8964 - val_loss: 0.4414 - val_acc: 0.8648\n",
      "Epoch 496/3000\n",
      "1506/1506 [==============================] - 0s 61us/step - loss: 0.3592 - acc: 0.8958 - val_loss: 0.4413 - val_acc: 0.8648\n",
      "Epoch 497/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3591 - acc: 0.8971 - val_loss: 0.4411 - val_acc: 0.8648\n",
      "Epoch 498/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3590 - acc: 0.8964 - val_loss: 0.4412 - val_acc: 0.8648\n",
      "Epoch 499/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3589 - acc: 0.8951 - val_loss: 0.4411 - val_acc: 0.8668\n",
      "Epoch 500/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3588 - acc: 0.8951 - val_loss: 0.4407 - val_acc: 0.8608\n",
      "Epoch 501/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3588 - acc: 0.8971 - val_loss: 0.4407 - val_acc: 0.8648\n",
      "Epoch 502/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3587 - acc: 0.8964 - val_loss: 0.4407 - val_acc: 0.8668\n",
      "Epoch 503/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3585 - acc: 0.8964 - val_loss: 0.4423 - val_acc: 0.8648\n",
      "Epoch 504/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3586 - acc: 0.8971 - val_loss: 0.4423 - val_acc: 0.8668\n",
      "Epoch 505/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3585 - acc: 0.8984 - val_loss: 0.4423 - val_acc: 0.8648\n",
      "Epoch 506/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3586 - acc: 0.8964 - val_loss: 0.4416 - val_acc: 0.8628\n",
      "Epoch 507/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3584 - acc: 0.8971 - val_loss: 0.4410 - val_acc: 0.8628\n",
      "Epoch 508/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3581 - acc: 0.8977 - val_loss: 0.4406 - val_acc: 0.8648\n",
      "Epoch 509/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3580 - acc: 0.8971 - val_loss: 0.4404 - val_acc: 0.8668\n",
      "Epoch 510/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3579 - acc: 0.8964 - val_loss: 0.4396 - val_acc: 0.8628\n",
      "Epoch 511/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3580 - acc: 0.8971 - val_loss: 0.4394 - val_acc: 0.8668\n",
      "Epoch 512/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3578 - acc: 0.8977 - val_loss: 0.4395 - val_acc: 0.8648\n",
      "Epoch 513/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3577 - acc: 0.8971 - val_loss: 0.4395 - val_acc: 0.8648\n",
      "Epoch 514/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3575 - acc: 0.8964 - val_loss: 0.4377 - val_acc: 0.8628\n",
      "Epoch 515/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3585 - acc: 0.8944 - val_loss: 0.4375 - val_acc: 0.8628\n",
      "Epoch 516/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3577 - acc: 0.8971 - val_loss: 0.4371 - val_acc: 0.8608\n",
      "Epoch 517/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3575 - acc: 0.8964 - val_loss: 0.4374 - val_acc: 0.8628\n",
      "Epoch 518/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3573 - acc: 0.8951 - val_loss: 0.4377 - val_acc: 0.8628\n",
      "Epoch 519/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3572 - acc: 0.8971 - val_loss: 0.4377 - val_acc: 0.8608\n",
      "Epoch 520/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3572 - acc: 0.8944 - val_loss: 0.4378 - val_acc: 0.8608\n",
      "Epoch 521/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3569 - acc: 0.8964 - val_loss: 0.4375 - val_acc: 0.8628\n",
      "Epoch 522/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3569 - acc: 0.8951 - val_loss: 0.4381 - val_acc: 0.8628\n",
      "Epoch 523/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3567 - acc: 0.8971 - val_loss: 0.4381 - val_acc: 0.8628\n",
      "Epoch 524/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3568 - acc: 0.8964 - val_loss: 0.4382 - val_acc: 0.8648\n",
      "Epoch 525/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3566 - acc: 0.8977 - val_loss: 0.4382 - val_acc: 0.8648\n",
      "Epoch 526/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3566 - acc: 0.8971 - val_loss: 0.4382 - val_acc: 0.8648\n",
      "Epoch 527/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3563 - acc: 0.8964 - val_loss: 0.4381 - val_acc: 0.8628\n",
      "Epoch 528/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3562 - acc: 0.8984 - val_loss: 0.4382 - val_acc: 0.8608\n",
      "Epoch 529/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3562 - acc: 0.8971 - val_loss: 0.4391 - val_acc: 0.8608\n",
      "Epoch 530/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3564 - acc: 0.8977 - val_loss: 0.4389 - val_acc: 0.8648\n",
      "Epoch 531/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3561 - acc: 0.8971 - val_loss: 0.4385 - val_acc: 0.8628\n",
      "Epoch 532/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3560 - acc: 0.8964 - val_loss: 0.4383 - val_acc: 0.8608\n",
      "Epoch 533/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3560 - acc: 0.8971 - val_loss: 0.4381 - val_acc: 0.8628\n",
      "Epoch 534/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3557 - acc: 0.8971 - val_loss: 0.4381 - val_acc: 0.8608\n",
      "Epoch 535/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3556 - acc: 0.8964 - val_loss: 0.4379 - val_acc: 0.8608\n",
      "Epoch 536/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3556 - acc: 0.8964 - val_loss: 0.4381 - val_acc: 0.8648\n",
      "Epoch 00536: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adagrad,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist4=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/3000\n",
      "1506/1506 [==============================] - 1s 696us/step - loss: 2.2730 - acc: 0.1328 - val_loss: 2.1839 - val_acc: 0.2366\n",
      "Epoch 2/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 2.1280 - acc: 0.2151 - val_loss: 2.0561 - val_acc: 0.2366\n",
      "Epoch 3/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 2.0172 - acc: 0.2224 - val_loss: 1.9556 - val_acc: 0.2604\n",
      "Epoch 4/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 1.9266 - acc: 0.2928 - val_loss: 1.8746 - val_acc: 0.4095\n",
      "Epoch 5/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 1.8485 - acc: 0.3884 - val_loss: 1.8041 - val_acc: 0.4414\n",
      "Epoch 6/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 1.7767 - acc: 0.3951 - val_loss: 1.7360 - val_acc: 0.4414\n",
      "Epoch 7/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 1.7078 - acc: 0.3958 - val_loss: 1.6723 - val_acc: 0.4414\n",
      "Epoch 8/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 1.6409 - acc: 0.3958 - val_loss: 1.6080 - val_acc: 0.4414\n",
      "Epoch 9/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 1.5788 - acc: 0.3977 - val_loss: 1.5549 - val_acc: 0.4414\n",
      "Epoch 10/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 1.5223 - acc: 0.3991 - val_loss: 1.5055 - val_acc: 0.4453\n",
      "Epoch 11/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 1.4721 - acc: 0.4024 - val_loss: 1.4613 - val_acc: 0.4473\n",
      "Epoch 12/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.4265 - acc: 0.4077 - val_loss: 1.4201 - val_acc: 0.4652\n",
      "Epoch 13/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.3835 - acc: 0.4283 - val_loss: 1.3799 - val_acc: 0.4851\n",
      "Epoch 14/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 1.3402 - acc: 0.4475 - val_loss: 1.3392 - val_acc: 0.5149\n",
      "Epoch 15/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.2988 - acc: 0.4954 - val_loss: 1.3015 - val_acc: 0.5487\n",
      "Epoch 16/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 1.2598 - acc: 0.5863 - val_loss: 1.2681 - val_acc: 0.5924\n",
      "Epoch 17/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 1.2229 - acc: 0.6222 - val_loss: 1.2325 - val_acc: 0.6322\n",
      "Epoch 18/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 1.1846 - acc: 0.6454 - val_loss: 1.1996 - val_acc: 0.6620\n",
      "Epoch 19/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 1.1480 - acc: 0.6819 - val_loss: 1.1621 - val_acc: 0.6660\n",
      "Epoch 20/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 1.1141 - acc: 0.6680 - val_loss: 1.1357 - val_acc: 0.7197\n",
      "Epoch 21/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 1.0792 - acc: 0.7198 - val_loss: 1.0975 - val_acc: 0.7177\n",
      "Epoch 22/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 1.0446 - acc: 0.7231 - val_loss: 1.0648 - val_acc: 0.7237\n",
      "Epoch 23/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 1.0141 - acc: 0.7437 - val_loss: 1.0370 - val_acc: 0.7555\n",
      "Epoch 24/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.9843 - acc: 0.7689 - val_loss: 1.0081 - val_acc: 0.7753\n",
      "Epoch 25/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.9550 - acc: 0.7935 - val_loss: 0.9795 - val_acc: 0.7694\n",
      "Epoch 26/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.9267 - acc: 0.7955 - val_loss: 0.9539 - val_acc: 0.7913\n",
      "Epoch 27/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.8993 - acc: 0.8234 - val_loss: 0.9275 - val_acc: 0.7952\n",
      "Epoch 28/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.8717 - acc: 0.8081 - val_loss: 0.9047 - val_acc: 0.8151\n",
      "Epoch 29/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.8463 - acc: 0.8327 - val_loss: 0.8763 - val_acc: 0.8072\n",
      "Epoch 30/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.8206 - acc: 0.8287 - val_loss: 0.8519 - val_acc: 0.8131\n",
      "Epoch 31/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.7977 - acc: 0.8274 - val_loss: 0.8324 - val_acc: 0.8171\n",
      "Epoch 32/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.7743 - acc: 0.8420 - val_loss: 0.8074 - val_acc: 0.8171\n",
      "Epoch 33/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.7513 - acc: 0.8367 - val_loss: 0.7879 - val_acc: 0.8250\n",
      "Epoch 34/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.7300 - acc: 0.8459 - val_loss: 0.7682 - val_acc: 0.8310\n",
      "Epoch 35/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.7105 - acc: 0.8453 - val_loss: 0.7490 - val_acc: 0.8330\n",
      "Epoch 36/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6918 - acc: 0.8486 - val_loss: 0.7304 - val_acc: 0.8390\n",
      "Epoch 37/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.6754 - acc: 0.8526 - val_loss: 0.7162 - val_acc: 0.8350\n",
      "Epoch 38/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6597 - acc: 0.8473 - val_loss: 0.7019 - val_acc: 0.8410\n",
      "Epoch 39/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.6451 - acc: 0.8672 - val_loss: 0.6878 - val_acc: 0.8410\n",
      "Epoch 40/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.6324 - acc: 0.8559 - val_loss: 0.6757 - val_acc: 0.8410\n",
      "Epoch 41/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6188 - acc: 0.8572 - val_loss: 0.6621 - val_acc: 0.8449\n",
      "Epoch 42/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.6067 - acc: 0.8592 - val_loss: 0.6493 - val_acc: 0.8449\n",
      "Epoch 43/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5950 - acc: 0.8632 - val_loss: 0.6398 - val_acc: 0.8449\n",
      "Epoch 44/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5832 - acc: 0.8586 - val_loss: 0.6292 - val_acc: 0.8509\n",
      "Epoch 45/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5737 - acc: 0.8645 - val_loss: 0.6216 - val_acc: 0.8449\n",
      "Epoch 46/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5646 - acc: 0.8712 - val_loss: 0.6133 - val_acc: 0.8489\n",
      "Epoch 47/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5558 - acc: 0.8679 - val_loss: 0.6041 - val_acc: 0.8489\n",
      "Epoch 48/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5474 - acc: 0.8692 - val_loss: 0.5969 - val_acc: 0.8489\n",
      "Epoch 49/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.5403 - acc: 0.8705 - val_loss: 0.5886 - val_acc: 0.8489\n",
      "Epoch 50/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5321 - acc: 0.8765 - val_loss: 0.5823 - val_acc: 0.8449\n",
      "Epoch 51/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.5253 - acc: 0.8705 - val_loss: 0.5747 - val_acc: 0.8489\n",
      "Epoch 52/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.5180 - acc: 0.8772 - val_loss: 0.5685 - val_acc: 0.8469\n",
      "Epoch 53/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.5115 - acc: 0.8738 - val_loss: 0.5614 - val_acc: 0.8509\n",
      "Epoch 54/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.5053 - acc: 0.8758 - val_loss: 0.5566 - val_acc: 0.8529\n",
      "Epoch 55/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4996 - acc: 0.8752 - val_loss: 0.5524 - val_acc: 0.8509\n",
      "Epoch 56/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4946 - acc: 0.8785 - val_loss: 0.5483 - val_acc: 0.8549\n",
      "Epoch 57/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4912 - acc: 0.8785 - val_loss: 0.5425 - val_acc: 0.8529\n",
      "Epoch 58/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4862 - acc: 0.8765 - val_loss: 0.5391 - val_acc: 0.8549\n",
      "Epoch 59/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4819 - acc: 0.8772 - val_loss: 0.5348 - val_acc: 0.8549\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4789 - acc: 0.8798 - val_loss: 0.5334 - val_acc: 0.8529\n",
      "Epoch 61/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4745 - acc: 0.8778 - val_loss: 0.5290 - val_acc: 0.8529\n",
      "Epoch 62/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.4708 - acc: 0.8805 - val_loss: 0.5253 - val_acc: 0.8569\n",
      "Epoch 63/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4668 - acc: 0.8792 - val_loss: 0.5236 - val_acc: 0.8529\n",
      "Epoch 64/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4636 - acc: 0.8831 - val_loss: 0.5209 - val_acc: 0.8509\n",
      "Epoch 65/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4606 - acc: 0.8785 - val_loss: 0.5171 - val_acc: 0.8569\n",
      "Epoch 66/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4568 - acc: 0.8805 - val_loss: 0.5130 - val_acc: 0.8569\n",
      "Epoch 67/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4539 - acc: 0.8825 - val_loss: 0.5108 - val_acc: 0.8549\n",
      "Epoch 68/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4502 - acc: 0.8818 - val_loss: 0.5064 - val_acc: 0.8608\n",
      "Epoch 69/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.4476 - acc: 0.8838 - val_loss: 0.5044 - val_acc: 0.8549\n",
      "Epoch 70/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4443 - acc: 0.8878 - val_loss: 0.5016 - val_acc: 0.8509\n",
      "Epoch 71/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.4422 - acc: 0.8818 - val_loss: 0.4985 - val_acc: 0.8529\n",
      "Epoch 72/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4370 - acc: 0.8884 - val_loss: 0.4969 - val_acc: 0.8549\n",
      "Epoch 73/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.4352 - acc: 0.8865 - val_loss: 0.4927 - val_acc: 0.8549\n",
      "Epoch 74/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4315 - acc: 0.8865 - val_loss: 0.4906 - val_acc: 0.8569\n",
      "Epoch 75/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4291 - acc: 0.8865 - val_loss: 0.4877 - val_acc: 0.8549\n",
      "Epoch 76/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4269 - acc: 0.8891 - val_loss: 0.4869 - val_acc: 0.8529\n",
      "Epoch 77/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4241 - acc: 0.8891 - val_loss: 0.4841 - val_acc: 0.8608\n",
      "Epoch 78/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4212 - acc: 0.8871 - val_loss: 0.4817 - val_acc: 0.8628\n",
      "Epoch 79/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4186 - acc: 0.8898 - val_loss: 0.4805 - val_acc: 0.8628\n",
      "Epoch 80/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.4169 - acc: 0.8898 - val_loss: 0.4782 - val_acc: 0.8608\n",
      "Epoch 81/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.4157 - acc: 0.8904 - val_loss: 0.4778 - val_acc: 0.8608\n",
      "Epoch 82/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4128 - acc: 0.8924 - val_loss: 0.4758 - val_acc: 0.8628\n",
      "Epoch 83/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4113 - acc: 0.8891 - val_loss: 0.4739 - val_acc: 0.8628\n",
      "Epoch 84/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.4101 - acc: 0.8865 - val_loss: 0.4719 - val_acc: 0.8608\n",
      "Epoch 85/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.4087 - acc: 0.8904 - val_loss: 0.4703 - val_acc: 0.8608\n",
      "Epoch 86/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.4075 - acc: 0.8911 - val_loss: 0.4710 - val_acc: 0.8688\n",
      "Epoch 87/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.4073 - acc: 0.8878 - val_loss: 0.4689 - val_acc: 0.8648\n",
      "Epoch 88/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4050 - acc: 0.8898 - val_loss: 0.4686 - val_acc: 0.8648\n",
      "Epoch 89/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.4039 - acc: 0.8898 - val_loss: 0.4672 - val_acc: 0.8648\n",
      "Epoch 90/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.4026 - acc: 0.8884 - val_loss: 0.4661 - val_acc: 0.8668\n",
      "Epoch 91/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4026 - acc: 0.8971 - val_loss: 0.4673 - val_acc: 0.8668\n",
      "Epoch 92/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3995 - acc: 0.8931 - val_loss: 0.4654 - val_acc: 0.8668\n",
      "Epoch 93/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3983 - acc: 0.8918 - val_loss: 0.4636 - val_acc: 0.8668\n",
      "Epoch 94/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3973 - acc: 0.8918 - val_loss: 0.4621 - val_acc: 0.8648\n",
      "Epoch 95/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3964 - acc: 0.8904 - val_loss: 0.4611 - val_acc: 0.8668\n",
      "Epoch 96/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3959 - acc: 0.8964 - val_loss: 0.4616 - val_acc: 0.8648\n",
      "Epoch 97/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3940 - acc: 0.8924 - val_loss: 0.4603 - val_acc: 0.8688\n",
      "Epoch 98/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3929 - acc: 0.8918 - val_loss: 0.4592 - val_acc: 0.8688\n",
      "Epoch 99/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3925 - acc: 0.8944 - val_loss: 0.4579 - val_acc: 0.8688\n",
      "Epoch 100/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3911 - acc: 0.8944 - val_loss: 0.4577 - val_acc: 0.8668\n",
      "Epoch 101/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3893 - acc: 0.8924 - val_loss: 0.4561 - val_acc: 0.8668\n",
      "Epoch 102/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3887 - acc: 0.8951 - val_loss: 0.4568 - val_acc: 0.8648\n",
      "Epoch 103/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3883 - acc: 0.8931 - val_loss: 0.4540 - val_acc: 0.8668\n",
      "Epoch 104/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3868 - acc: 0.8911 - val_loss: 0.4528 - val_acc: 0.8668\n",
      "Epoch 105/3000\n",
      "1506/1506 [==============================] - 0s 53us/step - loss: 0.3849 - acc: 0.8951 - val_loss: 0.4529 - val_acc: 0.8688\n",
      "Epoch 106/3000\n",
      "1506/1506 [==============================] - 0s 59us/step - loss: 0.3849 - acc: 0.8938 - val_loss: 0.4515 - val_acc: 0.8648\n",
      "Epoch 107/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3839 - acc: 0.8984 - val_loss: 0.4516 - val_acc: 0.8688\n",
      "Epoch 108/3000\n",
      "1506/1506 [==============================] - 0s 55us/step - loss: 0.3841 - acc: 0.8971 - val_loss: 0.4517 - val_acc: 0.8668\n",
      "Epoch 109/3000\n",
      "1506/1506 [==============================] - 0s 62us/step - loss: 0.3822 - acc: 0.8958 - val_loss: 0.4499 - val_acc: 0.8668\n",
      "Epoch 110/3000\n",
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.3820 - acc: 0.8931 - val_loss: 0.4495 - val_acc: 0.8668\n",
      "Epoch 111/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3803 - acc: 0.8944 - val_loss: 0.4484 - val_acc: 0.8668\n",
      "Epoch 112/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3798 - acc: 0.8958 - val_loss: 0.4464 - val_acc: 0.8668\n",
      "Epoch 113/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3785 - acc: 0.8977 - val_loss: 0.4463 - val_acc: 0.8688\n",
      "Epoch 114/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3783 - acc: 0.8918 - val_loss: 0.4456 - val_acc: 0.8668\n",
      "Epoch 115/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3774 - acc: 0.8977 - val_loss: 0.4449 - val_acc: 0.8688\n",
      "Epoch 116/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3760 - acc: 0.8964 - val_loss: 0.4449 - val_acc: 0.8688\n",
      "Epoch 117/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3763 - acc: 0.8938 - val_loss: 0.4438 - val_acc: 0.8688\n",
      "Epoch 118/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3750 - acc: 0.8951 - val_loss: 0.4432 - val_acc: 0.8688\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3741 - acc: 0.8938 - val_loss: 0.4425 - val_acc: 0.8668\n",
      "Epoch 120/3000\n",
      "1506/1506 [==============================] - 0s 143us/step - loss: 0.3733 - acc: 0.8951 - val_loss: 0.4419 - val_acc: 0.8668\n",
      "Epoch 121/3000\n",
      "1506/1506 [==============================] - 0s 154us/step - loss: 0.3726 - acc: 0.8964 - val_loss: 0.4412 - val_acc: 0.8688\n",
      "Epoch 122/3000\n",
      "1506/1506 [==============================] - 0s 130us/step - loss: 0.3716 - acc: 0.8931 - val_loss: 0.4406 - val_acc: 0.8688\n",
      "Epoch 123/3000\n",
      "1506/1506 [==============================] - 0s 146us/step - loss: 0.3709 - acc: 0.8984 - val_loss: 0.4397 - val_acc: 0.8708\n",
      "Epoch 124/3000\n",
      "1506/1506 [==============================] - 0s 133us/step - loss: 0.3699 - acc: 0.8958 - val_loss: 0.4393 - val_acc: 0.8688\n",
      "Epoch 125/3000\n",
      "1506/1506 [==============================] - 0s 135us/step - loss: 0.3696 - acc: 0.8951 - val_loss: 0.4389 - val_acc: 0.8708\n",
      "Epoch 126/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3690 - acc: 0.8958 - val_loss: 0.4378 - val_acc: 0.8688\n",
      "Epoch 127/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3671 - acc: 0.8971 - val_loss: 0.4377 - val_acc: 0.8688\n",
      "Epoch 128/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3666 - acc: 0.8938 - val_loss: 0.4357 - val_acc: 0.8708\n",
      "Epoch 129/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.3660 - acc: 0.8964 - val_loss: 0.4358 - val_acc: 0.8728\n",
      "Epoch 130/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3663 - acc: 0.8944 - val_loss: 0.4372 - val_acc: 0.8688\n",
      "Epoch 131/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3645 - acc: 0.8984 - val_loss: 0.4340 - val_acc: 0.8688\n",
      "Epoch 132/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3631 - acc: 0.8991 - val_loss: 0.4341 - val_acc: 0.8708\n",
      "Epoch 133/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3627 - acc: 0.8958 - val_loss: 0.4333 - val_acc: 0.8728\n",
      "Epoch 134/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3616 - acc: 0.8958 - val_loss: 0.4327 - val_acc: 0.8708\n",
      "Epoch 135/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3614 - acc: 0.8964 - val_loss: 0.4317 - val_acc: 0.8728\n",
      "Epoch 136/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3606 - acc: 0.8991 - val_loss: 0.4307 - val_acc: 0.8728\n",
      "Epoch 137/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3609 - acc: 0.8958 - val_loss: 0.4309 - val_acc: 0.8668\n",
      "Epoch 138/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3592 - acc: 0.8977 - val_loss: 0.4287 - val_acc: 0.8668\n",
      "Epoch 139/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3580 - acc: 0.8991 - val_loss: 0.4286 - val_acc: 0.8708\n",
      "Epoch 140/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3573 - acc: 0.8971 - val_loss: 0.4273 - val_acc: 0.8688\n",
      "Epoch 141/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3563 - acc: 0.8971 - val_loss: 0.4270 - val_acc: 0.8708\n",
      "Epoch 142/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3558 - acc: 0.9004 - val_loss: 0.4267 - val_acc: 0.8668\n",
      "Epoch 143/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3554 - acc: 0.8984 - val_loss: 0.4258 - val_acc: 0.8688\n",
      "Epoch 144/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3545 - acc: 0.8991 - val_loss: 0.4251 - val_acc: 0.8708\n",
      "Epoch 145/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3530 - acc: 0.9011 - val_loss: 0.4244 - val_acc: 0.8688\n",
      "Epoch 146/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3522 - acc: 0.8991 - val_loss: 0.4228 - val_acc: 0.8668\n",
      "Epoch 147/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3513 - acc: 0.8977 - val_loss: 0.4225 - val_acc: 0.8688\n",
      "Epoch 148/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3508 - acc: 0.8984 - val_loss: 0.4214 - val_acc: 0.8668\n",
      "Epoch 149/3000\n",
      "1506/1506 [==============================] - 0s 122us/step - loss: 0.3494 - acc: 0.8991 - val_loss: 0.4202 - val_acc: 0.8688\n",
      "Epoch 150/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3484 - acc: 0.8984 - val_loss: 0.4194 - val_acc: 0.8748\n",
      "Epoch 151/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3476 - acc: 0.9004 - val_loss: 0.4193 - val_acc: 0.8728\n",
      "Epoch 152/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3465 - acc: 0.8984 - val_loss: 0.4164 - val_acc: 0.8748\n",
      "Epoch 153/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3463 - acc: 0.8991 - val_loss: 0.4170 - val_acc: 0.8688\n",
      "Epoch 154/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3466 - acc: 0.9004 - val_loss: 0.4172 - val_acc: 0.8708\n",
      "Epoch 155/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.3446 - acc: 0.8991 - val_loss: 0.4166 - val_acc: 0.8748\n",
      "Epoch 156/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3445 - acc: 0.9011 - val_loss: 0.4157 - val_acc: 0.8748\n",
      "Epoch 157/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3428 - acc: 0.9004 - val_loss: 0.4154 - val_acc: 0.8668\n",
      "Epoch 158/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3427 - acc: 0.8997 - val_loss: 0.4153 - val_acc: 0.8708\n",
      "Epoch 159/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3413 - acc: 0.9011 - val_loss: 0.4140 - val_acc: 0.8708\n",
      "Epoch 160/3000\n",
      "1506/1506 [==============================] - 0s 112us/step - loss: 0.3408 - acc: 0.8991 - val_loss: 0.4146 - val_acc: 0.8708\n",
      "Epoch 161/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3409 - acc: 0.8997 - val_loss: 0.4142 - val_acc: 0.8708\n",
      "Epoch 162/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3405 - acc: 0.8991 - val_loss: 0.4126 - val_acc: 0.8708\n",
      "Epoch 163/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3402 - acc: 0.8984 - val_loss: 0.4127 - val_acc: 0.8728\n",
      "Epoch 164/3000\n",
      "1506/1506 [==============================] - 0s 101us/step - loss: 0.3396 - acc: 0.8977 - val_loss: 0.4127 - val_acc: 0.8688\n",
      "Epoch 165/3000\n",
      "1506/1506 [==============================] - 0s 109us/step - loss: 0.3389 - acc: 0.8991 - val_loss: 0.4127 - val_acc: 0.8728\n",
      "Epoch 166/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3387 - acc: 0.9004 - val_loss: 0.4099 - val_acc: 0.8708\n",
      "Epoch 167/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3381 - acc: 0.9004 - val_loss: 0.4110 - val_acc: 0.8688\n",
      "Epoch 168/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3376 - acc: 0.9004 - val_loss: 0.4101 - val_acc: 0.8708\n",
      "Epoch 169/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3366 - acc: 0.8997 - val_loss: 0.4092 - val_acc: 0.8708\n",
      "Epoch 170/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3369 - acc: 0.9004 - val_loss: 0.4077 - val_acc: 0.8728\n",
      "Epoch 171/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3360 - acc: 0.8991 - val_loss: 0.4088 - val_acc: 0.8708\n",
      "Epoch 172/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3353 - acc: 0.8984 - val_loss: 0.4076 - val_acc: 0.8688\n",
      "Epoch 173/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3347 - acc: 0.8991 - val_loss: 0.4078 - val_acc: 0.8708\n",
      "Epoch 174/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3337 - acc: 0.9011 - val_loss: 0.4074 - val_acc: 0.8728\n",
      "Epoch 175/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3330 - acc: 0.8984 - val_loss: 0.4062 - val_acc: 0.8728\n",
      "Epoch 176/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3331 - acc: 0.9017 - val_loss: 0.4059 - val_acc: 0.8767\n",
      "Epoch 177/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3326 - acc: 0.8997 - val_loss: 0.4057 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3320 - acc: 0.9004 - val_loss: 0.4054 - val_acc: 0.8728\n",
      "Epoch 179/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3314 - acc: 0.8991 - val_loss: 0.4050 - val_acc: 0.8688\n",
      "Epoch 180/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3309 - acc: 0.8977 - val_loss: 0.4044 - val_acc: 0.8728\n",
      "Epoch 181/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3305 - acc: 0.9011 - val_loss: 0.4054 - val_acc: 0.8688\n",
      "Epoch 182/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3300 - acc: 0.8997 - val_loss: 0.4036 - val_acc: 0.8708\n",
      "Epoch 183/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3304 - acc: 0.8984 - val_loss: 0.4030 - val_acc: 0.8688\n",
      "Epoch 184/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3306 - acc: 0.8991 - val_loss: 0.4018 - val_acc: 0.8728\n",
      "Epoch 185/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3296 - acc: 0.8997 - val_loss: 0.4021 - val_acc: 0.8708\n",
      "Epoch 186/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3293 - acc: 0.8991 - val_loss: 0.4012 - val_acc: 0.8728\n",
      "Epoch 187/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3281 - acc: 0.8984 - val_loss: 0.3999 - val_acc: 0.8708\n",
      "Epoch 188/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3273 - acc: 0.9011 - val_loss: 0.3999 - val_acc: 0.8728\n",
      "Epoch 189/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3276 - acc: 0.9011 - val_loss: 0.4008 - val_acc: 0.8728\n",
      "Epoch 190/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3272 - acc: 0.8977 - val_loss: 0.3993 - val_acc: 0.8708\n",
      "Epoch 191/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3260 - acc: 0.8977 - val_loss: 0.3985 - val_acc: 0.8688\n",
      "Epoch 192/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3271 - acc: 0.8977 - val_loss: 0.3980 - val_acc: 0.8688\n",
      "Epoch 193/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3261 - acc: 0.9011 - val_loss: 0.3970 - val_acc: 0.8708\n",
      "Epoch 194/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3245 - acc: 0.8991 - val_loss: 0.3969 - val_acc: 0.8688\n",
      "Epoch 195/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3245 - acc: 0.8984 - val_loss: 0.3963 - val_acc: 0.8708\n",
      "Epoch 196/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3241 - acc: 0.9017 - val_loss: 0.3949 - val_acc: 0.8767\n",
      "Epoch 197/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3243 - acc: 0.9011 - val_loss: 0.3951 - val_acc: 0.8748\n",
      "Epoch 198/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.3230 - acc: 0.9024 - val_loss: 0.3945 - val_acc: 0.8767\n",
      "Epoch 199/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3226 - acc: 0.9017 - val_loss: 0.3933 - val_acc: 0.8748\n",
      "Epoch 200/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3213 - acc: 0.9004 - val_loss: 0.3936 - val_acc: 0.8708\n",
      "Epoch 201/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3219 - acc: 0.8991 - val_loss: 0.3924 - val_acc: 0.8708\n",
      "Epoch 202/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3207 - acc: 0.9024 - val_loss: 0.3922 - val_acc: 0.8708\n",
      "Epoch 203/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3207 - acc: 0.8997 - val_loss: 0.3920 - val_acc: 0.8728\n",
      "Epoch 204/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3202 - acc: 0.9017 - val_loss: 0.3919 - val_acc: 0.8728\n",
      "Epoch 205/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3194 - acc: 0.9004 - val_loss: 0.3906 - val_acc: 0.8728\n",
      "Epoch 206/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3192 - acc: 0.9017 - val_loss: 0.3931 - val_acc: 0.8728\n",
      "Epoch 207/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3193 - acc: 0.9050 - val_loss: 0.3922 - val_acc: 0.8708\n",
      "Epoch 208/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3181 - acc: 0.9024 - val_loss: 0.3908 - val_acc: 0.8748\n",
      "Epoch 209/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3184 - acc: 0.9024 - val_loss: 0.3898 - val_acc: 0.8728\n",
      "Epoch 210/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3178 - acc: 0.9004 - val_loss: 0.3901 - val_acc: 0.8708\n",
      "Epoch 211/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3171 - acc: 0.9004 - val_loss: 0.3887 - val_acc: 0.8748\n",
      "Epoch 212/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3170 - acc: 0.9004 - val_loss: 0.3888 - val_acc: 0.8748\n",
      "Epoch 213/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3169 - acc: 0.9024 - val_loss: 0.3886 - val_acc: 0.8748\n",
      "Epoch 214/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3159 - acc: 0.9024 - val_loss: 0.3889 - val_acc: 0.8748\n",
      "Epoch 215/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3166 - acc: 0.8991 - val_loss: 0.3884 - val_acc: 0.8748\n",
      "Epoch 216/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3161 - acc: 0.9011 - val_loss: 0.3878 - val_acc: 0.8748\n",
      "Epoch 217/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3154 - acc: 0.9017 - val_loss: 0.3868 - val_acc: 0.8728\n",
      "Epoch 218/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3155 - acc: 0.9037 - val_loss: 0.3866 - val_acc: 0.8748\n",
      "Epoch 219/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3162 - acc: 0.9011 - val_loss: 0.3870 - val_acc: 0.8748\n",
      "Epoch 220/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3150 - acc: 0.9050 - val_loss: 0.3867 - val_acc: 0.8748\n",
      "Epoch 221/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3146 - acc: 0.9011 - val_loss: 0.3863 - val_acc: 0.8748\n",
      "Epoch 222/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3135 - acc: 0.9031 - val_loss: 0.3854 - val_acc: 0.8748\n",
      "Epoch 223/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3139 - acc: 0.9011 - val_loss: 0.3848 - val_acc: 0.8767\n",
      "Epoch 224/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3134 - acc: 0.9037 - val_loss: 0.3828 - val_acc: 0.8728\n",
      "Epoch 225/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3131 - acc: 0.9037 - val_loss: 0.3842 - val_acc: 0.8767\n",
      "Epoch 226/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3135 - acc: 0.9031 - val_loss: 0.3856 - val_acc: 0.8767\n",
      "Epoch 227/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3124 - acc: 0.9017 - val_loss: 0.3832 - val_acc: 0.8748\n",
      "Epoch 228/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3127 - acc: 0.9064 - val_loss: 0.3821 - val_acc: 0.8767\n",
      "Epoch 229/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3115 - acc: 0.9024 - val_loss: 0.3831 - val_acc: 0.8767\n",
      "Epoch 230/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3113 - acc: 0.9050 - val_loss: 0.3831 - val_acc: 0.8787\n",
      "Epoch 231/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3111 - acc: 0.9024 - val_loss: 0.3823 - val_acc: 0.8767\n",
      "Epoch 232/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3103 - acc: 0.9064 - val_loss: 0.3821 - val_acc: 0.8807\n",
      "Epoch 233/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3104 - acc: 0.9037 - val_loss: 0.3825 - val_acc: 0.8767\n",
      "Epoch 234/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3091 - acc: 0.9050 - val_loss: 0.3797 - val_acc: 0.8787\n",
      "Epoch 235/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3083 - acc: 0.9044 - val_loss: 0.3806 - val_acc: 0.8767\n",
      "Epoch 236/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3082 - acc: 0.9064 - val_loss: 0.3802 - val_acc: 0.8787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3090 - acc: 0.9070 - val_loss: 0.3802 - val_acc: 0.8827\n",
      "Epoch 238/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3080 - acc: 0.9050 - val_loss: 0.3808 - val_acc: 0.8767\n",
      "Epoch 239/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3071 - acc: 0.9044 - val_loss: 0.3789 - val_acc: 0.8767\n",
      "Epoch 240/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3077 - acc: 0.9044 - val_loss: 0.3791 - val_acc: 0.8787\n",
      "Epoch 241/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3074 - acc: 0.9070 - val_loss: 0.3795 - val_acc: 0.8767\n",
      "Epoch 242/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3079 - acc: 0.9064 - val_loss: 0.3784 - val_acc: 0.8807\n",
      "Epoch 243/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.3062 - acc: 0.9070 - val_loss: 0.3765 - val_acc: 0.8787\n",
      "Epoch 244/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3057 - acc: 0.9064 - val_loss: 0.3775 - val_acc: 0.8767\n",
      "Epoch 245/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3054 - acc: 0.9031 - val_loss: 0.3773 - val_acc: 0.8787\n",
      "Epoch 246/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3059 - acc: 0.9044 - val_loss: 0.3758 - val_acc: 0.8807\n",
      "Epoch 247/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3049 - acc: 0.9064 - val_loss: 0.3746 - val_acc: 0.8787\n",
      "Epoch 248/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3050 - acc: 0.9044 - val_loss: 0.3755 - val_acc: 0.8807\n",
      "Epoch 249/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3046 - acc: 0.9064 - val_loss: 0.3744 - val_acc: 0.8787\n",
      "Epoch 250/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3047 - acc: 0.9050 - val_loss: 0.3757 - val_acc: 0.8787\n",
      "Epoch 251/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3041 - acc: 0.9090 - val_loss: 0.3755 - val_acc: 0.8787\n",
      "Epoch 252/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3041 - acc: 0.9057 - val_loss: 0.3748 - val_acc: 0.8787\n",
      "Epoch 253/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.3037 - acc: 0.9084 - val_loss: 0.3743 - val_acc: 0.8767\n",
      "Epoch 254/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3027 - acc: 0.9077 - val_loss: 0.3738 - val_acc: 0.8807\n",
      "Epoch 255/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3030 - acc: 0.9070 - val_loss: 0.3731 - val_acc: 0.8827\n",
      "Epoch 256/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3043 - acc: 0.9090 - val_loss: 0.3707 - val_acc: 0.8767\n",
      "Epoch 257/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3016 - acc: 0.9050 - val_loss: 0.3731 - val_acc: 0.8767\n",
      "Epoch 258/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3016 - acc: 0.9050 - val_loss: 0.3724 - val_acc: 0.8787\n",
      "Epoch 259/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3026 - acc: 0.9037 - val_loss: 0.3721 - val_acc: 0.8807\n",
      "Epoch 260/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3010 - acc: 0.9064 - val_loss: 0.3709 - val_acc: 0.8767\n",
      "Epoch 261/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3006 - acc: 0.9064 - val_loss: 0.3727 - val_acc: 0.8807\n",
      "Epoch 262/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3024 - acc: 0.9044 - val_loss: 0.3704 - val_acc: 0.8767\n",
      "Epoch 263/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3015 - acc: 0.9070 - val_loss: 0.3711 - val_acc: 0.8807\n",
      "Epoch 264/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3006 - acc: 0.9077 - val_loss: 0.3703 - val_acc: 0.8787\n",
      "Epoch 265/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2997 - acc: 0.9050 - val_loss: 0.3704 - val_acc: 0.8807\n",
      "Epoch 266/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.3004 - acc: 0.9110 - val_loss: 0.3702 - val_acc: 0.8827\n",
      "Epoch 267/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2994 - acc: 0.9090 - val_loss: 0.3695 - val_acc: 0.8827\n",
      "Epoch 268/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2988 - acc: 0.9084 - val_loss: 0.3682 - val_acc: 0.8807\n",
      "Epoch 269/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2982 - acc: 0.9057 - val_loss: 0.3690 - val_acc: 0.8807\n",
      "Epoch 270/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2985 - acc: 0.9064 - val_loss: 0.3676 - val_acc: 0.8807\n",
      "Epoch 271/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2980 - acc: 0.9090 - val_loss: 0.3684 - val_acc: 0.8787\n",
      "Epoch 272/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2976 - acc: 0.9057 - val_loss: 0.3682 - val_acc: 0.8787\n",
      "Epoch 273/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2986 - acc: 0.9084 - val_loss: 0.3691 - val_acc: 0.8767\n",
      "Epoch 274/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2983 - acc: 0.9057 - val_loss: 0.3695 - val_acc: 0.8787\n",
      "Epoch 275/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2983 - acc: 0.9090 - val_loss: 0.3678 - val_acc: 0.8807\n",
      "Epoch 276/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2969 - acc: 0.9077 - val_loss: 0.3677 - val_acc: 0.8787\n",
      "Epoch 277/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2967 - acc: 0.9050 - val_loss: 0.3689 - val_acc: 0.8787\n",
      "Epoch 278/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2967 - acc: 0.9090 - val_loss: 0.3666 - val_acc: 0.8787\n",
      "Epoch 279/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2971 - acc: 0.9077 - val_loss: 0.3692 - val_acc: 0.8807\n",
      "Epoch 280/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2989 - acc: 0.9084 - val_loss: 0.3686 - val_acc: 0.8787\n",
      "Epoch 281/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2964 - acc: 0.9090 - val_loss: 0.3702 - val_acc: 0.8807\n",
      "Epoch 282/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2962 - acc: 0.9070 - val_loss: 0.3686 - val_acc: 0.8807\n",
      "Epoch 283/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2961 - acc: 0.9097 - val_loss: 0.3659 - val_acc: 0.8787\n",
      "Epoch 284/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2963 - acc: 0.9090 - val_loss: 0.3693 - val_acc: 0.8787\n",
      "Epoch 285/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2960 - acc: 0.9090 - val_loss: 0.3672 - val_acc: 0.8787\n",
      "Epoch 286/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2962 - acc: 0.9077 - val_loss: 0.3669 - val_acc: 0.8827\n",
      "Epoch 287/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2962 - acc: 0.9084 - val_loss: 0.3668 - val_acc: 0.8827\n",
      "Epoch 288/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2946 - acc: 0.9090 - val_loss: 0.3652 - val_acc: 0.8807\n",
      "Epoch 289/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2947 - acc: 0.9057 - val_loss: 0.3662 - val_acc: 0.8827\n",
      "Epoch 290/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2938 - acc: 0.9077 - val_loss: 0.3643 - val_acc: 0.8827\n",
      "Epoch 291/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2948 - acc: 0.9084 - val_loss: 0.3631 - val_acc: 0.8847\n",
      "Epoch 292/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2945 - acc: 0.9064 - val_loss: 0.3654 - val_acc: 0.8807\n",
      "Epoch 293/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2945 - acc: 0.9077 - val_loss: 0.3639 - val_acc: 0.8807\n",
      "Epoch 294/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2936 - acc: 0.9097 - val_loss: 0.3657 - val_acc: 0.8807\n",
      "Epoch 295/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2930 - acc: 0.9097 - val_loss: 0.3634 - val_acc: 0.8807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2940 - acc: 0.9097 - val_loss: 0.3631 - val_acc: 0.8827\n",
      "Epoch 297/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2930 - acc: 0.9090 - val_loss: 0.3635 - val_acc: 0.8807\n",
      "Epoch 298/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2941 - acc: 0.9097 - val_loss: 0.3624 - val_acc: 0.8807\n",
      "Epoch 299/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2928 - acc: 0.9097 - val_loss: 0.3637 - val_acc: 0.8827\n",
      "Epoch 300/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2921 - acc: 0.9104 - val_loss: 0.3631 - val_acc: 0.8827\n",
      "Epoch 301/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2919 - acc: 0.9104 - val_loss: 0.3632 - val_acc: 0.8807\n",
      "Epoch 302/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2917 - acc: 0.9077 - val_loss: 0.3644 - val_acc: 0.8847\n",
      "Epoch 303/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2923 - acc: 0.9084 - val_loss: 0.3614 - val_acc: 0.8807\n",
      "Epoch 304/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.2914 - acc: 0.9070 - val_loss: 0.3612 - val_acc: 0.8827\n",
      "Epoch 305/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2926 - acc: 0.9084 - val_loss: 0.3603 - val_acc: 0.8827\n",
      "Epoch 306/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2917 - acc: 0.9077 - val_loss: 0.3603 - val_acc: 0.8867\n",
      "Epoch 307/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2904 - acc: 0.9077 - val_loss: 0.3593 - val_acc: 0.8847\n",
      "Epoch 308/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2903 - acc: 0.9090 - val_loss: 0.3608 - val_acc: 0.8807\n",
      "Epoch 309/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2895 - acc: 0.9077 - val_loss: 0.3605 - val_acc: 0.8827\n",
      "Epoch 310/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2895 - acc: 0.9090 - val_loss: 0.3598 - val_acc: 0.8807\n",
      "Epoch 311/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2899 - acc: 0.9077 - val_loss: 0.3595 - val_acc: 0.8867\n",
      "Epoch 312/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2904 - acc: 0.9064 - val_loss: 0.3605 - val_acc: 0.8827\n",
      "Epoch 313/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2903 - acc: 0.9084 - val_loss: 0.3593 - val_acc: 0.8847\n",
      "Epoch 314/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2886 - acc: 0.9097 - val_loss: 0.3589 - val_acc: 0.8867\n",
      "Epoch 315/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2902 - acc: 0.9084 - val_loss: 0.3605 - val_acc: 0.8847\n",
      "Epoch 316/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2888 - acc: 0.9090 - val_loss: 0.3609 - val_acc: 0.8847\n",
      "Epoch 317/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2888 - acc: 0.9090 - val_loss: 0.3592 - val_acc: 0.8787\n",
      "Epoch 318/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2880 - acc: 0.9097 - val_loss: 0.3605 - val_acc: 0.8867\n",
      "Epoch 319/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2900 - acc: 0.9104 - val_loss: 0.3603 - val_acc: 0.8867\n",
      "Epoch 320/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2878 - acc: 0.9090 - val_loss: 0.3601 - val_acc: 0.8867\n",
      "Epoch 321/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2875 - acc: 0.9104 - val_loss: 0.3594 - val_acc: 0.8827\n",
      "Epoch 322/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2877 - acc: 0.9077 - val_loss: 0.3586 - val_acc: 0.8827\n",
      "Epoch 323/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2870 - acc: 0.9097 - val_loss: 0.3599 - val_acc: 0.8847\n",
      "Epoch 324/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2869 - acc: 0.9104 - val_loss: 0.3595 - val_acc: 0.8847\n",
      "Epoch 325/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2869 - acc: 0.9090 - val_loss: 0.3581 - val_acc: 0.8827\n",
      "Epoch 326/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2868 - acc: 0.9097 - val_loss: 0.3577 - val_acc: 0.8827\n",
      "Epoch 327/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2871 - acc: 0.9084 - val_loss: 0.3589 - val_acc: 0.8847\n",
      "Epoch 328/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2863 - acc: 0.9090 - val_loss: 0.3588 - val_acc: 0.8847\n",
      "Epoch 329/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2861 - acc: 0.9070 - val_loss: 0.3583 - val_acc: 0.8847\n",
      "Epoch 330/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2864 - acc: 0.9117 - val_loss: 0.3587 - val_acc: 0.8867\n",
      "Epoch 331/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2870 - acc: 0.9064 - val_loss: 0.3596 - val_acc: 0.8887\n",
      "Epoch 332/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2860 - acc: 0.9090 - val_loss: 0.3578 - val_acc: 0.8827\n",
      "Epoch 333/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2865 - acc: 0.9110 - val_loss: 0.3589 - val_acc: 0.8867\n",
      "Epoch 334/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2863 - acc: 0.9097 - val_loss: 0.3565 - val_acc: 0.8867\n",
      "Epoch 335/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2854 - acc: 0.9090 - val_loss: 0.3565 - val_acc: 0.8867\n",
      "Epoch 336/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2848 - acc: 0.9077 - val_loss: 0.3584 - val_acc: 0.8867\n",
      "Epoch 337/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2854 - acc: 0.9097 - val_loss: 0.3562 - val_acc: 0.8867\n",
      "Epoch 338/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2849 - acc: 0.9104 - val_loss: 0.3561 - val_acc: 0.8807\n",
      "Epoch 339/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2852 - acc: 0.9077 - val_loss: 0.3560 - val_acc: 0.8847\n",
      "Epoch 340/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2848 - acc: 0.9110 - val_loss: 0.3547 - val_acc: 0.8887\n",
      "Epoch 341/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2854 - acc: 0.9057 - val_loss: 0.3573 - val_acc: 0.8867\n",
      "Epoch 342/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2841 - acc: 0.9104 - val_loss: 0.3536 - val_acc: 0.8847\n",
      "Epoch 343/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2837 - acc: 0.9077 - val_loss: 0.3554 - val_acc: 0.8847\n",
      "Epoch 344/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2837 - acc: 0.9090 - val_loss: 0.3555 - val_acc: 0.8807\n",
      "Epoch 345/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2847 - acc: 0.9070 - val_loss: 0.3564 - val_acc: 0.8867\n",
      "Epoch 346/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2840 - acc: 0.9104 - val_loss: 0.3539 - val_acc: 0.8907\n",
      "Epoch 347/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2833 - acc: 0.9084 - val_loss: 0.3529 - val_acc: 0.8847\n",
      "Epoch 348/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2826 - acc: 0.9117 - val_loss: 0.3544 - val_acc: 0.8867\n",
      "Epoch 349/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2829 - acc: 0.9077 - val_loss: 0.3550 - val_acc: 0.8847\n",
      "Epoch 350/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2835 - acc: 0.9104 - val_loss: 0.3553 - val_acc: 0.8827\n",
      "Epoch 351/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2822 - acc: 0.9097 - val_loss: 0.3537 - val_acc: 0.8827\n",
      "Epoch 352/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2839 - acc: 0.9090 - val_loss: 0.3525 - val_acc: 0.8807\n",
      "Epoch 353/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2835 - acc: 0.9117 - val_loss: 0.3572 - val_acc: 0.8867\n",
      "Epoch 354/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2820 - acc: 0.9084 - val_loss: 0.3557 - val_acc: 0.8827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2821 - acc: 0.9090 - val_loss: 0.3557 - val_acc: 0.8847\n",
      "Epoch 356/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2814 - acc: 0.9097 - val_loss: 0.3534 - val_acc: 0.8827\n",
      "Epoch 357/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2813 - acc: 0.9097 - val_loss: 0.3530 - val_acc: 0.8847\n",
      "Epoch 358/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2821 - acc: 0.9090 - val_loss: 0.3523 - val_acc: 0.8867\n",
      "Epoch 359/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2819 - acc: 0.9090 - val_loss: 0.3518 - val_acc: 0.8887\n",
      "Epoch 360/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2819 - acc: 0.9084 - val_loss: 0.3537 - val_acc: 0.8867\n",
      "Epoch 361/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2808 - acc: 0.9104 - val_loss: 0.3526 - val_acc: 0.8847\n",
      "Epoch 362/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2811 - acc: 0.9117 - val_loss: 0.3538 - val_acc: 0.8887\n",
      "Epoch 363/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2805 - acc: 0.9084 - val_loss: 0.3513 - val_acc: 0.8827\n",
      "Epoch 364/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2795 - acc: 0.9110 - val_loss: 0.3539 - val_acc: 0.8847\n",
      "Epoch 365/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2822 - acc: 0.9117 - val_loss: 0.3551 - val_acc: 0.8847\n",
      "Epoch 366/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2802 - acc: 0.9090 - val_loss: 0.3494 - val_acc: 0.8907\n",
      "Epoch 367/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2805 - acc: 0.9090 - val_loss: 0.3528 - val_acc: 0.8847\n",
      "Epoch 368/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2794 - acc: 0.9070 - val_loss: 0.3518 - val_acc: 0.8867\n",
      "Epoch 369/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2789 - acc: 0.9110 - val_loss: 0.3510 - val_acc: 0.8887\n",
      "Epoch 370/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2785 - acc: 0.9117 - val_loss: 0.3497 - val_acc: 0.8827\n",
      "Epoch 371/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2785 - acc: 0.9104 - val_loss: 0.3508 - val_acc: 0.8867\n",
      "Epoch 372/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2784 - acc: 0.9117 - val_loss: 0.3504 - val_acc: 0.8867\n",
      "Epoch 373/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2792 - acc: 0.9104 - val_loss: 0.3504 - val_acc: 0.8867\n",
      "Epoch 374/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2772 - acc: 0.9104 - val_loss: 0.3479 - val_acc: 0.8867\n",
      "Epoch 375/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2786 - acc: 0.9097 - val_loss: 0.3502 - val_acc: 0.8847\n",
      "Epoch 376/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2789 - acc: 0.9077 - val_loss: 0.3481 - val_acc: 0.8847\n",
      "Epoch 377/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2779 - acc: 0.9090 - val_loss: 0.3476 - val_acc: 0.8867\n",
      "Epoch 378/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2765 - acc: 0.9084 - val_loss: 0.3491 - val_acc: 0.8887\n",
      "Epoch 379/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2770 - acc: 0.9097 - val_loss: 0.3468 - val_acc: 0.8847\n",
      "Epoch 380/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2775 - acc: 0.9104 - val_loss: 0.3465 - val_acc: 0.8867\n",
      "Epoch 381/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2777 - acc: 0.9084 - val_loss: 0.3453 - val_acc: 0.8887\n",
      "Epoch 382/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2771 - acc: 0.9104 - val_loss: 0.3482 - val_acc: 0.8887\n",
      "Epoch 383/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2767 - acc: 0.9097 - val_loss: 0.3483 - val_acc: 0.8867\n",
      "Epoch 384/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2776 - acc: 0.9117 - val_loss: 0.3460 - val_acc: 0.8907\n",
      "Epoch 385/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2763 - acc: 0.9124 - val_loss: 0.3447 - val_acc: 0.8847\n",
      "Epoch 386/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2753 - acc: 0.9137 - val_loss: 0.3483 - val_acc: 0.8847\n",
      "Epoch 387/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2755 - acc: 0.9110 - val_loss: 0.3456 - val_acc: 0.8867\n",
      "Epoch 388/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2764 - acc: 0.9104 - val_loss: 0.3447 - val_acc: 0.8847\n",
      "Epoch 389/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2749 - acc: 0.9097 - val_loss: 0.3454 - val_acc: 0.8867\n",
      "Epoch 390/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2748 - acc: 0.9104 - val_loss: 0.3469 - val_acc: 0.8887\n",
      "Epoch 391/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2758 - acc: 0.9110 - val_loss: 0.3459 - val_acc: 0.8887\n",
      "Epoch 392/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2752 - acc: 0.9097 - val_loss: 0.3448 - val_acc: 0.8847\n",
      "Epoch 393/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2752 - acc: 0.9110 - val_loss: 0.3453 - val_acc: 0.8887\n",
      "Epoch 394/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2769 - acc: 0.9104 - val_loss: 0.3452 - val_acc: 0.8887\n",
      "Epoch 395/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2739 - acc: 0.9097 - val_loss: 0.3434 - val_acc: 0.8847\n",
      "Epoch 396/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2770 - acc: 0.9090 - val_loss: 0.3456 - val_acc: 0.8867\n",
      "Epoch 397/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2745 - acc: 0.9097 - val_loss: 0.3417 - val_acc: 0.8867\n",
      "Epoch 398/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2742 - acc: 0.9124 - val_loss: 0.3446 - val_acc: 0.8867\n",
      "Epoch 399/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2741 - acc: 0.9117 - val_loss: 0.3440 - val_acc: 0.8867\n",
      "Epoch 400/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2730 - acc: 0.9110 - val_loss: 0.3435 - val_acc: 0.8847\n",
      "Epoch 401/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2722 - acc: 0.9117 - val_loss: 0.3441 - val_acc: 0.8847\n",
      "Epoch 402/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2726 - acc: 0.9124 - val_loss: 0.3409 - val_acc: 0.8867\n",
      "Epoch 403/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2736 - acc: 0.9110 - val_loss: 0.3424 - val_acc: 0.8867\n",
      "Epoch 404/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2743 - acc: 0.9110 - val_loss: 0.3450 - val_acc: 0.8887\n",
      "Epoch 405/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2730 - acc: 0.9104 - val_loss: 0.3435 - val_acc: 0.8847\n",
      "Epoch 406/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2717 - acc: 0.9104 - val_loss: 0.3469 - val_acc: 0.8847\n",
      "Epoch 407/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2739 - acc: 0.9137 - val_loss: 0.3409 - val_acc: 0.8867\n",
      "Epoch 408/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2723 - acc: 0.9104 - val_loss: 0.3488 - val_acc: 0.8827\n",
      "Epoch 409/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2720 - acc: 0.9130 - val_loss: 0.3420 - val_acc: 0.8887\n",
      "Epoch 410/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2715 - acc: 0.9104 - val_loss: 0.3421 - val_acc: 0.8847\n",
      "Epoch 411/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2711 - acc: 0.9130 - val_loss: 0.3417 - val_acc: 0.8887\n",
      "Epoch 412/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2716 - acc: 0.9104 - val_loss: 0.3447 - val_acc: 0.8847\n",
      "Epoch 413/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2727 - acc: 0.9143 - val_loss: 0.3384 - val_acc: 0.8887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2720 - acc: 0.9124 - val_loss: 0.3428 - val_acc: 0.8867\n",
      "Epoch 415/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2712 - acc: 0.9124 - val_loss: 0.3384 - val_acc: 0.8887\n",
      "Epoch 416/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2726 - acc: 0.9104 - val_loss: 0.3409 - val_acc: 0.8867\n",
      "Epoch 417/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2704 - acc: 0.9084 - val_loss: 0.3376 - val_acc: 0.8887\n",
      "Epoch 418/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2709 - acc: 0.9150 - val_loss: 0.3417 - val_acc: 0.8847\n",
      "Epoch 419/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.2703 - acc: 0.9110 - val_loss: 0.3409 - val_acc: 0.8847\n",
      "Epoch 420/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.2694 - acc: 0.9157 - val_loss: 0.3404 - val_acc: 0.8827\n",
      "Epoch 421/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2710 - acc: 0.9117 - val_loss: 0.3389 - val_acc: 0.8847\n",
      "Epoch 422/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2691 - acc: 0.9104 - val_loss: 0.3435 - val_acc: 0.8847\n",
      "Epoch 423/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2701 - acc: 0.9124 - val_loss: 0.3431 - val_acc: 0.8827\n",
      "Epoch 424/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2698 - acc: 0.9110 - val_loss: 0.3399 - val_acc: 0.8847\n",
      "Epoch 425/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2687 - acc: 0.9163 - val_loss: 0.3386 - val_acc: 0.8867\n",
      "Epoch 426/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2693 - acc: 0.9097 - val_loss: 0.3367 - val_acc: 0.8907\n",
      "Epoch 427/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2697 - acc: 0.9137 - val_loss: 0.3390 - val_acc: 0.8867\n",
      "Epoch 428/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2681 - acc: 0.9124 - val_loss: 0.3365 - val_acc: 0.8887\n",
      "Epoch 429/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2669 - acc: 0.9124 - val_loss: 0.3392 - val_acc: 0.8867\n",
      "Epoch 430/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2680 - acc: 0.9163 - val_loss: 0.3357 - val_acc: 0.8867\n",
      "Epoch 431/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2678 - acc: 0.9117 - val_loss: 0.3365 - val_acc: 0.8867\n",
      "Epoch 432/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2673 - acc: 0.9097 - val_loss: 0.3343 - val_acc: 0.8887\n",
      "Epoch 433/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2674 - acc: 0.9124 - val_loss: 0.3418 - val_acc: 0.8867\n",
      "Epoch 434/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2675 - acc: 0.9163 - val_loss: 0.3364 - val_acc: 0.8847\n",
      "Epoch 435/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2665 - acc: 0.9117 - val_loss: 0.3369 - val_acc: 0.8887\n",
      "Epoch 436/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2674 - acc: 0.9117 - val_loss: 0.3386 - val_acc: 0.8887\n",
      "Epoch 437/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2670 - acc: 0.9150 - val_loss: 0.3411 - val_acc: 0.8847\n",
      "Epoch 438/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2675 - acc: 0.9110 - val_loss: 0.3347 - val_acc: 0.8907\n",
      "Epoch 439/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2670 - acc: 0.9137 - val_loss: 0.3346 - val_acc: 0.8907\n",
      "Epoch 440/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2669 - acc: 0.9143 - val_loss: 0.3402 - val_acc: 0.8867\n",
      "Epoch 441/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2659 - acc: 0.9110 - val_loss: 0.3326 - val_acc: 0.8887\n",
      "Epoch 442/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2662 - acc: 0.9157 - val_loss: 0.3373 - val_acc: 0.8807\n",
      "Epoch 443/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2650 - acc: 0.9124 - val_loss: 0.3327 - val_acc: 0.8847\n",
      "Epoch 444/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2651 - acc: 0.9130 - val_loss: 0.3372 - val_acc: 0.8827\n",
      "Epoch 445/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2647 - acc: 0.9117 - val_loss: 0.3345 - val_acc: 0.8867\n",
      "Epoch 446/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2640 - acc: 0.9157 - val_loss: 0.3328 - val_acc: 0.8867\n",
      "Epoch 447/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2634 - acc: 0.9137 - val_loss: 0.3333 - val_acc: 0.8887\n",
      "Epoch 448/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2646 - acc: 0.9130 - val_loss: 0.3345 - val_acc: 0.8867\n",
      "Epoch 449/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2636 - acc: 0.9117 - val_loss: 0.3323 - val_acc: 0.8887\n",
      "Epoch 450/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2641 - acc: 0.9124 - val_loss: 0.3333 - val_acc: 0.8867\n",
      "Epoch 451/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2633 - acc: 0.9143 - val_loss: 0.3348 - val_acc: 0.8867\n",
      "Epoch 452/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2620 - acc: 0.9150 - val_loss: 0.3319 - val_acc: 0.8887\n",
      "Epoch 453/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2641 - acc: 0.9117 - val_loss: 0.3329 - val_acc: 0.8946\n",
      "Epoch 454/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2622 - acc: 0.9157 - val_loss: 0.3306 - val_acc: 0.8887\n",
      "Epoch 455/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2624 - acc: 0.9150 - val_loss: 0.3318 - val_acc: 0.8847\n",
      "Epoch 456/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2626 - acc: 0.9143 - val_loss: 0.3298 - val_acc: 0.8887\n",
      "Epoch 457/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2615 - acc: 0.9143 - val_loss: 0.3305 - val_acc: 0.8907\n",
      "Epoch 458/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2611 - acc: 0.9130 - val_loss: 0.3314 - val_acc: 0.8887\n",
      "Epoch 459/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2619 - acc: 0.9170 - val_loss: 0.3307 - val_acc: 0.8907\n",
      "Epoch 460/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2621 - acc: 0.9124 - val_loss: 0.3325 - val_acc: 0.8847\n",
      "Epoch 461/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2614 - acc: 0.9124 - val_loss: 0.3310 - val_acc: 0.8867\n",
      "Epoch 462/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2613 - acc: 0.9150 - val_loss: 0.3312 - val_acc: 0.8887\n",
      "Epoch 463/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2620 - acc: 0.9170 - val_loss: 0.3305 - val_acc: 0.8907\n",
      "Epoch 464/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2610 - acc: 0.9157 - val_loss: 0.3308 - val_acc: 0.8887\n",
      "Epoch 465/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2608 - acc: 0.9170 - val_loss: 0.3299 - val_acc: 0.8907\n",
      "Epoch 466/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2605 - acc: 0.9170 - val_loss: 0.3308 - val_acc: 0.8887\n",
      "Epoch 467/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2602 - acc: 0.9157 - val_loss: 0.3330 - val_acc: 0.8887\n",
      "Epoch 468/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2612 - acc: 0.9190 - val_loss: 0.3300 - val_acc: 0.8946\n",
      "Epoch 469/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2597 - acc: 0.9130 - val_loss: 0.3287 - val_acc: 0.8926\n",
      "Epoch 470/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2585 - acc: 0.9150 - val_loss: 0.3294 - val_acc: 0.8907\n",
      "Epoch 471/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2587 - acc: 0.9170 - val_loss: 0.3286 - val_acc: 0.8867\n",
      "Epoch 472/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2586 - acc: 0.9157 - val_loss: 0.3284 - val_acc: 0.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2593 - acc: 0.9150 - val_loss: 0.3293 - val_acc: 0.8887\n",
      "Epoch 474/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2591 - acc: 0.9157 - val_loss: 0.3307 - val_acc: 0.8867\n",
      "Epoch 475/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2592 - acc: 0.9170 - val_loss: 0.3302 - val_acc: 0.8907\n",
      "Epoch 476/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2584 - acc: 0.9137 - val_loss: 0.3284 - val_acc: 0.8867\n",
      "Epoch 477/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.2589 - acc: 0.9143 - val_loss: 0.3281 - val_acc: 0.8887\n",
      "Epoch 478/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2581 - acc: 0.9157 - val_loss: 0.3282 - val_acc: 0.8907\n",
      "Epoch 479/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2585 - acc: 0.9190 - val_loss: 0.3284 - val_acc: 0.8887\n",
      "Epoch 480/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2589 - acc: 0.9170 - val_loss: 0.3271 - val_acc: 0.8867\n",
      "Epoch 481/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2587 - acc: 0.9143 - val_loss: 0.3275 - val_acc: 0.8887\n",
      "Epoch 482/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2579 - acc: 0.9137 - val_loss: 0.3286 - val_acc: 0.8887\n",
      "Epoch 483/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2579 - acc: 0.9177 - val_loss: 0.3290 - val_acc: 0.8887\n",
      "Epoch 484/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2579 - acc: 0.9163 - val_loss: 0.3283 - val_acc: 0.8867\n",
      "Epoch 485/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2592 - acc: 0.9157 - val_loss: 0.3282 - val_acc: 0.8887\n",
      "Epoch 486/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2580 - acc: 0.9130 - val_loss: 0.3271 - val_acc: 0.8867\n",
      "Epoch 487/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2578 - acc: 0.9143 - val_loss: 0.3285 - val_acc: 0.8887\n",
      "Epoch 488/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2571 - acc: 0.9163 - val_loss: 0.3293 - val_acc: 0.8887\n",
      "Epoch 489/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2573 - acc: 0.9170 - val_loss: 0.3276 - val_acc: 0.8867\n",
      "Epoch 490/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2575 - acc: 0.9137 - val_loss: 0.3296 - val_acc: 0.8887\n",
      "Epoch 491/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2579 - acc: 0.9143 - val_loss: 0.3253 - val_acc: 0.8887\n",
      "Epoch 492/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2572 - acc: 0.9150 - val_loss: 0.3268 - val_acc: 0.8867\n",
      "Epoch 493/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2579 - acc: 0.9197 - val_loss: 0.3263 - val_acc: 0.8907\n",
      "Epoch 494/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2581 - acc: 0.9157 - val_loss: 0.3270 - val_acc: 0.8867\n",
      "Epoch 495/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2578 - acc: 0.9150 - val_loss: 0.3256 - val_acc: 0.8887\n",
      "Epoch 496/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2567 - acc: 0.9177 - val_loss: 0.3282 - val_acc: 0.8907\n",
      "Epoch 497/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2572 - acc: 0.9143 - val_loss: 0.3296 - val_acc: 0.8867\n",
      "Epoch 498/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2570 - acc: 0.9137 - val_loss: 0.3281 - val_acc: 0.8887\n",
      "Epoch 499/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2570 - acc: 0.9150 - val_loss: 0.3240 - val_acc: 0.8887\n",
      "Epoch 500/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2572 - acc: 0.9183 - val_loss: 0.3253 - val_acc: 0.8887\n",
      "Epoch 501/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2572 - acc: 0.9163 - val_loss: 0.3316 - val_acc: 0.8867\n",
      "Epoch 502/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2562 - acc: 0.9150 - val_loss: 0.3248 - val_acc: 0.8867\n",
      "Epoch 503/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2572 - acc: 0.9183 - val_loss: 0.3246 - val_acc: 0.8907\n",
      "Epoch 504/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2555 - acc: 0.9157 - val_loss: 0.3291 - val_acc: 0.8887\n",
      "Epoch 505/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2565 - acc: 0.9163 - val_loss: 0.3256 - val_acc: 0.8907\n",
      "Epoch 506/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2562 - acc: 0.9163 - val_loss: 0.3251 - val_acc: 0.8867\n",
      "Epoch 507/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2556 - acc: 0.9170 - val_loss: 0.3246 - val_acc: 0.8867\n",
      "Epoch 508/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2561 - acc: 0.9177 - val_loss: 0.3272 - val_acc: 0.8887\n",
      "Epoch 509/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2574 - acc: 0.9163 - val_loss: 0.3251 - val_acc: 0.8887\n",
      "Epoch 510/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2564 - acc: 0.9170 - val_loss: 0.3250 - val_acc: 0.8867\n",
      "Epoch 511/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2543 - acc: 0.9163 - val_loss: 0.3244 - val_acc: 0.8887\n",
      "Epoch 512/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2542 - acc: 0.9143 - val_loss: 0.3229 - val_acc: 0.8907\n",
      "Epoch 513/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2551 - acc: 0.9170 - val_loss: 0.3249 - val_acc: 0.8887\n",
      "Epoch 514/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2549 - acc: 0.9177 - val_loss: 0.3244 - val_acc: 0.8867\n",
      "Epoch 515/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2543 - acc: 0.9163 - val_loss: 0.3258 - val_acc: 0.8867\n",
      "Epoch 516/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2545 - acc: 0.9157 - val_loss: 0.3235 - val_acc: 0.8887\n",
      "Epoch 517/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2544 - acc: 0.9163 - val_loss: 0.3234 - val_acc: 0.8907\n",
      "Epoch 518/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2537 - acc: 0.9163 - val_loss: 0.3238 - val_acc: 0.8907\n",
      "Epoch 519/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2537 - acc: 0.9163 - val_loss: 0.3233 - val_acc: 0.8907\n",
      "Epoch 520/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2545 - acc: 0.9163 - val_loss: 0.3222 - val_acc: 0.8907\n",
      "Epoch 521/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2545 - acc: 0.9163 - val_loss: 0.3237 - val_acc: 0.8887\n",
      "Epoch 522/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2546 - acc: 0.9183 - val_loss: 0.3233 - val_acc: 0.8907\n",
      "Epoch 523/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2543 - acc: 0.9170 - val_loss: 0.3214 - val_acc: 0.8926\n",
      "Epoch 524/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2545 - acc: 0.9163 - val_loss: 0.3218 - val_acc: 0.8926\n",
      "Epoch 525/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2536 - acc: 0.9157 - val_loss: 0.3242 - val_acc: 0.8887\n",
      "Epoch 526/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2540 - acc: 0.9170 - val_loss: 0.3251 - val_acc: 0.8867\n",
      "Epoch 527/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2542 - acc: 0.9170 - val_loss: 0.3216 - val_acc: 0.8907\n",
      "Epoch 528/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2546 - acc: 0.9163 - val_loss: 0.3234 - val_acc: 0.8867\n",
      "Epoch 529/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2541 - acc: 0.9183 - val_loss: 0.3232 - val_acc: 0.8867\n",
      "Epoch 530/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2540 - acc: 0.9177 - val_loss: 0.3202 - val_acc: 0.8907\n",
      "Epoch 531/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2522 - acc: 0.9150 - val_loss: 0.3239 - val_acc: 0.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2512 - acc: 0.9170 - val_loss: 0.3203 - val_acc: 0.8887\n",
      "Epoch 533/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2543 - acc: 0.9210 - val_loss: 0.3248 - val_acc: 0.8887\n",
      "Epoch 534/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2522 - acc: 0.9177 - val_loss: 0.3210 - val_acc: 0.8907\n",
      "Epoch 535/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2529 - acc: 0.9177 - val_loss: 0.3217 - val_acc: 0.8887\n",
      "Epoch 536/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2526 - acc: 0.9183 - val_loss: 0.3185 - val_acc: 0.8907\n",
      "Epoch 537/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2518 - acc: 0.9203 - val_loss: 0.3302 - val_acc: 0.8847\n",
      "Epoch 538/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2525 - acc: 0.9157 - val_loss: 0.3209 - val_acc: 0.8887\n",
      "Epoch 539/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2525 - acc: 0.9157 - val_loss: 0.3202 - val_acc: 0.8867\n",
      "Epoch 540/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2518 - acc: 0.9177 - val_loss: 0.3218 - val_acc: 0.8867\n",
      "Epoch 541/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2509 - acc: 0.9163 - val_loss: 0.3218 - val_acc: 0.8887\n",
      "Epoch 542/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2512 - acc: 0.9157 - val_loss: 0.3184 - val_acc: 0.8926\n",
      "Epoch 543/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2512 - acc: 0.9170 - val_loss: 0.3218 - val_acc: 0.8887\n",
      "Epoch 544/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2518 - acc: 0.9170 - val_loss: 0.3239 - val_acc: 0.8887\n",
      "Epoch 545/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2520 - acc: 0.9190 - val_loss: 0.3167 - val_acc: 0.8907\n",
      "Epoch 546/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2512 - acc: 0.9170 - val_loss: 0.3238 - val_acc: 0.8867\n",
      "Epoch 547/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2521 - acc: 0.9163 - val_loss: 0.3156 - val_acc: 0.8966\n",
      "Epoch 548/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2512 - acc: 0.9190 - val_loss: 0.3180 - val_acc: 0.8966\n",
      "Epoch 549/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2509 - acc: 0.9163 - val_loss: 0.3185 - val_acc: 0.8926\n",
      "Epoch 550/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2496 - acc: 0.9197 - val_loss: 0.3174 - val_acc: 0.8926\n",
      "Epoch 551/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2504 - acc: 0.9150 - val_loss: 0.3209 - val_acc: 0.8907\n",
      "Epoch 552/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2508 - acc: 0.9197 - val_loss: 0.3240 - val_acc: 0.8887\n",
      "Epoch 553/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2495 - acc: 0.9177 - val_loss: 0.3213 - val_acc: 0.8907\n",
      "Epoch 554/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2505 - acc: 0.9190 - val_loss: 0.3163 - val_acc: 0.8926\n",
      "Epoch 555/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2488 - acc: 0.9197 - val_loss: 0.3165 - val_acc: 0.8926\n",
      "Epoch 556/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2490 - acc: 0.9157 - val_loss: 0.3179 - val_acc: 0.8867\n",
      "Epoch 557/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2487 - acc: 0.9170 - val_loss: 0.3208 - val_acc: 0.8907\n",
      "Epoch 558/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2490 - acc: 0.9197 - val_loss: 0.3177 - val_acc: 0.8926\n",
      "Epoch 559/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2488 - acc: 0.9163 - val_loss: 0.3150 - val_acc: 0.8907\n",
      "Epoch 560/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2492 - acc: 0.9157 - val_loss: 0.3152 - val_acc: 0.8926\n",
      "Epoch 561/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2483 - acc: 0.9163 - val_loss: 0.3153 - val_acc: 0.8926\n",
      "Epoch 562/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2496 - acc: 0.9190 - val_loss: 0.3170 - val_acc: 0.8887\n",
      "Epoch 563/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2482 - acc: 0.9177 - val_loss: 0.3165 - val_acc: 0.8867\n",
      "Epoch 564/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2481 - acc: 0.9190 - val_loss: 0.3162 - val_acc: 0.8887\n",
      "Epoch 565/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2478 - acc: 0.9170 - val_loss: 0.3139 - val_acc: 0.8926\n",
      "Epoch 566/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2486 - acc: 0.9170 - val_loss: 0.3156 - val_acc: 0.8926\n",
      "Epoch 567/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2479 - acc: 0.9157 - val_loss: 0.3147 - val_acc: 0.8926\n",
      "Epoch 568/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2483 - acc: 0.9210 - val_loss: 0.3145 - val_acc: 0.8946\n",
      "Epoch 569/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2472 - acc: 0.9183 - val_loss: 0.3150 - val_acc: 0.8907\n",
      "Epoch 570/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2481 - acc: 0.9190 - val_loss: 0.3148 - val_acc: 0.8887\n",
      "Epoch 571/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2475 - acc: 0.9170 - val_loss: 0.3149 - val_acc: 0.8867\n",
      "Epoch 572/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2468 - acc: 0.9177 - val_loss: 0.3160 - val_acc: 0.8907\n",
      "Epoch 573/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2470 - acc: 0.9163 - val_loss: 0.3144 - val_acc: 0.8907\n",
      "Epoch 574/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2478 - acc: 0.9203 - val_loss: 0.3129 - val_acc: 0.8946\n",
      "Epoch 575/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2462 - acc: 0.9177 - val_loss: 0.3158 - val_acc: 0.8907\n",
      "Epoch 576/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2468 - acc: 0.9197 - val_loss: 0.3126 - val_acc: 0.8926\n",
      "Epoch 577/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2460 - acc: 0.9150 - val_loss: 0.3168 - val_acc: 0.9006\n",
      "Epoch 578/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2476 - acc: 0.9177 - val_loss: 0.3138 - val_acc: 0.8946\n",
      "Epoch 579/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2459 - acc: 0.9183 - val_loss: 0.3134 - val_acc: 0.8907\n",
      "Epoch 580/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2466 - acc: 0.9157 - val_loss: 0.3156 - val_acc: 0.8907\n",
      "Epoch 581/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2507 - acc: 0.9210 - val_loss: 0.3122 - val_acc: 0.8907\n",
      "Epoch 582/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2463 - acc: 0.9203 - val_loss: 0.3110 - val_acc: 0.8946\n",
      "Epoch 583/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2459 - acc: 0.9177 - val_loss: 0.3144 - val_acc: 0.8847\n",
      "Epoch 584/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2466 - acc: 0.9157 - val_loss: 0.3138 - val_acc: 0.8887\n",
      "Epoch 585/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2466 - acc: 0.9197 - val_loss: 0.3180 - val_acc: 0.8887\n",
      "Epoch 586/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2477 - acc: 0.9150 - val_loss: 0.3112 - val_acc: 0.8926\n",
      "Epoch 587/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2469 - acc: 0.9210 - val_loss: 0.3124 - val_acc: 0.8907\n",
      "Epoch 588/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2458 - acc: 0.9150 - val_loss: 0.3115 - val_acc: 0.8907\n",
      "Epoch 589/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2452 - acc: 0.9163 - val_loss: 0.3146 - val_acc: 0.8907\n",
      "Epoch 590/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2453 - acc: 0.9157 - val_loss: 0.3091 - val_acc: 0.8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2459 - acc: 0.9170 - val_loss: 0.3113 - val_acc: 0.8946\n",
      "Epoch 592/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2438 - acc: 0.9170 - val_loss: 0.3129 - val_acc: 0.8887\n",
      "Epoch 593/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2451 - acc: 0.9157 - val_loss: 0.3101 - val_acc: 0.8926\n",
      "Epoch 594/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2439 - acc: 0.9190 - val_loss: 0.3124 - val_acc: 0.8887\n",
      "Epoch 595/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2442 - acc: 0.9163 - val_loss: 0.3115 - val_acc: 0.8907\n",
      "Epoch 596/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2454 - acc: 0.9170 - val_loss: 0.3083 - val_acc: 0.8966\n",
      "Epoch 597/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2447 - acc: 0.9183 - val_loss: 0.3101 - val_acc: 0.8907\n",
      "Epoch 598/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2435 - acc: 0.9203 - val_loss: 0.3129 - val_acc: 0.8907\n",
      "Epoch 599/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2438 - acc: 0.9223 - val_loss: 0.3115 - val_acc: 0.8887\n",
      "Epoch 600/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.2439 - acc: 0.9170 - val_loss: 0.3113 - val_acc: 0.8926\n",
      "Epoch 601/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2442 - acc: 0.9163 - val_loss: 0.3121 - val_acc: 0.8926\n",
      "Epoch 602/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2434 - acc: 0.9197 - val_loss: 0.3130 - val_acc: 0.8887\n",
      "Epoch 603/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2430 - acc: 0.9177 - val_loss: 0.3108 - val_acc: 0.8907\n",
      "Epoch 604/3000\n",
      "1506/1506 [==============================] - 0s 141us/step - loss: 0.2432 - acc: 0.9177 - val_loss: 0.3092 - val_acc: 0.8946\n",
      "Epoch 605/3000\n",
      "1506/1506 [==============================] - 0s 197us/step - loss: 0.2453 - acc: 0.9216 - val_loss: 0.3120 - val_acc: 0.8887\n",
      "Epoch 606/3000\n",
      "1506/1506 [==============================] - 0s 129us/step - loss: 0.2443 - acc: 0.9190 - val_loss: 0.3108 - val_acc: 0.8946\n",
      "Epoch 607/3000\n",
      "1506/1506 [==============================] - 0s 105us/step - loss: 0.2446 - acc: 0.9210 - val_loss: 0.3112 - val_acc: 0.8907\n",
      "Epoch 608/3000\n",
      "1506/1506 [==============================] - 0s 111us/step - loss: 0.2425 - acc: 0.9190 - val_loss: 0.3098 - val_acc: 0.8926\n",
      "Epoch 609/3000\n",
      "1506/1506 [==============================] - 0s 92us/step - loss: 0.2422 - acc: 0.9183 - val_loss: 0.3110 - val_acc: 0.8887\n",
      "Epoch 610/3000\n",
      "1506/1506 [==============================] - 0s 110us/step - loss: 0.2425 - acc: 0.9190 - val_loss: 0.3101 - val_acc: 0.8907\n",
      "Epoch 611/3000\n",
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.2422 - acc: 0.9223 - val_loss: 0.3106 - val_acc: 0.8907\n",
      "Epoch 612/3000\n",
      "1506/1506 [==============================] - 0s 89us/step - loss: 0.2436 - acc: 0.9183 - val_loss: 0.3112 - val_acc: 0.8907\n",
      "Epoch 613/3000\n",
      "1506/1506 [==============================] - 0s 91us/step - loss: 0.2430 - acc: 0.9197 - val_loss: 0.3108 - val_acc: 0.8926\n",
      "Epoch 614/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2435 - acc: 0.9170 - val_loss: 0.3115 - val_acc: 0.8926\n",
      "Epoch 615/3000\n",
      "1506/1506 [==============================] - 0s 104us/step - loss: 0.2434 - acc: 0.9177 - val_loss: 0.3109 - val_acc: 0.8926\n",
      "Epoch 616/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2423 - acc: 0.9216 - val_loss: 0.3113 - val_acc: 0.8867\n",
      "Epoch 00616: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adamax,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist5=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1506 samples, validate on 503 samples\n",
      "Epoch 1/3000\n",
      "1506/1506 [==============================] - 1s 551us/step - loss: 2.1991 - acc: 0.2045 - val_loss: 2.0864 - val_acc: 0.2366\n",
      "Epoch 2/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 2.0650 - acc: 0.2185 - val_loss: 1.9883 - val_acc: 0.2565\n",
      "Epoch 3/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.9438 - acc: 0.3406 - val_loss: 1.8238 - val_acc: 0.4414\n",
      "Epoch 4/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 1.7697 - acc: 0.3958 - val_loss: 1.6622 - val_acc: 0.4414\n",
      "Epoch 5/3000\n",
      "1506/1506 [==============================] - 0s 89us/step - loss: 1.5988 - acc: 0.3971 - val_loss: 1.5365 - val_acc: 0.4692\n",
      "Epoch 6/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 1.4753 - acc: 0.4495 - val_loss: 1.4389 - val_acc: 0.4851\n",
      "Epoch 7/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 1.3691 - acc: 0.5279 - val_loss: 1.3493 - val_acc: 0.5447\n",
      "Epoch 8/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 1.2662 - acc: 0.6089 - val_loss: 1.2937 - val_acc: 0.6143\n",
      "Epoch 9/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 1.1745 - acc: 0.6627 - val_loss: 1.1823 - val_acc: 0.7018\n",
      "Epoch 10/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 1.0832 - acc: 0.7058 - val_loss: 1.0840 - val_acc: 0.7296\n",
      "Epoch 11/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.9962 - acc: 0.7444 - val_loss: 1.0136 - val_acc: 0.7952\n",
      "Epoch 12/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.9160 - acc: 0.7908 - val_loss: 0.9499 - val_acc: 0.6899\n",
      "Epoch 13/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.8436 - acc: 0.7981 - val_loss: 0.8911 - val_acc: 0.7435\n",
      "Epoch 14/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.7841 - acc: 0.8340 - val_loss: 0.8305 - val_acc: 0.7674\n",
      "Epoch 15/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.7356 - acc: 0.8466 - val_loss: 0.7961 - val_acc: 0.8191\n",
      "Epoch 16/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.6946 - acc: 0.8526 - val_loss: 0.7410 - val_acc: 0.8231\n",
      "Epoch 17/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.6593 - acc: 0.8639 - val_loss: 0.7160 - val_acc: 0.8370\n",
      "Epoch 18/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.6317 - acc: 0.8652 - val_loss: 0.6897 - val_acc: 0.8330\n",
      "Epoch 19/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.6065 - acc: 0.8652 - val_loss: 0.6600 - val_acc: 0.8449\n",
      "Epoch 20/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.5864 - acc: 0.8685 - val_loss: 0.6444 - val_acc: 0.8449\n",
      "Epoch 21/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.5678 - acc: 0.8692 - val_loss: 0.6256 - val_acc: 0.8410\n",
      "Epoch 22/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.5511 - acc: 0.8672 - val_loss: 0.6126 - val_acc: 0.8489\n",
      "Epoch 23/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.5372 - acc: 0.8745 - val_loss: 0.6008 - val_acc: 0.8469\n",
      "Epoch 24/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.5245 - acc: 0.8765 - val_loss: 0.5939 - val_acc: 0.8509\n",
      "Epoch 25/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.5142 - acc: 0.8785 - val_loss: 0.5748 - val_acc: 0.8469\n",
      "Epoch 26/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.5058 - acc: 0.8752 - val_loss: 0.5795 - val_acc: 0.8410\n",
      "Epoch 27/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4985 - acc: 0.8752 - val_loss: 0.5610 - val_acc: 0.8529\n",
      "Epoch 28/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4914 - acc: 0.8752 - val_loss: 0.5732 - val_acc: 0.8390\n",
      "Epoch 29/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4870 - acc: 0.8758 - val_loss: 0.5488 - val_acc: 0.8588\n",
      "Epoch 30/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.4790 - acc: 0.8811 - val_loss: 0.5574 - val_acc: 0.8509\n",
      "Epoch 31/3000\n",
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.4733 - acc: 0.8851 - val_loss: 0.5494 - val_acc: 0.8489\n",
      "Epoch 32/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.4690 - acc: 0.8825 - val_loss: 0.5395 - val_acc: 0.8509\n",
      "Epoch 33/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4651 - acc: 0.8838 - val_loss: 0.5321 - val_acc: 0.8569\n",
      "Epoch 34/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4608 - acc: 0.8838 - val_loss: 0.5290 - val_acc: 0.8489\n",
      "Epoch 35/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.4556 - acc: 0.8851 - val_loss: 0.5281 - val_acc: 0.8529\n",
      "Epoch 36/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4521 - acc: 0.8845 - val_loss: 0.5190 - val_acc: 0.8569\n",
      "Epoch 37/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.4472 - acc: 0.8851 - val_loss: 0.5122 - val_acc: 0.8529\n",
      "Epoch 38/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4428 - acc: 0.8865 - val_loss: 0.5154 - val_acc: 0.8509\n",
      "Epoch 39/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4387 - acc: 0.8845 - val_loss: 0.5044 - val_acc: 0.8509\n",
      "Epoch 40/3000\n",
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.4357 - acc: 0.8891 - val_loss: 0.5078 - val_acc: 0.8549\n",
      "Epoch 41/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4318 - acc: 0.8918 - val_loss: 0.5067 - val_acc: 0.8509\n",
      "Epoch 42/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4290 - acc: 0.8865 - val_loss: 0.4965 - val_acc: 0.8569\n",
      "Epoch 43/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4253 - acc: 0.8878 - val_loss: 0.4969 - val_acc: 0.8489\n",
      "Epoch 44/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.4214 - acc: 0.8878 - val_loss: 0.4915 - val_acc: 0.8648\n",
      "Epoch 45/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.4200 - acc: 0.8918 - val_loss: 0.5061 - val_acc: 0.8588\n",
      "Epoch 46/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.4171 - acc: 0.8911 - val_loss: 0.4901 - val_acc: 0.8549\n",
      "Epoch 47/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4130 - acc: 0.8904 - val_loss: 0.4862 - val_acc: 0.8549\n",
      "Epoch 48/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.4106 - acc: 0.8871 - val_loss: 0.4938 - val_acc: 0.8549\n",
      "Epoch 49/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4106 - acc: 0.8931 - val_loss: 0.4812 - val_acc: 0.8588\n",
      "Epoch 50/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4060 - acc: 0.8904 - val_loss: 0.4816 - val_acc: 0.8588\n",
      "Epoch 51/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.4048 - acc: 0.8904 - val_loss: 0.4916 - val_acc: 0.8569\n",
      "Epoch 52/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.4028 - acc: 0.8904 - val_loss: 0.4737 - val_acc: 0.8549\n",
      "Epoch 53/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3988 - acc: 0.8924 - val_loss: 0.4757 - val_acc: 0.8648\n",
      "Epoch 54/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.3978 - acc: 0.8918 - val_loss: 0.4729 - val_acc: 0.8588\n",
      "Epoch 55/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3964 - acc: 0.8911 - val_loss: 0.4680 - val_acc: 0.8588\n",
      "Epoch 56/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3930 - acc: 0.8924 - val_loss: 0.4809 - val_acc: 0.8648\n",
      "Epoch 57/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3938 - acc: 0.8911 - val_loss: 0.4808 - val_acc: 0.8648\n",
      "Epoch 58/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3916 - acc: 0.8931 - val_loss: 0.4697 - val_acc: 0.8608\n",
      "Epoch 59/3000\n",
      "1506/1506 [==============================] - 0s 96us/step - loss: 0.3882 - acc: 0.8924 - val_loss: 0.4678 - val_acc: 0.8628\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.3868 - acc: 0.8944 - val_loss: 0.4648 - val_acc: 0.8569\n",
      "Epoch 61/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3850 - acc: 0.8911 - val_loss: 0.4603 - val_acc: 0.8628\n",
      "Epoch 62/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.3828 - acc: 0.8938 - val_loss: 0.4586 - val_acc: 0.8608\n",
      "Epoch 63/3000\n",
      "1506/1506 [==============================] - 0s 119us/step - loss: 0.3811 - acc: 0.8944 - val_loss: 0.4590 - val_acc: 0.8549\n",
      "Epoch 64/3000\n",
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.3795 - acc: 0.8944 - val_loss: 0.4573 - val_acc: 0.8569\n",
      "Epoch 65/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3771 - acc: 0.8918 - val_loss: 0.4540 - val_acc: 0.8588\n",
      "Epoch 66/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.3755 - acc: 0.8951 - val_loss: 0.4549 - val_acc: 0.8648\n",
      "Epoch 67/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.3731 - acc: 0.8971 - val_loss: 0.4511 - val_acc: 0.8569\n",
      "Epoch 68/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3751 - acc: 0.8924 - val_loss: 0.4452 - val_acc: 0.8608\n",
      "Epoch 69/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3712 - acc: 0.8938 - val_loss: 0.4492 - val_acc: 0.8569\n",
      "Epoch 70/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3711 - acc: 0.8918 - val_loss: 0.4555 - val_acc: 0.8509\n",
      "Epoch 71/3000\n",
      "1506/1506 [==============================] - 0s 85us/step - loss: 0.3689 - acc: 0.8931 - val_loss: 0.4441 - val_acc: 0.8648\n",
      "Epoch 72/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.3674 - acc: 0.8924 - val_loss: 0.4484 - val_acc: 0.8648\n",
      "Epoch 73/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.3656 - acc: 0.8938 - val_loss: 0.4443 - val_acc: 0.8588\n",
      "Epoch 74/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.3635 - acc: 0.8938 - val_loss: 0.4472 - val_acc: 0.8569\n",
      "Epoch 75/3000\n",
      "1506/1506 [==============================] - 0s 98us/step - loss: 0.3643 - acc: 0.8924 - val_loss: 0.4478 - val_acc: 0.8628\n",
      "Epoch 76/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3629 - acc: 0.8944 - val_loss: 0.4457 - val_acc: 0.8628\n",
      "Epoch 77/3000\n",
      "1506/1506 [==============================] - 0s 93us/step - loss: 0.3609 - acc: 0.8951 - val_loss: 0.4435 - val_acc: 0.8588\n",
      "Epoch 78/3000\n",
      "1506/1506 [==============================] - 0s 100us/step - loss: 0.3593 - acc: 0.8958 - val_loss: 0.4563 - val_acc: 0.8648\n",
      "Epoch 79/3000\n",
      "1506/1506 [==============================] - 0s 100us/step - loss: 0.3594 - acc: 0.8958 - val_loss: 0.4356 - val_acc: 0.8628\n",
      "Epoch 80/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3560 - acc: 0.8964 - val_loss: 0.4375 - val_acc: 0.8588\n",
      "Epoch 81/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3563 - acc: 0.8944 - val_loss: 0.4461 - val_acc: 0.8588\n",
      "Epoch 82/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3542 - acc: 0.8958 - val_loss: 0.4293 - val_acc: 0.8569\n",
      "Epoch 83/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3511 - acc: 0.8938 - val_loss: 0.4430 - val_acc: 0.8728\n",
      "Epoch 84/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3525 - acc: 0.8911 - val_loss: 0.4423 - val_acc: 0.8728\n",
      "Epoch 85/3000\n",
      "1506/1506 [==============================] - 0s 100us/step - loss: 0.3513 - acc: 0.8938 - val_loss: 0.4313 - val_acc: 0.8728\n",
      "Epoch 86/3000\n",
      "1506/1506 [==============================] - 0s 90us/step - loss: 0.3499 - acc: 0.8958 - val_loss: 0.4395 - val_acc: 0.8688\n",
      "Epoch 87/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3481 - acc: 0.8984 - val_loss: 0.4286 - val_acc: 0.8728\n",
      "Epoch 88/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3475 - acc: 0.8951 - val_loss: 0.4350 - val_acc: 0.8648\n",
      "Epoch 89/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3464 - acc: 0.8958 - val_loss: 0.4263 - val_acc: 0.8509\n",
      "Epoch 90/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3452 - acc: 0.9004 - val_loss: 0.4267 - val_acc: 0.8628\n",
      "Epoch 91/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3459 - acc: 0.8951 - val_loss: 0.4221 - val_acc: 0.8688\n",
      "Epoch 92/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3433 - acc: 0.8984 - val_loss: 0.4302 - val_acc: 0.8688\n",
      "Epoch 93/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3452 - acc: 0.8977 - val_loss: 0.4196 - val_acc: 0.8708\n",
      "Epoch 94/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3424 - acc: 0.8991 - val_loss: 0.4296 - val_acc: 0.8728\n",
      "Epoch 95/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.3426 - acc: 0.9017 - val_loss: 0.4178 - val_acc: 0.8708\n",
      "Epoch 96/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3407 - acc: 0.9004 - val_loss: 0.4246 - val_acc: 0.8728\n",
      "Epoch 97/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3410 - acc: 0.8991 - val_loss: 0.4203 - val_acc: 0.8608\n",
      "Epoch 98/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3411 - acc: 0.8984 - val_loss: 0.4181 - val_acc: 0.8588\n",
      "Epoch 99/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3381 - acc: 0.8984 - val_loss: 0.4196 - val_acc: 0.8688\n",
      "Epoch 100/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3394 - acc: 0.9004 - val_loss: 0.4208 - val_acc: 0.8688\n",
      "Epoch 101/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3376 - acc: 0.8984 - val_loss: 0.4154 - val_acc: 0.8728\n",
      "Epoch 102/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3357 - acc: 0.9004 - val_loss: 0.4159 - val_acc: 0.8628\n",
      "Epoch 103/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.3345 - acc: 0.8991 - val_loss: 0.4279 - val_acc: 0.8648\n",
      "Epoch 104/3000\n",
      "1506/1506 [==============================] - 0s 102us/step - loss: 0.3346 - acc: 0.8977 - val_loss: 0.4279 - val_acc: 0.8708\n",
      "Epoch 105/3000\n",
      "1506/1506 [==============================] - 0s 106us/step - loss: 0.3359 - acc: 0.8997 - val_loss: 0.4127 - val_acc: 0.8668\n",
      "Epoch 106/3000\n",
      "1506/1506 [==============================] - 0s 114us/step - loss: 0.3333 - acc: 0.8984 - val_loss: 0.4094 - val_acc: 0.8748\n",
      "Epoch 107/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.3300 - acc: 0.9044 - val_loss: 0.4105 - val_acc: 0.8648\n",
      "Epoch 108/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.3311 - acc: 0.8991 - val_loss: 0.4141 - val_acc: 0.8668\n",
      "Epoch 109/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3308 - acc: 0.9011 - val_loss: 0.4085 - val_acc: 0.8668\n",
      "Epoch 110/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3295 - acc: 0.9017 - val_loss: 0.4094 - val_acc: 0.8708\n",
      "Epoch 111/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3289 - acc: 0.8991 - val_loss: 0.4071 - val_acc: 0.8708\n",
      "Epoch 112/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3286 - acc: 0.8991 - val_loss: 0.4151 - val_acc: 0.8708\n",
      "Epoch 113/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3279 - acc: 0.8997 - val_loss: 0.4169 - val_acc: 0.8688\n",
      "Epoch 114/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3282 - acc: 0.8997 - val_loss: 0.4088 - val_acc: 0.8688\n",
      "Epoch 115/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3257 - acc: 0.9017 - val_loss: 0.4129 - val_acc: 0.8767\n",
      "Epoch 116/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3272 - acc: 0.9004 - val_loss: 0.4140 - val_acc: 0.8648\n",
      "Epoch 117/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3254 - acc: 0.9024 - val_loss: 0.4048 - val_acc: 0.8748\n",
      "Epoch 118/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.3247 - acc: 0.9037 - val_loss: 0.4018 - val_acc: 0.8728\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3226 - acc: 0.9050 - val_loss: 0.4059 - val_acc: 0.8728\n",
      "Epoch 120/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3232 - acc: 0.9017 - val_loss: 0.4002 - val_acc: 0.8767\n",
      "Epoch 121/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3221 - acc: 0.9037 - val_loss: 0.4031 - val_acc: 0.8708\n",
      "Epoch 122/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3215 - acc: 0.8991 - val_loss: 0.4028 - val_acc: 0.8787\n",
      "Epoch 123/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3211 - acc: 0.9044 - val_loss: 0.4105 - val_acc: 0.8787\n",
      "Epoch 124/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.3200 - acc: 0.9024 - val_loss: 0.4040 - val_acc: 0.8787\n",
      "Epoch 125/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3195 - acc: 0.9057 - val_loss: 0.4091 - val_acc: 0.8748\n",
      "Epoch 126/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3202 - acc: 0.9057 - val_loss: 0.4076 - val_acc: 0.8708\n",
      "Epoch 127/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.3206 - acc: 0.9050 - val_loss: 0.4025 - val_acc: 0.8728\n",
      "Epoch 128/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.3193 - acc: 0.9024 - val_loss: 0.3993 - val_acc: 0.8748\n",
      "Epoch 129/3000\n",
      "1506/1506 [==============================] - 0s 65us/step - loss: 0.3176 - acc: 0.9057 - val_loss: 0.4029 - val_acc: 0.8748\n",
      "Epoch 130/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3200 - acc: 0.9037 - val_loss: 0.4030 - val_acc: 0.8628\n",
      "Epoch 131/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3167 - acc: 0.9084 - val_loss: 0.4014 - val_acc: 0.8787\n",
      "Epoch 132/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3158 - acc: 0.9064 - val_loss: 0.3935 - val_acc: 0.8728\n",
      "Epoch 133/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3151 - acc: 0.9070 - val_loss: 0.3924 - val_acc: 0.8767\n",
      "Epoch 134/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.3156 - acc: 0.9050 - val_loss: 0.3951 - val_acc: 0.8787\n",
      "Epoch 135/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.3149 - acc: 0.9064 - val_loss: 0.3950 - val_acc: 0.8787\n",
      "Epoch 136/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3110 - acc: 0.9077 - val_loss: 0.3896 - val_acc: 0.8748\n",
      "Epoch 137/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3145 - acc: 0.9057 - val_loss: 0.3882 - val_acc: 0.8827\n",
      "Epoch 138/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3117 - acc: 0.9090 - val_loss: 0.3933 - val_acc: 0.8787\n",
      "Epoch 139/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.3102 - acc: 0.9050 - val_loss: 0.4032 - val_acc: 0.8748\n",
      "Epoch 140/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.3127 - acc: 0.9050 - val_loss: 0.3940 - val_acc: 0.8807\n",
      "Epoch 141/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3117 - acc: 0.9057 - val_loss: 0.3926 - val_acc: 0.8728\n",
      "Epoch 142/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3097 - acc: 0.9077 - val_loss: 0.3936 - val_acc: 0.8787\n",
      "Epoch 143/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3104 - acc: 0.9077 - val_loss: 0.3903 - val_acc: 0.8787\n",
      "Epoch 144/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3101 - acc: 0.9070 - val_loss: 0.3877 - val_acc: 0.8787\n",
      "Epoch 145/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3084 - acc: 0.9077 - val_loss: 0.3906 - val_acc: 0.8787\n",
      "Epoch 146/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.3095 - acc: 0.9050 - val_loss: 0.3920 - val_acc: 0.8767\n",
      "Epoch 147/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3084 - acc: 0.9077 - val_loss: 0.3935 - val_acc: 0.8748\n",
      "Epoch 148/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3065 - acc: 0.9090 - val_loss: 0.3908 - val_acc: 0.8648\n",
      "Epoch 149/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.3064 - acc: 0.9070 - val_loss: 0.3831 - val_acc: 0.8767\n",
      "Epoch 150/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3061 - acc: 0.9077 - val_loss: 0.3909 - val_acc: 0.8847\n",
      "Epoch 151/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3063 - acc: 0.9057 - val_loss: 0.3956 - val_acc: 0.8787\n",
      "Epoch 152/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3021 - acc: 0.9110 - val_loss: 0.3831 - val_acc: 0.8787\n",
      "Epoch 153/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.3047 - acc: 0.9110 - val_loss: 0.3891 - val_acc: 0.8827\n",
      "Epoch 154/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3038 - acc: 0.9084 - val_loss: 0.3928 - val_acc: 0.8867\n",
      "Epoch 155/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3045 - acc: 0.9097 - val_loss: 0.3836 - val_acc: 0.8748\n",
      "Epoch 156/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3029 - acc: 0.9097 - val_loss: 0.3854 - val_acc: 0.8807\n",
      "Epoch 157/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.3022 - acc: 0.9110 - val_loss: 0.3780 - val_acc: 0.8827\n",
      "Epoch 158/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.3027 - acc: 0.9077 - val_loss: 0.3828 - val_acc: 0.8807\n",
      "Epoch 159/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.3016 - acc: 0.9084 - val_loss: 0.3857 - val_acc: 0.8787\n",
      "Epoch 160/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.3009 - acc: 0.9070 - val_loss: 0.3798 - val_acc: 0.8787\n",
      "Epoch 161/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2993 - acc: 0.9097 - val_loss: 0.3857 - val_acc: 0.8787\n",
      "Epoch 162/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3004 - acc: 0.9104 - val_loss: 0.3880 - val_acc: 0.8827\n",
      "Epoch 163/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.3012 - acc: 0.9090 - val_loss: 0.3784 - val_acc: 0.8807\n",
      "Epoch 164/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2997 - acc: 0.9097 - val_loss: 0.3858 - val_acc: 0.8787\n",
      "Epoch 165/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2987 - acc: 0.9117 - val_loss: 0.3809 - val_acc: 0.8867\n",
      "Epoch 166/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2987 - acc: 0.9070 - val_loss: 0.3740 - val_acc: 0.8787\n",
      "Epoch 167/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2963 - acc: 0.9090 - val_loss: 0.3917 - val_acc: 0.8748\n",
      "Epoch 168/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2957 - acc: 0.9097 - val_loss: 0.3807 - val_acc: 0.8787\n",
      "Epoch 169/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2964 - acc: 0.9097 - val_loss: 0.3840 - val_acc: 0.8787\n",
      "Epoch 170/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2963 - acc: 0.9097 - val_loss: 0.3767 - val_acc: 0.8807\n",
      "Epoch 171/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2958 - acc: 0.9090 - val_loss: 0.3791 - val_acc: 0.8867\n",
      "Epoch 172/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2943 - acc: 0.9097 - val_loss: 0.3823 - val_acc: 0.8867\n",
      "Epoch 173/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2957 - acc: 0.9084 - val_loss: 0.3907 - val_acc: 0.8807\n",
      "Epoch 174/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2934 - acc: 0.9124 - val_loss: 0.3802 - val_acc: 0.8807\n",
      "Epoch 175/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2936 - acc: 0.9077 - val_loss: 0.3723 - val_acc: 0.8807\n",
      "Epoch 176/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2925 - acc: 0.9077 - val_loss: 0.3722 - val_acc: 0.8827\n",
      "Epoch 177/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2912 - acc: 0.9117 - val_loss: 0.3752 - val_acc: 0.8807\n",
      "Epoch 178/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2917 - acc: 0.9104 - val_loss: 0.3735 - val_acc: 0.8827\n",
      "Epoch 179/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2935 - acc: 0.9090 - val_loss: 0.3716 - val_acc: 0.8807\n",
      "Epoch 180/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2914 - acc: 0.9110 - val_loss: 0.3780 - val_acc: 0.8847\n",
      "Epoch 181/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2937 - acc: 0.9084 - val_loss: 0.3787 - val_acc: 0.8847\n",
      "Epoch 182/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2920 - acc: 0.9097 - val_loss: 0.3723 - val_acc: 0.8787\n",
      "Epoch 183/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2911 - acc: 0.9117 - val_loss: 0.3741 - val_acc: 0.8807\n",
      "Epoch 184/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2913 - acc: 0.9104 - val_loss: 0.3786 - val_acc: 0.8807\n",
      "Epoch 185/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2921 - acc: 0.9090 - val_loss: 0.3724 - val_acc: 0.8827\n",
      "Epoch 186/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2885 - acc: 0.9090 - val_loss: 0.3824 - val_acc: 0.8767\n",
      "Epoch 187/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2923 - acc: 0.9104 - val_loss: 0.3744 - val_acc: 0.8827\n",
      "Epoch 188/3000\n",
      "1506/1506 [==============================] - 0s 66us/step - loss: 0.2897 - acc: 0.9110 - val_loss: 0.3681 - val_acc: 0.8847\n",
      "Epoch 189/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2883 - acc: 0.9104 - val_loss: 0.3744 - val_acc: 0.8847\n",
      "Epoch 190/3000\n",
      "1506/1506 [==============================] - 0s 87us/step - loss: 0.2874 - acc: 0.9124 - val_loss: 0.3979 - val_acc: 0.8728\n",
      "Epoch 191/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2890 - acc: 0.9097 - val_loss: 0.3688 - val_acc: 0.8867\n",
      "Epoch 192/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2869 - acc: 0.9104 - val_loss: 0.3668 - val_acc: 0.8827\n",
      "Epoch 193/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2872 - acc: 0.9137 - val_loss: 0.3771 - val_acc: 0.8827\n",
      "Epoch 194/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2866 - acc: 0.9104 - val_loss: 0.3682 - val_acc: 0.8807\n",
      "Epoch 195/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2863 - acc: 0.9117 - val_loss: 0.3812 - val_acc: 0.8807\n",
      "Epoch 196/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2861 - acc: 0.9104 - val_loss: 0.3671 - val_acc: 0.8887\n",
      "Epoch 197/3000\n",
      "1506/1506 [==============================] - 0s 92us/step - loss: 0.2860 - acc: 0.9130 - val_loss: 0.3863 - val_acc: 0.8827\n",
      "Epoch 198/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2851 - acc: 0.9130 - val_loss: 0.3727 - val_acc: 0.8807\n",
      "Epoch 199/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2841 - acc: 0.9117 - val_loss: 0.3754 - val_acc: 0.8827\n",
      "Epoch 200/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2838 - acc: 0.9110 - val_loss: 0.3665 - val_acc: 0.8787\n",
      "Epoch 201/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2805 - acc: 0.9143 - val_loss: 0.3644 - val_acc: 0.8867\n",
      "Epoch 202/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.2848 - acc: 0.9124 - val_loss: 0.3611 - val_acc: 0.8847\n",
      "Epoch 203/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2819 - acc: 0.9157 - val_loss: 0.3577 - val_acc: 0.8827\n",
      "Epoch 204/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.2812 - acc: 0.9137 - val_loss: 0.3818 - val_acc: 0.8807\n",
      "Epoch 205/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2851 - acc: 0.9124 - val_loss: 0.3580 - val_acc: 0.8787\n",
      "Epoch 206/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2819 - acc: 0.9130 - val_loss: 0.3707 - val_acc: 0.8807\n",
      "Epoch 207/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2850 - acc: 0.9104 - val_loss: 0.3734 - val_acc: 0.8787\n",
      "Epoch 208/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2816 - acc: 0.9104 - val_loss: 0.3738 - val_acc: 0.8787\n",
      "Epoch 209/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.2830 - acc: 0.9130 - val_loss: 0.3646 - val_acc: 0.8827\n",
      "Epoch 210/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2817 - acc: 0.9117 - val_loss: 0.3822 - val_acc: 0.8827\n",
      "Epoch 211/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2819 - acc: 0.9117 - val_loss: 0.3636 - val_acc: 0.8807\n",
      "Epoch 212/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2802 - acc: 0.9097 - val_loss: 0.3986 - val_acc: 0.8748\n",
      "Epoch 213/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2827 - acc: 0.9110 - val_loss: 0.3586 - val_acc: 0.8807\n",
      "Epoch 214/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2797 - acc: 0.9137 - val_loss: 0.3542 - val_acc: 0.8827\n",
      "Epoch 215/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2782 - acc: 0.9130 - val_loss: 0.3631 - val_acc: 0.8827\n",
      "Epoch 216/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2806 - acc: 0.9117 - val_loss: 0.3590 - val_acc: 0.8807\n",
      "Epoch 217/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2796 - acc: 0.9143 - val_loss: 0.3536 - val_acc: 0.8847\n",
      "Epoch 218/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2760 - acc: 0.9137 - val_loss: 0.3622 - val_acc: 0.8787\n",
      "Epoch 219/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2784 - acc: 0.9150 - val_loss: 0.3615 - val_acc: 0.8827\n",
      "Epoch 220/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2780 - acc: 0.9117 - val_loss: 0.3591 - val_acc: 0.8767\n",
      "Epoch 221/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2770 - acc: 0.9143 - val_loss: 0.3497 - val_acc: 0.8887\n",
      "Epoch 222/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.2763 - acc: 0.9130 - val_loss: 0.3514 - val_acc: 0.8887\n",
      "Epoch 223/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2779 - acc: 0.9143 - val_loss: 0.3526 - val_acc: 0.8867\n",
      "Epoch 224/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2761 - acc: 0.9124 - val_loss: 0.3729 - val_acc: 0.8807\n",
      "Epoch 225/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2754 - acc: 0.9124 - val_loss: 0.3536 - val_acc: 0.8827\n",
      "Epoch 226/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2761 - acc: 0.9117 - val_loss: 0.3849 - val_acc: 0.8688\n",
      "Epoch 227/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2757 - acc: 0.9137 - val_loss: 0.3581 - val_acc: 0.8867\n",
      "Epoch 228/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2765 - acc: 0.9143 - val_loss: 0.3559 - val_acc: 0.8867\n",
      "Epoch 229/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2743 - acc: 0.9137 - val_loss: 0.3619 - val_acc: 0.8827\n",
      "Epoch 230/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2738 - acc: 0.9150 - val_loss: 0.3587 - val_acc: 0.8807\n",
      "Epoch 231/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2739 - acc: 0.9130 - val_loss: 0.3628 - val_acc: 0.8807\n",
      "Epoch 232/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2762 - acc: 0.9130 - val_loss: 0.3652 - val_acc: 0.8807\n",
      "Epoch 233/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2753 - acc: 0.9137 - val_loss: 0.3727 - val_acc: 0.8847\n",
      "Epoch 234/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2745 - acc: 0.9143 - val_loss: 0.3613 - val_acc: 0.8827\n",
      "Epoch 235/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2721 - acc: 0.9137 - val_loss: 0.3904 - val_acc: 0.8807\n",
      "Epoch 236/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2737 - acc: 0.9143 - val_loss: 0.3501 - val_acc: 0.8887\n",
      "Epoch 237/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2735 - acc: 0.9163 - val_loss: 0.3539 - val_acc: 0.8847\n",
      "Epoch 238/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2714 - acc: 0.9157 - val_loss: 0.3576 - val_acc: 0.8827\n",
      "Epoch 239/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2729 - acc: 0.9130 - val_loss: 0.3485 - val_acc: 0.8887\n",
      "Epoch 240/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2716 - acc: 0.9124 - val_loss: 0.3460 - val_acc: 0.8887\n",
      "Epoch 241/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2702 - acc: 0.9177 - val_loss: 0.3573 - val_acc: 0.8887\n",
      "Epoch 242/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2728 - acc: 0.9137 - val_loss: 0.3483 - val_acc: 0.8787\n",
      "Epoch 243/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2703 - acc: 0.9157 - val_loss: 0.3611 - val_acc: 0.8807\n",
      "Epoch 244/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2707 - acc: 0.9137 - val_loss: 0.3484 - val_acc: 0.8907\n",
      "Epoch 245/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2708 - acc: 0.9163 - val_loss: 0.3591 - val_acc: 0.8867\n",
      "Epoch 246/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.2689 - acc: 0.9143 - val_loss: 0.3473 - val_acc: 0.8827\n",
      "Epoch 247/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2701 - acc: 0.9130 - val_loss: 0.3437 - val_acc: 0.8907\n",
      "Epoch 248/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2685 - acc: 0.9137 - val_loss: 0.3512 - val_acc: 0.8887\n",
      "Epoch 249/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2706 - acc: 0.9137 - val_loss: 0.3554 - val_acc: 0.8847\n",
      "Epoch 250/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2691 - acc: 0.9157 - val_loss: 0.3428 - val_acc: 0.8867\n",
      "Epoch 251/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2689 - acc: 0.9190 - val_loss: 0.3461 - val_acc: 0.8867\n",
      "Epoch 252/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2688 - acc: 0.9150 - val_loss: 0.3882 - val_acc: 0.8748\n",
      "Epoch 253/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2695 - acc: 0.9117 - val_loss: 0.3422 - val_acc: 0.8867\n",
      "Epoch 254/3000\n",
      "1506/1506 [==============================] - 0s 82us/step - loss: 0.2688 - acc: 0.9150 - val_loss: 0.3477 - val_acc: 0.8807\n",
      "Epoch 255/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2676 - acc: 0.9157 - val_loss: 0.3400 - val_acc: 0.8827\n",
      "Epoch 256/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2676 - acc: 0.9150 - val_loss: 0.3440 - val_acc: 0.8867\n",
      "Epoch 257/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2686 - acc: 0.9150 - val_loss: 0.3438 - val_acc: 0.8827\n",
      "Epoch 258/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2683 - acc: 0.9157 - val_loss: 0.3378 - val_acc: 0.8926\n",
      "Epoch 259/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.2672 - acc: 0.9143 - val_loss: 0.3554 - val_acc: 0.8827\n",
      "Epoch 260/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2681 - acc: 0.9130 - val_loss: 0.3580 - val_acc: 0.8847\n",
      "Epoch 261/3000\n",
      "1506/1506 [==============================] - 0s 89us/step - loss: 0.2659 - acc: 0.9157 - val_loss: 0.3459 - val_acc: 0.8867\n",
      "Epoch 262/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2647 - acc: 0.9163 - val_loss: 0.3499 - val_acc: 0.8907\n",
      "Epoch 263/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2660 - acc: 0.9177 - val_loss: 0.3407 - val_acc: 0.8827\n",
      "Epoch 264/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2648 - acc: 0.9150 - val_loss: 0.3482 - val_acc: 0.8807\n",
      "Epoch 265/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2627 - acc: 0.9143 - val_loss: 0.3735 - val_acc: 0.8807\n",
      "Epoch 266/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2655 - acc: 0.9170 - val_loss: 0.3371 - val_acc: 0.8907\n",
      "Epoch 267/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2647 - acc: 0.9177 - val_loss: 0.3402 - val_acc: 0.8847\n",
      "Epoch 268/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2628 - acc: 0.9143 - val_loss: 0.3423 - val_acc: 0.8926\n",
      "Epoch 269/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2643 - acc: 0.9143 - val_loss: 0.3378 - val_acc: 0.8867\n",
      "Epoch 270/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2625 - acc: 0.9170 - val_loss: 0.3535 - val_acc: 0.8946\n",
      "Epoch 271/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2644 - acc: 0.9137 - val_loss: 0.3373 - val_acc: 0.8867\n",
      "Epoch 272/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2635 - acc: 0.9163 - val_loss: 0.3422 - val_acc: 0.8807\n",
      "Epoch 273/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2618 - acc: 0.9157 - val_loss: 0.3429 - val_acc: 0.8867\n",
      "Epoch 274/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2619 - acc: 0.9150 - val_loss: 0.3353 - val_acc: 0.8887\n",
      "Epoch 275/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2623 - acc: 0.9170 - val_loss: 0.3384 - val_acc: 0.8867\n",
      "Epoch 276/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2625 - acc: 0.9150 - val_loss: 0.3462 - val_acc: 0.9085\n",
      "Epoch 277/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2620 - acc: 0.9137 - val_loss: 0.3510 - val_acc: 0.8847\n",
      "Epoch 278/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2637 - acc: 0.9183 - val_loss: 0.3380 - val_acc: 0.8847\n",
      "Epoch 279/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2619 - acc: 0.9157 - val_loss: 0.3317 - val_acc: 0.8887\n",
      "Epoch 280/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2630 - acc: 0.9170 - val_loss: 0.3335 - val_acc: 0.8867\n",
      "Epoch 281/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2603 - acc: 0.9143 - val_loss: 0.3345 - val_acc: 0.8907\n",
      "Epoch 282/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2605 - acc: 0.9197 - val_loss: 0.3570 - val_acc: 0.8807\n",
      "Epoch 283/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.2614 - acc: 0.9163 - val_loss: 0.3333 - val_acc: 0.8867\n",
      "Epoch 284/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2602 - acc: 0.9150 - val_loss: 0.3341 - val_acc: 0.8867\n",
      "Epoch 285/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2611 - acc: 0.9130 - val_loss: 0.3629 - val_acc: 0.8767\n",
      "Epoch 286/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2600 - acc: 0.9170 - val_loss: 0.3534 - val_acc: 0.8827\n",
      "Epoch 287/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2591 - acc: 0.9170 - val_loss: 0.3285 - val_acc: 0.8986\n",
      "Epoch 288/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2598 - acc: 0.9190 - val_loss: 0.3327 - val_acc: 0.8867\n",
      "Epoch 289/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2597 - acc: 0.9177 - val_loss: 0.3422 - val_acc: 0.8827\n",
      "Epoch 290/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2600 - acc: 0.9143 - val_loss: 0.3436 - val_acc: 0.8847\n",
      "Epoch 291/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2603 - acc: 0.9143 - val_loss: 0.3259 - val_acc: 0.8887\n",
      "Epoch 292/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2581 - acc: 0.9190 - val_loss: 0.3325 - val_acc: 0.8847\n",
      "Epoch 293/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2579 - acc: 0.9183 - val_loss: 0.3434 - val_acc: 0.8847\n",
      "Epoch 294/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2575 - acc: 0.9183 - val_loss: 0.3370 - val_acc: 0.8847\n",
      "Epoch 295/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2560 - acc: 0.9150 - val_loss: 0.3319 - val_acc: 0.8867\n",
      "Epoch 296/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2585 - acc: 0.9170 - val_loss: 0.3326 - val_acc: 0.8966\n",
      "Epoch 297/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2559 - acc: 0.9163 - val_loss: 0.3442 - val_acc: 0.8946\n",
      "Epoch 298/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2566 - acc: 0.9170 - val_loss: 0.3297 - val_acc: 0.8946\n",
      "Epoch 299/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2538 - acc: 0.9170 - val_loss: 0.3371 - val_acc: 0.8827\n",
      "Epoch 300/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2567 - acc: 0.9157 - val_loss: 0.3272 - val_acc: 0.8926\n",
      "Epoch 301/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2540 - acc: 0.9183 - val_loss: 0.3647 - val_acc: 0.8708\n",
      "Epoch 302/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2555 - acc: 0.9197 - val_loss: 0.3348 - val_acc: 0.9046\n",
      "Epoch 303/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2552 - acc: 0.9170 - val_loss: 0.3272 - val_acc: 0.8867\n",
      "Epoch 304/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2536 - acc: 0.9177 - val_loss: 0.3204 - val_acc: 0.8966\n",
      "Epoch 305/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2539 - acc: 0.9183 - val_loss: 0.3229 - val_acc: 0.8907\n",
      "Epoch 306/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2565 - acc: 0.9190 - val_loss: 0.3265 - val_acc: 0.8966\n",
      "Epoch 307/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2539 - acc: 0.9183 - val_loss: 0.3387 - val_acc: 0.8887\n",
      "Epoch 308/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2558 - acc: 0.9190 - val_loss: 0.3205 - val_acc: 0.9006\n",
      "Epoch 309/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2551 - acc: 0.9163 - val_loss: 0.3239 - val_acc: 0.8946\n",
      "Epoch 310/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2534 - acc: 0.9190 - val_loss: 0.3259 - val_acc: 0.8946\n",
      "Epoch 311/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2530 - acc: 0.9203 - val_loss: 0.3210 - val_acc: 0.8926\n",
      "Epoch 312/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.2529 - acc: 0.9190 - val_loss: 0.3309 - val_acc: 0.8887\n",
      "Epoch 313/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2548 - acc: 0.9150 - val_loss: 0.3215 - val_acc: 0.8986\n",
      "Epoch 314/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2529 - acc: 0.9170 - val_loss: 0.3194 - val_acc: 0.8986\n",
      "Epoch 315/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2520 - acc: 0.9150 - val_loss: 0.3216 - val_acc: 0.8907\n",
      "Epoch 316/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2500 - acc: 0.9203 - val_loss: 0.3310 - val_acc: 0.8887\n",
      "Epoch 317/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2515 - acc: 0.9150 - val_loss: 0.3445 - val_acc: 0.8867\n",
      "Epoch 318/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2519 - acc: 0.9190 - val_loss: 0.3282 - val_acc: 0.8887\n",
      "Epoch 319/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2485 - acc: 0.9223 - val_loss: 0.3324 - val_acc: 0.8847\n",
      "Epoch 320/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2517 - acc: 0.9157 - val_loss: 0.3203 - val_acc: 0.8887\n",
      "Epoch 321/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2519 - acc: 0.9183 - val_loss: 0.3180 - val_acc: 0.8946\n",
      "Epoch 322/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2482 - acc: 0.9170 - val_loss: 0.3225 - val_acc: 0.8966\n",
      "Epoch 323/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2490 - acc: 0.9203 - val_loss: 0.3176 - val_acc: 0.8946\n",
      "Epoch 324/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2490 - acc: 0.9197 - val_loss: 0.3223 - val_acc: 0.8847\n",
      "Epoch 325/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2476 - acc: 0.9183 - val_loss: 0.3147 - val_acc: 0.9006\n",
      "Epoch 326/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2474 - acc: 0.9197 - val_loss: 0.3166 - val_acc: 0.9026\n",
      "Epoch 327/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2481 - acc: 0.9163 - val_loss: 0.3163 - val_acc: 0.8966\n",
      "Epoch 328/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2485 - acc: 0.9183 - val_loss: 0.3335 - val_acc: 0.8847\n",
      "Epoch 329/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2498 - acc: 0.9197 - val_loss: 0.3240 - val_acc: 0.8867\n",
      "Epoch 330/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2476 - acc: 0.9183 - val_loss: 0.3233 - val_acc: 0.8966\n",
      "Epoch 331/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2468 - acc: 0.9197 - val_loss: 0.3267 - val_acc: 0.8867\n",
      "Epoch 332/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2487 - acc: 0.9177 - val_loss: 0.3206 - val_acc: 0.8907\n",
      "Epoch 333/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2483 - acc: 0.9157 - val_loss: 0.3104 - val_acc: 0.8986\n",
      "Epoch 334/3000\n",
      "1506/1506 [==============================] - 0s 87us/step - loss: 0.2448 - acc: 0.9203 - val_loss: 0.3235 - val_acc: 0.8907\n",
      "Epoch 335/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2488 - acc: 0.9183 - val_loss: 0.3206 - val_acc: 0.8847\n",
      "Epoch 336/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2462 - acc: 0.9177 - val_loss: 0.3139 - val_acc: 0.8887\n",
      "Epoch 337/3000\n",
      "1506/1506 [==============================] - 0s 84us/step - loss: 0.2468 - acc: 0.9197 - val_loss: 0.3474 - val_acc: 0.8787\n",
      "Epoch 338/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2479 - acc: 0.9216 - val_loss: 0.3161 - val_acc: 0.8907\n",
      "Epoch 339/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2454 - acc: 0.9170 - val_loss: 0.3224 - val_acc: 0.8946\n",
      "Epoch 340/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2448 - acc: 0.9163 - val_loss: 0.3152 - val_acc: 0.9085\n",
      "Epoch 341/3000\n",
      "1506/1506 [==============================] - 0s 86us/step - loss: 0.2464 - acc: 0.9190 - val_loss: 0.3529 - val_acc: 0.8827\n",
      "Epoch 342/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2465 - acc: 0.9203 - val_loss: 0.3113 - val_acc: 0.9046\n",
      "Epoch 343/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2437 - acc: 0.9210 - val_loss: 0.3170 - val_acc: 0.9066\n",
      "Epoch 344/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2444 - acc: 0.9197 - val_loss: 0.3104 - val_acc: 0.8926\n",
      "Epoch 345/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2449 - acc: 0.9183 - val_loss: 0.3125 - val_acc: 0.8847\n",
      "Epoch 346/3000\n",
      "1506/1506 [==============================] - 0s 88us/step - loss: 0.2436 - acc: 0.9170 - val_loss: 0.3114 - val_acc: 0.8946\n",
      "Epoch 347/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2451 - acc: 0.9197 - val_loss: 0.3152 - val_acc: 0.8946\n",
      "Epoch 348/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2435 - acc: 0.9203 - val_loss: 0.3313 - val_acc: 0.8827\n",
      "Epoch 349/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2435 - acc: 0.9223 - val_loss: 0.3065 - val_acc: 0.8986\n",
      "Epoch 350/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2431 - acc: 0.9230 - val_loss: 0.3058 - val_acc: 0.9006\n",
      "Epoch 351/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2422 - acc: 0.9197 - val_loss: 0.3157 - val_acc: 0.8926\n",
      "Epoch 352/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2394 - acc: 0.9197 - val_loss: 0.3044 - val_acc: 0.9006\n",
      "Epoch 353/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2431 - acc: 0.9177 - val_loss: 0.3298 - val_acc: 0.8867\n",
      "Epoch 354/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2409 - acc: 0.9163 - val_loss: 0.3250 - val_acc: 0.8847\n",
      "Epoch 355/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2426 - acc: 0.9170 - val_loss: 0.3076 - val_acc: 0.8946\n",
      "Epoch 356/3000\n",
      "1506/1506 [==============================] - 0s 64us/step - loss: 0.2401 - acc: 0.9183 - val_loss: 0.3265 - val_acc: 0.8887\n",
      "Epoch 357/3000\n",
      "1506/1506 [==============================] - 0s 67us/step - loss: 0.2400 - acc: 0.9216 - val_loss: 0.3088 - val_acc: 0.8926\n",
      "Epoch 358/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2394 - acc: 0.9197 - val_loss: 0.3086 - val_acc: 0.9006\n",
      "Epoch 359/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2393 - acc: 0.9250 - val_loss: 0.3138 - val_acc: 0.8926\n",
      "Epoch 360/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2392 - acc: 0.9197 - val_loss: 0.3080 - val_acc: 0.8907\n",
      "Epoch 361/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2362 - acc: 0.9230 - val_loss: 0.4216 - val_acc: 0.8887\n",
      "Epoch 362/3000\n",
      "1506/1506 [==============================] - 0s 81us/step - loss: 0.2425 - acc: 0.9210 - val_loss: 0.3056 - val_acc: 0.8926\n",
      "Epoch 363/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2365 - acc: 0.9210 - val_loss: 0.3433 - val_acc: 0.9324\n",
      "Epoch 364/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2386 - acc: 0.9256 - val_loss: 0.3088 - val_acc: 0.8867\n",
      "Epoch 365/3000\n",
      "1506/1506 [==============================] - 0s 69us/step - loss: 0.2349 - acc: 0.9230 - val_loss: 0.3133 - val_acc: 0.9105\n",
      "Epoch 366/3000\n",
      "1506/1506 [==============================] - 0s 80us/step - loss: 0.2389 - acc: 0.9210 - val_loss: 0.3008 - val_acc: 0.9026\n",
      "Epoch 367/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2347 - acc: 0.9223 - val_loss: 0.3208 - val_acc: 0.8847\n",
      "Epoch 368/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2352 - acc: 0.9216 - val_loss: 0.3027 - val_acc: 0.8946\n",
      "Epoch 369/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2354 - acc: 0.9203 - val_loss: 0.3088 - val_acc: 0.8966\n",
      "Epoch 370/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2356 - acc: 0.9210 - val_loss: 0.3073 - val_acc: 0.8946\n",
      "Epoch 371/3000\n",
      "1506/1506 [==============================] - 0s 87us/step - loss: 0.2347 - acc: 0.9223 - val_loss: 0.3084 - val_acc: 0.8946\n",
      "Epoch 372/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2353 - acc: 0.9203 - val_loss: 0.3486 - val_acc: 0.8926\n",
      "Epoch 373/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2375 - acc: 0.9210 - val_loss: 0.3131 - val_acc: 0.8887\n",
      "Epoch 374/3000\n",
      "1506/1506 [==============================] - 0s 68us/step - loss: 0.2330 - acc: 0.9210 - val_loss: 0.3031 - val_acc: 0.9026\n",
      "Epoch 375/3000\n",
      "1506/1506 [==============================] - 0s 70us/step - loss: 0.2352 - acc: 0.9210 - val_loss: 0.3007 - val_acc: 0.9105\n",
      "Epoch 376/3000\n",
      "1506/1506 [==============================] - 0s 83us/step - loss: 0.2319 - acc: 0.9243 - val_loss: 0.3337 - val_acc: 0.9264\n",
      "Epoch 377/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2341 - acc: 0.9223 - val_loss: 0.3245 - val_acc: 0.8867\n",
      "Epoch 378/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2343 - acc: 0.9223 - val_loss: 0.3022 - val_acc: 0.8946\n",
      "Epoch 379/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2315 - acc: 0.9223 - val_loss: 0.3045 - val_acc: 0.9085\n",
      "Epoch 380/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.2318 - acc: 0.9203 - val_loss: 0.3279 - val_acc: 0.9006\n",
      "Epoch 381/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2331 - acc: 0.9216 - val_loss: 0.3008 - val_acc: 0.9006\n",
      "Epoch 382/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2330 - acc: 0.9177 - val_loss: 0.2964 - val_acc: 0.9006\n",
      "Epoch 383/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2320 - acc: 0.9216 - val_loss: 0.2965 - val_acc: 0.9105\n",
      "Epoch 384/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2306 - acc: 0.9230 - val_loss: 0.3069 - val_acc: 0.8926\n",
      "Epoch 385/3000\n",
      "1506/1506 [==============================] - 0s 73us/step - loss: 0.2306 - acc: 0.9210 - val_loss: 0.3318 - val_acc: 0.8907\n",
      "Epoch 386/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2306 - acc: 0.9197 - val_loss: 0.2901 - val_acc: 0.9026\n",
      "Epoch 387/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.2297 - acc: 0.9216 - val_loss: 0.3019 - val_acc: 0.8966\n",
      "Epoch 388/3000\n",
      "1506/1506 [==============================] - 0s 71us/step - loss: 0.2281 - acc: 0.9216 - val_loss: 0.3188 - val_acc: 0.9165\n",
      "Epoch 389/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2306 - acc: 0.9210 - val_loss: 0.2902 - val_acc: 0.9125\n",
      "Epoch 390/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2301 - acc: 0.9236 - val_loss: 0.3152 - val_acc: 0.8887\n",
      "Epoch 391/3000\n",
      "1506/1506 [==============================] - 0s 72us/step - loss: 0.2291 - acc: 0.9223 - val_loss: 0.2999 - val_acc: 0.9066\n",
      "Epoch 392/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2284 - acc: 0.9230 - val_loss: 0.2987 - val_acc: 0.8907\n",
      "Epoch 393/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2298 - acc: 0.9236 - val_loss: 0.3009 - val_acc: 0.8926\n",
      "Epoch 394/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2293 - acc: 0.9236 - val_loss: 0.2984 - val_acc: 0.9066\n",
      "Epoch 395/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2266 - acc: 0.9216 - val_loss: 0.3090 - val_acc: 0.8907\n",
      "Epoch 396/3000\n",
      "1506/1506 [==============================] - 0s 74us/step - loss: 0.2271 - acc: 0.9216 - val_loss: 0.2964 - val_acc: 0.9205\n",
      "Epoch 397/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2276 - acc: 0.9236 - val_loss: 0.2952 - val_acc: 0.9105\n",
      "Epoch 398/3000\n",
      "1506/1506 [==============================] - 0s 78us/step - loss: 0.2273 - acc: 0.9230 - val_loss: 0.2996 - val_acc: 0.9026\n",
      "Epoch 399/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2245 - acc: 0.9256 - val_loss: 0.2947 - val_acc: 0.9085\n",
      "Epoch 400/3000\n",
      "1506/1506 [==============================] - 0s 77us/step - loss: 0.2286 - acc: 0.9243 - val_loss: 0.2950 - val_acc: 0.9046\n",
      "Epoch 401/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2252 - acc: 0.9223 - val_loss: 0.3068 - val_acc: 0.8926\n",
      "Epoch 402/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2262 - acc: 0.9243 - val_loss: 0.3107 - val_acc: 0.9185\n",
      "Epoch 403/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2286 - acc: 0.9230 - val_loss: 0.3103 - val_acc: 0.8867\n",
      "Epoch 404/3000\n",
      "1506/1506 [==============================] - 0s 76us/step - loss: 0.2271 - acc: 0.9216 - val_loss: 0.3298 - val_acc: 0.9225\n",
      "Epoch 405/3000\n",
      "1506/1506 [==============================] - 0s 79us/step - loss: 0.2252 - acc: 0.9236 - val_loss: 0.3027 - val_acc: 0.8887\n",
      "Epoch 406/3000\n",
      "1506/1506 [==============================] - 0s 75us/step - loss: 0.2269 - acc: 0.9230 - val_loss: 0.2948 - val_acc: 0.8966\n",
      "Epoch 00406: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(10))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adad,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist6=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2634fb36cc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmCkYhmEYfcwUDMMwjD5mCoZhGEaf/wM2YQxOc7KYBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26349df9dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist3.history['loss'])\n",
    "plt.plot(hist4.history['loss'])\n",
    "plt.plot(hist5.history['loss'])\n",
    "plt.plot(hist6.history['loss'])\n",
    "plt.savefig('loss with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('loss with diff. optimizers.eps', format='eps', dpi=1000)\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMaEMhkKhUCjGxP8H7uBkLewgxlUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634b8b5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "#plt.plot(hist1.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "#plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist2.history['val_loss'])\n",
    "#plt.plot(hist3.history['loss'])\n",
    "plt.plot(hist3.history['val_loss'])\n",
    "#plt.plot(hist4.history['loss'])\n",
    "plt.plot(hist4.history['val_loss'])\n",
    "#plt.plot(hist5.history['loss'])\n",
    "plt.plot(hist5.history['val_loss'])\n",
    "#plt.plot(hist6.history['loss'])\n",
    "plt.plot(hist6.history['val_loss'])\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])\n",
    "plt.savefig('Validation loss with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('Validation loss with diff. optimizers.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h+sp8CsaoDlMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634fb1deb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.plot(hist1.history['acc'])\n",
    "#plt.plot(hist1.history['val_loss'])\n",
    "plt.plot(hist2.history['acc'])\n",
    "#plt.plot(hist2.history['val_loss'])\n",
    "plt.plot(hist3.history['acc'])\n",
    "#plt.plot(hist3.history['val_loss'])\n",
    "plt.plot(hist4.history['acc'])\n",
    "#plt.plot(hist4.history['val_loss'])\n",
    "plt.plot(hist5.history['acc'])\n",
    "#plt.plot(hist5.history['val_loss'])\n",
    "plt.plot(hist6.history['acc'])\n",
    "#plt.plot(hist6.history['val_loss'])\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])\n",
    "plt.savefig('training accuracy with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('training accuracy with diff. optimizers.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlhSMMcaEWVIwxhgTZknBGGNM2P8DMlBJA4bMy08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2634fb1de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.plot(hist1.history['val_acc'])\n",
    "plt.plot(hist2.history['val_acc'])\n",
    "plt.plot(hist3.history['val_acc'])\n",
    "plt.plot(hist4.history['val_acc'])\n",
    "plt.plot(hist5.history['val_acc'])\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])\n",
    "plt.savefig('validation accuracy with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('validation accuracy with diff. optimizers.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
