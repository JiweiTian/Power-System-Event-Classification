{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"datacls7.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D1</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Kurtotis1</th>\n",
       "      <th>Skew1</th>\n",
       "      <th>Entrophy1</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.336000e+03</td>\n",
       "      <td>1.336000e+03</td>\n",
       "      <td>1.336000e+03</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>1336.000000</td>\n",
       "      <td>1336.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.165834e+04</td>\n",
       "      <td>3.731561e+06</td>\n",
       "      <td>2.762408e+14</td>\n",
       "      <td>201.496461</td>\n",
       "      <td>363.398658</td>\n",
       "      <td>37.829025</td>\n",
       "      <td>2.215569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.380903e+05</td>\n",
       "      <td>2.683563e+07</td>\n",
       "      <td>3.240747e+15</td>\n",
       "      <td>79.316865</td>\n",
       "      <td>144.894480</td>\n",
       "      <td>20.385024</td>\n",
       "      <td>2.138626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.713131e-02</td>\n",
       "      <td>8.951112e+00</td>\n",
       "      <td>4.006120e+01</td>\n",
       "      <td>53.852610</td>\n",
       "      <td>142.304766</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.144032e+00</td>\n",
       "      <td>1.044930e+03</td>\n",
       "      <td>5.972116e+05</td>\n",
       "      <td>146.570951</td>\n",
       "      <td>234.887690</td>\n",
       "      <td>19.226221</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.608493e+02</td>\n",
       "      <td>3.341341e+04</td>\n",
       "      <td>7.993970e+08</td>\n",
       "      <td>186.332594</td>\n",
       "      <td>344.047014</td>\n",
       "      <td>34.800662</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.168356e+03</td>\n",
       "      <td>4.001864e+05</td>\n",
       "      <td>8.559106e+10</td>\n",
       "      <td>268.607276</td>\n",
       "      <td>484.622758</td>\n",
       "      <td>56.747413</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.635434e+06</td>\n",
       "      <td>4.608146e+08</td>\n",
       "      <td>7.060000e+16</td>\n",
       "      <td>550.672507</td>\n",
       "      <td>793.945796</td>\n",
       "      <td>97.910728</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mean          S.D1      Variance    Kurtotis1        Skew1  \\\n",
       "count  1.336000e+03  1.336000e+03  1.336000e+03  1336.000000  1336.000000   \n",
       "mean   3.165834e+04  3.731561e+06  2.762408e+14   201.496461   363.398658   \n",
       "std    2.380903e+05  2.683563e+07  3.240747e+15    79.316865   144.894480   \n",
       "min    5.713131e-02  8.951112e+00  4.006120e+01    53.852610   142.304766   \n",
       "25%    7.144032e+00  1.044930e+03  5.972116e+05   146.570951   234.887690   \n",
       "50%    2.608493e+02  3.341341e+04  7.993970e+08   186.332594   344.047014   \n",
       "75%    3.168356e+03  4.001864e+05  8.559106e+10   268.607276   484.622758   \n",
       "max    4.635434e+06  4.608146e+08  7.060000e+16   550.672507   793.945796   \n",
       "\n",
       "         Entrophy1        Class  \n",
       "count  1336.000000  1336.000000  \n",
       "mean     37.829025     2.215569  \n",
       "std      20.385024     2.138626  \n",
       "min       0.069054     0.000000  \n",
       "25%      19.226221     0.000000  \n",
       "50%      34.800662     1.000000  \n",
       "75%      56.747413     4.000000  \n",
       "max      97.910728     6.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=data.pop(\"Class\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2d = tsne.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE/CAYAAABFK3gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169a097e8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_ids = range(len(target_names))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k'\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=label)\n",
    "plt.legend( loc='lower left')\n",
    "plt.savefig('t-SNE.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('t-SNE.eps', format='eps', dpi=1000)\n",
    "plt.title('t-SNE plot for the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from numpy import nan\n",
    "\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, random_state = 42)\n",
    "nb_classes = 7\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "334\n",
      "6 dims\n",
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'dims')\n",
    "print(\"Building model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_train = X_train.min(axis=0)\n",
    "range_train = (X_train - min_train).max(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - min_train)/range_train\n",
    "\n",
    "#print('Minimum per feature\\n{}'.format(X_train_scaled.min(axis=0)))\n",
    "#print('Maximum per feature\\n{}'.format(X_train_scaled.max(axis=0)))\n",
    "\n",
    "X_test_scaled = (X_test - min_train)/range_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE/CAYAAABFK3gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWV06q3AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1699d3d1a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_2d = tsne.fit_transform(X_train_scaled)\n",
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "target_ids = range(len(target_names))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k'\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(X_2d[y_train == i, 0], X_2d[y_train == i, 1], c=c, label=label)\n",
    "plt.legend( loc='lower left')\n",
    "plt.savefig('t-SNE_for_scaled.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('t-SNE_for_scaled.eps', format='eps', dpi=1000)\n",
    "plt.title('t-SNE plot for the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAE/CAYAAABFK3gIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAz4RkUv8f3xIMnaGNjLYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169a47d7f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_2d = tsne.fit_transform(X_test_scaled)\n",
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "target_ids = range(len(target_names))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k'\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(X_2d[y_test == i, 0], X_2d[y_test == i, 1], c=c, label=label)\n",
    "plt.legend( loc='lower left')\n",
    "plt.savefig('t-SNE_for_scaled_testdata.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('t-SNE_for_scaled_testdata.eps', format='eps', dpi=1000)\n",
    "plt.title('t-SNE plot for the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/3000\n",
      "1002/1002 [==============================] - 2s 2ms/step - loss: 1.8793 - acc: 0.2864 - val_loss: 1.8197 - val_acc: 0.3054\n",
      "Epoch 2/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 1.7516 - acc: 0.3074 - val_loss: 1.6803 - val_acc: 0.3054\n",
      "Epoch 3/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 1.6052 - acc: 0.2994 - val_loss: 1.5287 - val_acc: 0.3263\n",
      "Epoch 4/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 1.4514 - acc: 0.4351 - val_loss: 1.3771 - val_acc: 0.5449\n",
      "Epoch 5/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 1.2959 - acc: 0.5419 - val_loss: 1.2230 - val_acc: 0.5659\n",
      "Epoch 6/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 1.1449 - acc: 0.5778 - val_loss: 1.0809 - val_acc: 0.5808\n",
      "Epoch 7/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 1.0157 - acc: 0.6008 - val_loss: 0.9652 - val_acc: 0.5958\n",
      "Epoch 8/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.9027 - acc: 0.6168 - val_loss: 0.8600 - val_acc: 0.6168\n",
      "Epoch 9/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.7999 - acc: 0.6307 - val_loss: 0.7578 - val_acc: 0.6677\n",
      "Epoch 10/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.7050 - acc: 0.6816 - val_loss: 0.6697 - val_acc: 0.7605\n",
      "Epoch 11/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.6225 - acc: 0.8184 - val_loss: 0.5929 - val_acc: 0.8982\n",
      "Epoch 12/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.5509 - acc: 0.8982 - val_loss: 0.5281 - val_acc: 0.9281\n",
      "Epoch 13/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.4907 - acc: 0.9192 - val_loss: 0.4706 - val_acc: 0.9491\n",
      "Epoch 14/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.4351 - acc: 0.9331 - val_loss: 0.4196 - val_acc: 0.9551\n",
      "Epoch 15/3000\n",
      "1002/1002 [==============================] - 0s 112us/step - loss: 0.3879 - acc: 0.9591 - val_loss: 0.3760 - val_acc: 0.9521\n",
      "Epoch 16/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.3486 - acc: 0.9591 - val_loss: 0.3381 - val_acc: 0.9671\n",
      "Epoch 17/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.3153 - acc: 0.9661 - val_loss: 0.3088 - val_acc: 0.9671\n",
      "Epoch 18/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.2867 - acc: 0.9611 - val_loss: 0.2825 - val_acc: 0.9701\n",
      "Epoch 19/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.2644 - acc: 0.9641 - val_loss: 0.2635 - val_acc: 0.9581\n",
      "Epoch 20/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.2437 - acc: 0.9721 - val_loss: 0.2419 - val_acc: 0.9701\n",
      "Epoch 21/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.2260 - acc: 0.9711 - val_loss: 0.2279 - val_acc: 0.9581\n",
      "Epoch 22/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.2118 - acc: 0.9701 - val_loss: 0.2134 - val_acc: 0.9611\n",
      "Epoch 23/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1994 - acc: 0.9701 - val_loss: 0.2035 - val_acc: 0.9581\n",
      "Epoch 24/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1899 - acc: 0.9671 - val_loss: 0.1905 - val_acc: 0.9731\n",
      "Epoch 25/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1778 - acc: 0.9711 - val_loss: 0.1838 - val_acc: 0.9731\n",
      "Epoch 26/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.1687 - acc: 0.9741 - val_loss: 0.1730 - val_acc: 0.9671\n",
      "Epoch 27/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.1612 - acc: 0.9731 - val_loss: 0.1679 - val_acc: 0.9701\n",
      "Epoch 28/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.1576 - acc: 0.9750 - val_loss: 0.1629 - val_acc: 0.9731\n",
      "Epoch 29/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.1480 - acc: 0.9750 - val_loss: 0.1531 - val_acc: 0.9641\n",
      "Epoch 30/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1409 - acc: 0.9741 - val_loss: 0.1494 - val_acc: 0.9731\n",
      "Epoch 31/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.1371 - acc: 0.9770 - val_loss: 0.1444 - val_acc: 0.9671\n",
      "Epoch 32/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.1325 - acc: 0.9750 - val_loss: 0.1402 - val_acc: 0.9731\n",
      "Epoch 33/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.1308 - acc: 0.9721 - val_loss: 0.1374 - val_acc: 0.9671\n",
      "Epoch 34/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.1262 - acc: 0.9770 - val_loss: 0.1351 - val_acc: 0.9611\n",
      "Epoch 35/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1223 - acc: 0.9721 - val_loss: 0.1296 - val_acc: 0.9641\n",
      "Epoch 36/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.1177 - acc: 0.9750 - val_loss: 0.1248 - val_acc: 0.9671\n",
      "Epoch 37/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.1150 - acc: 0.9741 - val_loss: 0.1266 - val_acc: 0.9611\n",
      "Epoch 38/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1122 - acc: 0.9731 - val_loss: 0.1194 - val_acc: 0.9671\n",
      "Epoch 39/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.1076 - acc: 0.9741 - val_loss: 0.1182 - val_acc: 0.9641\n",
      "Epoch 40/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1076 - acc: 0.9741 - val_loss: 0.1159 - val_acc: 0.9671\n",
      "Epoch 41/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1037 - acc: 0.9770 - val_loss: 0.1129 - val_acc: 0.9671\n",
      "Epoch 42/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1012 - acc: 0.9770 - val_loss: 0.1113 - val_acc: 0.9611\n",
      "Epoch 43/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.1001 - acc: 0.9760 - val_loss: 0.1074 - val_acc: 0.9701\n",
      "Epoch 44/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0982 - acc: 0.9731 - val_loss: 0.1127 - val_acc: 0.9671\n",
      "Epoch 45/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0963 - acc: 0.9750 - val_loss: 0.1051 - val_acc: 0.9641\n",
      "Epoch 46/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0947 - acc: 0.9770 - val_loss: 0.1055 - val_acc: 0.9671\n",
      "Epoch 47/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0929 - acc: 0.9731 - val_loss: 0.0998 - val_acc: 0.9701\n",
      "Epoch 48/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0895 - acc: 0.9750 - val_loss: 0.1010 - val_acc: 0.9611\n",
      "Epoch 49/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0883 - acc: 0.9770 - val_loss: 0.1040 - val_acc: 0.9671\n",
      "Epoch 50/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0919 - acc: 0.9760 - val_loss: 0.1041 - val_acc: 0.9611\n",
      "Epoch 51/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0897 - acc: 0.9790 - val_loss: 0.1016 - val_acc: 0.9671\n",
      "Epoch 52/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0855 - acc: 0.9780 - val_loss: 0.0972 - val_acc: 0.9641\n",
      "Epoch 53/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0841 - acc: 0.9770 - val_loss: 0.0941 - val_acc: 0.9671\n",
      "Epoch 54/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0809 - acc: 0.9790 - val_loss: 0.0934 - val_acc: 0.9671\n",
      "Epoch 55/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0809 - acc: 0.9800 - val_loss: 0.0985 - val_acc: 0.9671\n",
      "Epoch 56/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0793 - acc: 0.9780 - val_loss: 0.0916 - val_acc: 0.9671\n",
      "Epoch 57/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0781 - acc: 0.9780 - val_loss: 0.0942 - val_acc: 0.9611\n",
      "Epoch 58/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0782 - acc: 0.9790 - val_loss: 0.0908 - val_acc: 0.9671\n",
      "Epoch 59/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0771 - acc: 0.9770 - val_loss: 0.0918 - val_acc: 0.9701\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0764 - acc: 0.9780 - val_loss: 0.0892 - val_acc: 0.9701\n",
      "Epoch 61/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0757 - acc: 0.9790 - val_loss: 0.0880 - val_acc: 0.9731\n",
      "Epoch 62/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0752 - acc: 0.9790 - val_loss: 0.0906 - val_acc: 0.9641\n",
      "Epoch 63/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0737 - acc: 0.9770 - val_loss: 0.0914 - val_acc: 0.9701\n",
      "Epoch 64/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0718 - acc: 0.9800 - val_loss: 0.0898 - val_acc: 0.9641\n",
      "Epoch 65/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0726 - acc: 0.9770 - val_loss: 0.0866 - val_acc: 0.9701\n",
      "Epoch 66/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0716 - acc: 0.9800 - val_loss: 0.0907 - val_acc: 0.9641\n",
      "Epoch 67/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0704 - acc: 0.9800 - val_loss: 0.0871 - val_acc: 0.9641\n",
      "Epoch 68/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0693 - acc: 0.9780 - val_loss: 0.0869 - val_acc: 0.9671\n",
      "Epoch 69/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0701 - acc: 0.9790 - val_loss: 0.0849 - val_acc: 0.9641\n",
      "Epoch 70/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0704 - acc: 0.9741 - val_loss: 0.0901 - val_acc: 0.9671\n",
      "Epoch 71/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0670 - acc: 0.9800 - val_loss: 0.0863 - val_acc: 0.9641\n",
      "Epoch 72/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0690 - acc: 0.9780 - val_loss: 0.0915 - val_acc: 0.9701\n",
      "Epoch 73/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0682 - acc: 0.9790 - val_loss: 0.0839 - val_acc: 0.9671\n",
      "Epoch 74/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0663 - acc: 0.9800 - val_loss: 0.0875 - val_acc: 0.9671\n",
      "Epoch 75/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0676 - acc: 0.9800 - val_loss: 0.0880 - val_acc: 0.9731\n",
      "Epoch 76/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0676 - acc: 0.9760 - val_loss: 0.0863 - val_acc: 0.9671\n",
      "Epoch 77/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0651 - acc: 0.9830 - val_loss: 0.0850 - val_acc: 0.9701\n",
      "Epoch 78/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0655 - acc: 0.9820 - val_loss: 0.0846 - val_acc: 0.9701\n",
      "Epoch 79/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0845 - val_acc: 0.9671\n",
      "Epoch 80/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0634 - acc: 0.9800 - val_loss: 0.0857 - val_acc: 0.9671\n",
      "Epoch 81/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0632 - acc: 0.9800 - val_loss: 0.0827 - val_acc: 0.9731\n",
      "Epoch 82/3000\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0641 - acc: 0.9790 - val_loss: 0.0862 - val_acc: 0.9671\n",
      "Epoch 83/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0630 - acc: 0.9800 - val_loss: 0.0840 - val_acc: 0.9731\n",
      "Epoch 84/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0632 - acc: 0.9800 - val_loss: 0.0854 - val_acc: 0.9701\n",
      "Epoch 85/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0626 - acc: 0.9800 - val_loss: 0.0844 - val_acc: 0.9671\n",
      "Epoch 86/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0611 - acc: 0.9790 - val_loss: 0.0836 - val_acc: 0.9701\n",
      "Epoch 87/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0634 - acc: 0.9790 - val_loss: 0.0861 - val_acc: 0.9701\n",
      "Epoch 88/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0601 - acc: 0.9810 - val_loss: 0.0859 - val_acc: 0.9671\n",
      "Epoch 89/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.0832 - val_acc: 0.9731\n",
      "Epoch 90/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0624 - acc: 0.9770 - val_loss: 0.0838 - val_acc: 0.9731\n",
      "Epoch 91/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0605 - acc: 0.9800 - val_loss: 0.0819 - val_acc: 0.9731\n",
      "Epoch 92/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0597 - acc: 0.9820 - val_loss: 0.0817 - val_acc: 0.9731\n",
      "Epoch 93/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0597 - acc: 0.9820 - val_loss: 0.0827 - val_acc: 0.9671\n",
      "Epoch 94/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0589 - acc: 0.9790 - val_loss: 0.0818 - val_acc: 0.9731\n",
      "Epoch 95/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0581 - acc: 0.9810 - val_loss: 0.0824 - val_acc: 0.9701\n",
      "Epoch 96/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0580 - acc: 0.9810 - val_loss: 0.0819 - val_acc: 0.9731\n",
      "Epoch 97/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.0759 - acc: 0.968 - 0s 61us/step - loss: 0.0569 - acc: 0.9830 - val_loss: 0.0806 - val_acc: 0.9760\n",
      "Epoch 98/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0590 - acc: 0.9810 - val_loss: 0.0871 - val_acc: 0.9731\n",
      "Epoch 99/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.0582 - acc: 0.9810 - val_loss: 0.0823 - val_acc: 0.9701\n",
      "Epoch 100/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0560 - acc: 0.9800 - val_loss: 0.0847 - val_acc: 0.9731\n",
      "Epoch 101/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0573 - acc: 0.9810 - val_loss: 0.0867 - val_acc: 0.9731\n",
      "Epoch 102/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0549 - acc: 0.9840 - val_loss: 0.0823 - val_acc: 0.9731\n",
      "Epoch 103/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0550 - acc: 0.9820 - val_loss: 0.0799 - val_acc: 0.9731\n",
      "Epoch 104/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0547 - acc: 0.9850 - val_loss: 0.0838 - val_acc: 0.9731\n",
      "Epoch 105/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0595 - acc: 0.9790 - val_loss: 0.0845 - val_acc: 0.9671\n",
      "Epoch 106/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0559 - acc: 0.9850 - val_loss: 0.0808 - val_acc: 0.9731\n",
      "Epoch 107/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0863 - val_acc: 0.9731\n",
      "Epoch 108/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0537 - acc: 0.9810 - val_loss: 0.0764 - val_acc: 0.9760\n",
      "Epoch 109/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0546 - acc: 0.9840 - val_loss: 0.0809 - val_acc: 0.9760\n",
      "Epoch 110/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0535 - acc: 0.9840 - val_loss: 0.0794 - val_acc: 0.9760\n",
      "Epoch 111/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0544 - acc: 0.9820 - val_loss: 0.0776 - val_acc: 0.9760\n",
      "Epoch 112/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0512 - acc: 0.9830 - val_loss: 0.0780 - val_acc: 0.9731\n",
      "Epoch 113/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0530 - acc: 0.9840 - val_loss: 0.0745 - val_acc: 0.9790\n",
      "Epoch 114/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0513 - acc: 0.9830 - val_loss: 0.0785 - val_acc: 0.9731\n",
      "Epoch 115/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0513 - acc: 0.9850 - val_loss: 0.0759 - val_acc: 0.9731\n",
      "Epoch 116/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0513 - acc: 0.9840 - val_loss: 0.0784 - val_acc: 0.9790\n",
      "Epoch 117/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0508 - acc: 0.9820 - val_loss: 0.0768 - val_acc: 0.9731\n",
      "Epoch 118/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0494 - acc: 0.9830 - val_loss: 0.0770 - val_acc: 0.9760\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0503 - acc: 0.9860 - val_loss: 0.0795 - val_acc: 0.9760\n",
      "Epoch 120/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0503 - acc: 0.9860 - val_loss: 0.0753 - val_acc: 0.9760\n",
      "Epoch 121/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0505 - acc: 0.9830 - val_loss: 0.0760 - val_acc: 0.9731\n",
      "Epoch 122/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0511 - acc: 0.9830 - val_loss: 0.0770 - val_acc: 0.9760\n",
      "Epoch 123/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0491 - acc: 0.9830 - val_loss: 0.0757 - val_acc: 0.9731\n",
      "Epoch 124/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0483 - acc: 0.9840 - val_loss: 0.0739 - val_acc: 0.9731\n",
      "Epoch 125/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0494 - acc: 0.9870 - val_loss: 0.0754 - val_acc: 0.9731\n",
      "Epoch 126/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0479 - acc: 0.9860 - val_loss: 0.0755 - val_acc: 0.9731\n",
      "Epoch 127/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0485 - acc: 0.9840 - val_loss: 0.0773 - val_acc: 0.9731\n",
      "Epoch 128/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0473 - acc: 0.9870 - val_loss: 0.0741 - val_acc: 0.9760\n",
      "Epoch 129/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0488 - acc: 0.9840 - val_loss: 0.0754 - val_acc: 0.9760\n",
      "Epoch 130/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0484 - acc: 0.9850 - val_loss: 0.0778 - val_acc: 0.9731\n",
      "Epoch 131/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0481 - acc: 0.9870 - val_loss: 0.0739 - val_acc: 0.9731\n",
      "Epoch 132/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0457 - acc: 0.9850 - val_loss: 0.0718 - val_acc: 0.9790\n",
      "Epoch 133/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0499 - acc: 0.9840 - val_loss: 0.0732 - val_acc: 0.9760\n",
      "Epoch 134/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0464 - acc: 0.9850 - val_loss: 0.0738 - val_acc: 0.9731\n",
      "Epoch 135/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0466 - acc: 0.9840 - val_loss: 0.0714 - val_acc: 0.9790\n",
      "Epoch 136/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0461 - acc: 0.9840 - val_loss: 0.0706 - val_acc: 0.9790\n",
      "Epoch 137/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0757 - val_acc: 0.9760\n",
      "Epoch 138/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0473 - acc: 0.9890 - val_loss: 0.0744 - val_acc: 0.9760\n",
      "Epoch 139/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0455 - acc: 0.9890 - val_loss: 0.0745 - val_acc: 0.9760\n",
      "Epoch 140/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0455 - acc: 0.9880 - val_loss: 0.0714 - val_acc: 0.9790\n",
      "Epoch 141/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0460 - acc: 0.9890 - val_loss: 0.0760 - val_acc: 0.9760\n",
      "Epoch 142/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0439 - acc: 0.9870 - val_loss: 0.0735 - val_acc: 0.9731\n",
      "Epoch 143/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0445 - acc: 0.9880 - val_loss: 0.0781 - val_acc: 0.9760\n",
      "Epoch 144/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0446 - acc: 0.9880 - val_loss: 0.0722 - val_acc: 0.9790\n",
      "Epoch 145/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0437 - acc: 0.9870 - val_loss: 0.0809 - val_acc: 0.9760\n",
      "Epoch 146/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0430 - acc: 0.9870 - val_loss: 0.0735 - val_acc: 0.9790\n",
      "Epoch 147/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0467 - acc: 0.9860 - val_loss: 0.0716 - val_acc: 0.9760\n",
      "Epoch 148/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0442 - acc: 0.9840 - val_loss: 0.0811 - val_acc: 0.9731\n",
      "Epoch 149/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0448 - acc: 0.9900 - val_loss: 0.0828 - val_acc: 0.9731\n",
      "Epoch 150/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0431 - acc: 0.9880 - val_loss: 0.0756 - val_acc: 0.9760\n",
      "Epoch 151/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0430 - acc: 0.9900 - val_loss: 0.0778 - val_acc: 0.9760\n",
      "Epoch 152/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0448 - acc: 0.9870 - val_loss: 0.0734 - val_acc: 0.9790\n",
      "Epoch 153/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0422 - acc: 0.9880 - val_loss: 0.0787 - val_acc: 0.9731\n",
      "Epoch 154/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0424 - acc: 0.9870 - val_loss: 0.0772 - val_acc: 0.9731\n",
      "Epoch 155/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0428 - acc: 0.9880 - val_loss: 0.0836 - val_acc: 0.9731\n",
      "Epoch 156/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0415 - acc: 0.9910 - val_loss: 0.0736 - val_acc: 0.9790\n",
      "Epoch 00156: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adam,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist1=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 39)                273       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 39)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1000      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 182       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,455\n",
      "Trainable params: 1,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x169a6010e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169a47d7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhjTxpKCMcaYNv8fYRKUNfrvIuwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169a5fcc240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist1.history['loss'])\n",
    "plt.plot(hist1.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'], prop={'size': 15})\n",
    "#plt.savefig('loss with adam.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('loss with adam.eps', format='eps', dpi=1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (p.u)')\n",
    "plt.plot(hist1.history['acc'])\n",
    "plt.plot(hist1.history['val_acc'])\n",
    "#plt.savefig('accuracy with adam.fig', format='eps', dpi=1000)\n",
    "#plt.savefig('accuracy with adam.eps', format='eps', dpi=1000)\n",
    "plt.legend(['Training', 'Validation'], loc='lower right', prop={'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/1500\n",
      "1002/1002 [==============================] - 1s 821us/step - loss: 1.9257 - acc: 0.2924 - val_loss: 1.9059 - val_acc: 0.2994\n",
      "Epoch 2/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 1.8897 - acc: 0.2924 - val_loss: 1.8665 - val_acc: 0.2994\n",
      "Epoch 3/1500\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 1.8554 - acc: 0.2924 - val_loss: 1.8309 - val_acc: 0.2994\n",
      "Epoch 4/1500\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 1.8293 - acc: 0.2924 - val_loss: 1.8023 - val_acc: 0.2994\n",
      "Epoch 5/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 1.8097 - acc: 0.2924 - val_loss: 1.7853 - val_acc: 0.2994\n",
      "Epoch 6/1500\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 1.7990 - acc: 0.2924 - val_loss: 1.7731 - val_acc: 0.2994\n",
      "Epoch 7/1500\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 1.7891 - acc: 0.298 - 0s 75us/step - loss: 1.7907 - acc: 0.2924 - val_loss: 1.7645 - val_acc: 0.2994\n",
      "Epoch 8/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 1.7845 - acc: 0.2924 - val_loss: 1.7562 - val_acc: 0.2994\n",
      "Epoch 9/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 1.7774 - acc: 0.2924 - val_loss: 1.7468 - val_acc: 0.2994\n",
      "Epoch 10/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 1.7677 - acc: 0.2924 - val_loss: 1.7356 - val_acc: 0.2994\n",
      "Epoch 11/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 1.7535 - acc: 0.2924 - val_loss: 1.7178 - val_acc: 0.2994\n",
      "Epoch 12/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 1.7325 - acc: 0.2924 - val_loss: 1.6938 - val_acc: 0.2994\n",
      "Epoch 13/1500\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 1.7042 - acc: 0.2924 - val_loss: 1.6626 - val_acc: 0.2994\n",
      "Epoch 14/1500\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 1.6679 - acc: 0.3014 - val_loss: 1.6231 - val_acc: 0.3922\n",
      "Epoch 15/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 1.6223 - acc: 0.4491 - val_loss: 1.5764 - val_acc: 0.5240\n",
      "Epoch 16/1500\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 1.5711 - acc: 0.5240 - val_loss: 1.5244 - val_acc: 0.5419\n",
      "Epoch 17/1500\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 1.5166 - acc: 0.5309 - val_loss: 1.4699 - val_acc: 0.5479\n",
      "Epoch 18/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 1.4630 - acc: 0.5309 - val_loss: 1.4199 - val_acc: 0.5509\n",
      "Epoch 19/1500\n",
      "1002/1002 [==============================] - 0s 138us/step - loss: 1.4122 - acc: 0.5329 - val_loss: 1.3723 - val_acc: 0.5509\n",
      "Epoch 20/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 1.3685 - acc: 0.5329 - val_loss: 1.3305 - val_acc: 0.5509\n",
      "Epoch 21/1500\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 1.3299 - acc: 0.5329 - val_loss: 1.2960 - val_acc: 0.5509\n",
      "Epoch 22/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 1.2952 - acc: 0.5329 - val_loss: 1.2632 - val_acc: 0.5509\n",
      "Epoch 23/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 1.2657 - acc: 0.5329 - val_loss: 1.2346 - val_acc: 0.5509\n",
      "Epoch 24/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 1.2395 - acc: 0.5329 - val_loss: 1.2097 - val_acc: 0.5509\n",
      "Epoch 25/1500\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 1.2161 - acc: 0.5329 - val_loss: 1.1868 - val_acc: 0.5509\n",
      "Epoch 26/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 1.1941 - acc: 0.5329 - val_loss: 1.1653 - val_acc: 0.5509\n",
      "Epoch 27/1500\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 1.1733 - acc: 0.5329 - val_loss: 1.1458 - val_acc: 0.5509\n",
      "Epoch 28/1500\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 1.1533 - acc: 0.5329 - val_loss: 1.1254 - val_acc: 0.5509\n",
      "Epoch 29/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 1.1340 - acc: 0.5329 - val_loss: 1.1065 - val_acc: 0.5509\n",
      "Epoch 30/1500\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 1.1152 - acc: 0.5329 - val_loss: 1.0888 - val_acc: 0.5509\n",
      "Epoch 31/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 1.0975 - acc: 0.5329 - val_loss: 1.0698 - val_acc: 0.5509\n",
      "Epoch 32/1500\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 1.0799 - acc: 0.5329 - val_loss: 1.0521 - val_acc: 0.5509\n",
      "Epoch 33/1500\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 1.0636 - acc: 0.5329 - val_loss: 1.0368 - val_acc: 0.5509\n",
      "Epoch 34/1500\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 1.0469 - acc: 0.5329 - val_loss: 1.0188 - val_acc: 0.5509\n",
      "Epoch 35/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 1.0314 - acc: 0.5329 - val_loss: 1.0037 - val_acc: 0.5509\n",
      "Epoch 36/1500\n",
      "1002/1002 [==============================] - 0s 43us/step - loss: 1.0156 - acc: 0.5339 - val_loss: 0.9886 - val_acc: 0.5509\n",
      "Epoch 37/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 1.0010 - acc: 0.5369 - val_loss: 0.9725 - val_acc: 0.5569\n",
      "Epoch 38/1500\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.9861 - acc: 0.5399 - val_loss: 0.9586 - val_acc: 0.5569\n",
      "Epoch 39/1500\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.9721 - acc: 0.5479 - val_loss: 0.9434 - val_acc: 0.5569\n",
      "Epoch 40/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.9594 - acc: 0.5529 - val_loss: 0.9313 - val_acc: 0.5778\n",
      "Epoch 41/1500\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.9453 - acc: 0.5669 - val_loss: 0.9172 - val_acc: 0.5898\n",
      "Epoch 42/1500\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.9327 - acc: 0.5808 - val_loss: 0.9045 - val_acc: 0.6018\n",
      "Epoch 43/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.9205 - acc: 0.5808 - val_loss: 0.8918 - val_acc: 0.6018\n",
      "Epoch 44/1500\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.9083 - acc: 0.6088 - val_loss: 0.8806 - val_acc: 0.6228\n",
      "Epoch 45/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.8970 - acc: 0.6108 - val_loss: 0.8688 - val_acc: 0.6168\n",
      "Epoch 46/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.8857 - acc: 0.6257 - val_loss: 0.8558 - val_acc: 0.6287\n",
      "Epoch 47/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.8757 - acc: 0.6417 - val_loss: 0.8458 - val_acc: 0.6467\n",
      "Epoch 48/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.8644 - acc: 0.6477 - val_loss: 0.8361 - val_acc: 0.6467\n",
      "Epoch 49/1500\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.8542 - acc: 0.6507 - val_loss: 0.8256 - val_acc: 0.6587\n",
      "Epoch 50/1500\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.8445 - acc: 0.6587 - val_loss: 0.8155 - val_acc: 0.6707\n",
      "Epoch 51/1500\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.8248 - acc: 0.675 - 0s 86us/step - loss: 0.8355 - acc: 0.6627 - val_loss: 0.8054 - val_acc: 0.6796\n",
      "Epoch 52/1500\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.8270 - acc: 0.6737 - val_loss: 0.7991 - val_acc: 0.6916\n",
      "Epoch 53/1500\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.8194 - acc: 0.6687 - val_loss: 0.7903 - val_acc: 0.6856\n",
      "Epoch 54/1500\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.8107 - acc: 0.6776 - val_loss: 0.7805 - val_acc: 0.7006\n",
      "Epoch 55/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.8030 - acc: 0.6896 - val_loss: 0.7721 - val_acc: 0.6976\n",
      "Epoch 56/1500\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.7956 - acc: 0.7006 - val_loss: 0.7663 - val_acc: 0.7246\n",
      "Epoch 57/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.7875 - acc: 0.7216 - val_loss: 0.7576 - val_acc: 0.7335\n",
      "Epoch 58/1500\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.7808 - acc: 0.7136 - val_loss: 0.7508 - val_acc: 0.7365\n",
      "Epoch 59/1500\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.7743 - acc: 0.7196 - val_loss: 0.7441 - val_acc: 0.7485\n",
      "Epoch 60/1500\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.7674 - acc: 0.7395 - val_loss: 0.7369 - val_acc: 0.7515\n",
      "Epoch 61/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.7615 - acc: 0.7136 - val_loss: 0.7306 - val_acc: 0.7455\n",
      "Epoch 62/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.7553 - acc: 0.7186 - val_loss: 0.7237 - val_acc: 0.7515\n",
      "Epoch 63/1500\n",
      "1002/1002 [==============================] - 0s 42us/step - loss: 0.7504 - acc: 0.7405 - val_loss: 0.7183 - val_acc: 0.7695\n",
      "Epoch 64/1500\n",
      "1002/1002 [==============================] - 0s 39us/step - loss: 0.7440 - acc: 0.7465 - val_loss: 0.7132 - val_acc: 0.7754\n",
      "Epoch 65/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.7386 - acc: 0.7485 - val_loss: 0.7056 - val_acc: 0.7784\n",
      "Epoch 66/1500\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.7326 - acc: 0.7465 - val_loss: 0.7012 - val_acc: 0.7784\n",
      "Epoch 67/1500\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.7272 - acc: 0.7485 - val_loss: 0.6954 - val_acc: 0.7844\n",
      "Epoch 68/1500\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.7226 - acc: 0.7465 - val_loss: 0.6888 - val_acc: 0.7814\n",
      "Epoch 69/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.7181 - acc: 0.7465 - val_loss: 0.6853 - val_acc: 0.7725\n",
      "Epoch 70/1500\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.7121 - acc: 0.7495 - val_loss: 0.6793 - val_acc: 0.7904\n",
      "Epoch 71/1500\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.7072 - acc: 0.7545 - val_loss: 0.6746 - val_acc: 0.7904\n",
      "Epoch 72/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.7032 - acc: 0.7515 - val_loss: 0.6691 - val_acc: 0.7904\n",
      "Epoch 73/1500\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.6976 - acc: 0.7495 - val_loss: 0.6653 - val_acc: 0.7784\n",
      "Epoch 74/1500\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.6944 - acc: 0.7515 - val_loss: 0.6605 - val_acc: 0.7784\n",
      "Epoch 75/1500\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.6989 - acc: 0.745 - 0s 94us/step - loss: 0.6895 - acc: 0.7555 - val_loss: 0.6550 - val_acc: 0.7874\n",
      "Epoch 76/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.6846 - acc: 0.7585 - val_loss: 0.6509 - val_acc: 0.7844\n",
      "Epoch 77/1500\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 0.6806 - acc: 0.7495 - val_loss: 0.6467 - val_acc: 0.7814\n",
      "Epoch 78/1500\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.6760 - acc: 0.7595 - val_loss: 0.6424 - val_acc: 0.7874\n",
      "Epoch 79/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.6723 - acc: 0.7615 - val_loss: 0.6400 - val_acc: 0.7874\n",
      "Epoch 80/1500\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.6673 - acc: 0.7605 - val_loss: 0.6336 - val_acc: 0.7844\n",
      "Epoch 81/1500\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.6629 - acc: 0.7605 - val_loss: 0.6286 - val_acc: 0.7934\n",
      "Epoch 82/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.6582 - acc: 0.7615 - val_loss: 0.6259 - val_acc: 0.7904\n",
      "Epoch 83/1500\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.6541 - acc: 0.7615 - val_loss: 0.6202 - val_acc: 0.7934\n",
      "Epoch 84/1500\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.6502 - acc: 0.7605 - val_loss: 0.6178 - val_acc: 0.7994\n",
      "Epoch 85/1500\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.6454 - acc: 0.7635 - val_loss: 0.6116 - val_acc: 0.7934\n",
      "Epoch 86/1500\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.6413 - acc: 0.7675 - val_loss: 0.6074 - val_acc: 0.7994\n",
      "Epoch 87/1500\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.6361 - acc: 0.7655 - val_loss: 0.6026 - val_acc: 0.7994\n",
      "Epoch 88/1500\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.6320 - acc: 0.7655 - val_loss: 0.5989 - val_acc: 0.8024\n",
      "Epoch 89/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.6276 - acc: 0.7655 - val_loss: 0.5951 - val_acc: 0.7994\n",
      "Epoch 90/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.6224 - acc: 0.7695 - val_loss: 0.5894 - val_acc: 0.8054\n",
      "Epoch 91/1500\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 0.6189 - acc: 0.7665 - val_loss: 0.5855 - val_acc: 0.7994\n",
      "Epoch 92/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.6134 - acc: 0.7675 - val_loss: 0.5819 - val_acc: 0.8024\n",
      "Epoch 93/1500\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.6088 - acc: 0.7675 - val_loss: 0.5763 - val_acc: 0.8054\n",
      "Epoch 94/1500\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.6038 - acc: 0.7685 - val_loss: 0.5729 - val_acc: 0.7994\n",
      "Epoch 95/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.5985 - acc: 0.7685 - val_loss: 0.5683 - val_acc: 0.8054\n",
      "Epoch 96/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.5935 - acc: 0.7705 - val_loss: 0.5628 - val_acc: 0.8084\n",
      "Epoch 97/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.5880 - acc: 0.7715 - val_loss: 0.5582 - val_acc: 0.8084\n",
      "Epoch 98/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.5830 - acc: 0.7754 - val_loss: 0.5539 - val_acc: 0.8084\n",
      "Epoch 99/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.5778 - acc: 0.7814 - val_loss: 0.5506 - val_acc: 0.8144\n",
      "Epoch 100/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.5727 - acc: 0.7774 - val_loss: 0.5449 - val_acc: 0.8174\n",
      "Epoch 101/1500\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.5675 - acc: 0.7924 - val_loss: 0.5388 - val_acc: 0.8204\n",
      "Epoch 102/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.5615 - acc: 0.7874 - val_loss: 0.5352 - val_acc: 0.8263\n",
      "Epoch 103/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.5561 - acc: 0.7894 - val_loss: 0.5300 - val_acc: 0.8323\n",
      "Epoch 104/1500\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.5503 - acc: 0.8104 - val_loss: 0.5246 - val_acc: 0.8323\n",
      "Epoch 105/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.5449 - acc: 0.8004 - val_loss: 0.5204 - val_acc: 0.8413\n",
      "Epoch 106/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.5400 - acc: 0.8144 - val_loss: 0.5156 - val_acc: 0.8383\n",
      "Epoch 107/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.5342 - acc: 0.8214 - val_loss: 0.5105 - val_acc: 0.8413\n",
      "Epoch 108/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.5281 - acc: 0.8154 - val_loss: 0.5066 - val_acc: 0.8353\n",
      "Epoch 109/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.5221 - acc: 0.8303 - val_loss: 0.5019 - val_acc: 0.8413\n",
      "Epoch 110/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.5171 - acc: 0.8263 - val_loss: 0.4976 - val_acc: 0.8413\n",
      "Epoch 111/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.5114 - acc: 0.8253 - val_loss: 0.4913 - val_acc: 0.8413\n",
      "Epoch 112/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.5055 - acc: 0.8373 - val_loss: 0.4872 - val_acc: 0.8473\n",
      "Epoch 113/1500\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.5005 - acc: 0.8313 - val_loss: 0.4826 - val_acc: 0.8473\n",
      "Epoch 114/1500\n",
      "1002/1002 [==============================] - 0s 49us/step - loss: 0.4946 - acc: 0.8433 - val_loss: 0.4782 - val_acc: 0.8533\n",
      "Epoch 115/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.4897 - acc: 0.8403 - val_loss: 0.4742 - val_acc: 0.8533\n",
      "Epoch 116/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4843 - acc: 0.8413 - val_loss: 0.4697 - val_acc: 0.8533\n",
      "Epoch 117/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.4785 - acc: 0.8443 - val_loss: 0.4649 - val_acc: 0.8503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1500\n",
      "1002/1002 [==============================] - 0s 42us/step - loss: 0.4751 - acc: 0.8423 - val_loss: 0.4602 - val_acc: 0.8473\n",
      "Epoch 119/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4692 - acc: 0.8463 - val_loss: 0.4568 - val_acc: 0.8503\n",
      "Epoch 120/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.4637 - acc: 0.8433 - val_loss: 0.4522 - val_acc: 0.8503\n",
      "Epoch 121/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4593 - acc: 0.8423 - val_loss: 0.4484 - val_acc: 0.8503\n",
      "Epoch 122/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4540 - acc: 0.8453 - val_loss: 0.4445 - val_acc: 0.8503\n",
      "Epoch 123/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.4488 - acc: 0.8463 - val_loss: 0.4398 - val_acc: 0.8533\n",
      "Epoch 124/1500\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.4448 - acc: 0.8453 - val_loss: 0.4354 - val_acc: 0.8533\n",
      "Epoch 125/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4392 - acc: 0.8483 - val_loss: 0.4313 - val_acc: 0.8563\n",
      "Epoch 126/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.4345 - acc: 0.8483 - val_loss: 0.4269 - val_acc: 0.8563\n",
      "Epoch 127/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.4301 - acc: 0.8433 - val_loss: 0.4225 - val_acc: 0.8563\n",
      "Epoch 128/1500\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.4249 - acc: 0.8503 - val_loss: 0.4173 - val_acc: 0.8563\n",
      "Epoch 129/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4205 - acc: 0.8443 - val_loss: 0.4139 - val_acc: 0.8593\n",
      "Epoch 130/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.4152 - acc: 0.8493 - val_loss: 0.4080 - val_acc: 0.8593\n",
      "Epoch 131/1500\n",
      "1002/1002 [==============================] - 0s 49us/step - loss: 0.4103 - acc: 0.8483 - val_loss: 0.4036 - val_acc: 0.8593\n",
      "Epoch 132/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4050 - acc: 0.8513 - val_loss: 0.4000 - val_acc: 0.8563\n",
      "Epoch 133/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.4015 - acc: 0.8513 - val_loss: 0.3949 - val_acc: 0.8593\n",
      "Epoch 134/1500\n",
      "1002/1002 [==============================] - 0s 39us/step - loss: 0.3953 - acc: 0.8493 - val_loss: 0.3897 - val_acc: 0.8593\n",
      "Epoch 135/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.3892 - acc: 0.8523 - val_loss: 0.3833 - val_acc: 0.8593\n",
      "Epoch 136/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.3832 - acc: 0.8513 - val_loss: 0.3779 - val_acc: 0.8593\n",
      "Epoch 137/1500\n",
      "1002/1002 [==============================] - 0s 46us/step - loss: 0.3783 - acc: 0.8513 - val_loss: 0.3724 - val_acc: 0.8563\n",
      "Epoch 138/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.3715 - acc: 0.8533 - val_loss: 0.3678 - val_acc: 0.8533\n",
      "Epoch 139/1500\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.3658 - acc: 0.8543 - val_loss: 0.3605 - val_acc: 0.8593\n",
      "Epoch 140/1500\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.3602 - acc: 0.8533 - val_loss: 0.3553 - val_acc: 0.8593\n",
      "Epoch 141/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.3548 - acc: 0.8553 - val_loss: 0.3506 - val_acc: 0.8593\n",
      "Epoch 142/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.3479 - acc: 0.8533 - val_loss: 0.3438 - val_acc: 0.8623\n",
      "Epoch 143/1500\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.3419 - acc: 0.8563 - val_loss: 0.3368 - val_acc: 0.8623\n",
      "Epoch 144/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.3366 - acc: 0.8553 - val_loss: 0.3332 - val_acc: 0.8653\n",
      "Epoch 145/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.3303 - acc: 0.8523 - val_loss: 0.3269 - val_acc: 0.8653\n",
      "Epoch 146/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.3239 - acc: 0.8563 - val_loss: 0.3203 - val_acc: 0.8653\n",
      "Epoch 147/1500\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.3185 - acc: 0.8633 - val_loss: 0.3142 - val_acc: 0.8653\n",
      "Epoch 148/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.3119 - acc: 0.8703 - val_loss: 0.3084 - val_acc: 0.8772\n",
      "Epoch 149/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.3056 - acc: 0.8613 - val_loss: 0.3033 - val_acc: 0.8892\n",
      "Epoch 150/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.3002 - acc: 0.8792 - val_loss: 0.2976 - val_acc: 0.8922\n",
      "Epoch 151/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.2941 - acc: 0.8802 - val_loss: 0.2922 - val_acc: 0.9042\n",
      "Epoch 152/1500\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.2893 - acc: 0.9122 - val_loss: 0.2856 - val_acc: 0.8892\n",
      "Epoch 153/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.2823 - acc: 0.9142 - val_loss: 0.2799 - val_acc: 0.9102\n",
      "Epoch 154/1500\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.2745 - acc: 0.9182 - val_loss: 0.2724 - val_acc: 0.9521\n",
      "Epoch 155/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.2669 - acc: 0.9421 - val_loss: 0.2621 - val_acc: 0.9521\n",
      "Epoch 156/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.2585 - acc: 0.9551 - val_loss: 0.2551 - val_acc: 0.9641\n",
      "Epoch 157/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.2511 - acc: 0.9581 - val_loss: 0.2487 - val_acc: 0.9641\n",
      "Epoch 158/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.2438 - acc: 0.9601 - val_loss: 0.2424 - val_acc: 0.9671\n",
      "Epoch 159/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.2382 - acc: 0.9641 - val_loss: 0.2358 - val_acc: 0.9641\n",
      "Epoch 160/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.2318 - acc: 0.9631 - val_loss: 0.2296 - val_acc: 0.9671\n",
      "Epoch 161/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.2259 - acc: 0.9601 - val_loss: 0.2244 - val_acc: 0.9641\n",
      "Epoch 162/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.2199 - acc: 0.9651 - val_loss: 0.2185 - val_acc: 0.9671\n",
      "Epoch 163/1500\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.2148 - acc: 0.9581 - val_loss: 0.2140 - val_acc: 0.9671\n",
      "Epoch 164/1500\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.2086 - acc: 0.9631 - val_loss: 0.2078 - val_acc: 0.9671\n",
      "Epoch 165/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.2041 - acc: 0.9651 - val_loss: 0.2031 - val_acc: 0.9671\n",
      "Epoch 166/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1988 - acc: 0.9661 - val_loss: 0.1990 - val_acc: 0.9671\n",
      "Epoch 167/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1944 - acc: 0.9641 - val_loss: 0.1952 - val_acc: 0.9671\n",
      "Epoch 168/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.1903 - acc: 0.9671 - val_loss: 0.1904 - val_acc: 0.9671\n",
      "Epoch 169/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.1855 - acc: 0.9651 - val_loss: 0.1867 - val_acc: 0.9641\n",
      "Epoch 170/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.1818 - acc: 0.9651 - val_loss: 0.1826 - val_acc: 0.9671\n",
      "Epoch 171/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1780 - acc: 0.9661 - val_loss: 0.1785 - val_acc: 0.9671\n",
      "Epoch 172/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.1746 - acc: 0.9681 - val_loss: 0.1747 - val_acc: 0.9671\n",
      "Epoch 173/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1704 - acc: 0.9651 - val_loss: 0.1726 - val_acc: 0.9641\n",
      "Epoch 174/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1675 - acc: 0.9641 - val_loss: 0.1674 - val_acc: 0.9671\n",
      "Epoch 175/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.1650 - acc: 0.9661 - val_loss: 0.1646 - val_acc: 0.9671\n",
      "Epoch 176/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1620 - acc: 0.9651 - val_loss: 0.1618 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1590 - acc: 0.9681 - val_loss: 0.1597 - val_acc: 0.9641\n",
      "Epoch 178/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.1559 - acc: 0.9651 - val_loss: 0.1559 - val_acc: 0.9641\n",
      "Epoch 179/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1533 - acc: 0.9661 - val_loss: 0.1533 - val_acc: 0.9671\n",
      "Epoch 180/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.1505 - acc: 0.9671 - val_loss: 0.1505 - val_acc: 0.9671\n",
      "Epoch 181/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1486 - acc: 0.9641 - val_loss: 0.1486 - val_acc: 0.9671\n",
      "Epoch 182/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1467 - acc: 0.9671 - val_loss: 0.1463 - val_acc: 0.9671\n",
      "Epoch 183/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.1437 - acc: 0.9661 - val_loss: 0.1437 - val_acc: 0.9671\n",
      "Epoch 184/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1414 - acc: 0.9661 - val_loss: 0.1411 - val_acc: 0.9701\n",
      "Epoch 185/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1393 - acc: 0.9681 - val_loss: 0.1386 - val_acc: 0.9701\n",
      "Epoch 186/1500\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.1377 - acc: 0.9671 - val_loss: 0.1369 - val_acc: 0.9701\n",
      "Epoch 187/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.1360 - acc: 0.9641 - val_loss: 0.1347 - val_acc: 0.9701\n",
      "Epoch 188/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1340 - acc: 0.9671 - val_loss: 0.1326 - val_acc: 0.9671\n",
      "Epoch 189/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.1321 - acc: 0.9691 - val_loss: 0.1306 - val_acc: 0.9701\n",
      "Epoch 190/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.1307 - acc: 0.9671 - val_loss: 0.1295 - val_acc: 0.9701\n",
      "Epoch 191/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1286 - acc: 0.9671 - val_loss: 0.1268 - val_acc: 0.9701\n",
      "Epoch 192/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.1274 - acc: 0.9691 - val_loss: 0.1252 - val_acc: 0.9701\n",
      "Epoch 193/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.1258 - acc: 0.9661 - val_loss: 0.1240 - val_acc: 0.9701\n",
      "Epoch 194/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.1239 - acc: 0.9681 - val_loss: 0.1223 - val_acc: 0.9701\n",
      "Epoch 195/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1232 - acc: 0.9681 - val_loss: 0.1205 - val_acc: 0.9731\n",
      "Epoch 196/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1225 - acc: 0.9671 - val_loss: 0.1192 - val_acc: 0.9671\n",
      "Epoch 197/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.1203 - acc: 0.9691 - val_loss: 0.1175 - val_acc: 0.9731\n",
      "Epoch 198/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.1185 - acc: 0.9671 - val_loss: 0.1153 - val_acc: 0.9731\n",
      "Epoch 199/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1173 - acc: 0.9701 - val_loss: 0.1148 - val_acc: 0.9731\n",
      "Epoch 200/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1162 - acc: 0.9671 - val_loss: 0.1130 - val_acc: 0.9731\n",
      "Epoch 201/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.1150 - acc: 0.9671 - val_loss: 0.1114 - val_acc: 0.9731\n",
      "Epoch 202/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.1146 - acc: 0.9691 - val_loss: 0.1109 - val_acc: 0.9760\n",
      "Epoch 203/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1125 - acc: 0.9701 - val_loss: 0.1091 - val_acc: 0.9731\n",
      "Epoch 204/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1117 - acc: 0.9691 - val_loss: 0.1090 - val_acc: 0.9760\n",
      "Epoch 205/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.1102 - acc: 0.9671 - val_loss: 0.1069 - val_acc: 0.9760\n",
      "Epoch 206/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1095 - acc: 0.9701 - val_loss: 0.1063 - val_acc: 0.9760\n",
      "Epoch 207/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1087 - acc: 0.9701 - val_loss: 0.1053 - val_acc: 0.9701\n",
      "Epoch 208/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1073 - acc: 0.9681 - val_loss: 0.1040 - val_acc: 0.9760\n",
      "Epoch 209/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.1067 - acc: 0.9691 - val_loss: 0.1029 - val_acc: 0.9760\n",
      "Epoch 210/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.1055 - acc: 0.9691 - val_loss: 0.1015 - val_acc: 0.9760\n",
      "Epoch 211/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.1050 - acc: 0.9721 - val_loss: 0.1008 - val_acc: 0.9760\n",
      "Epoch 212/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1037 - acc: 0.9711 - val_loss: 0.0999 - val_acc: 0.9760\n",
      "Epoch 213/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1035 - acc: 0.9731 - val_loss: 0.0992 - val_acc: 0.9760\n",
      "Epoch 214/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1023 - acc: 0.9711 - val_loss: 0.0984 - val_acc: 0.9701\n",
      "Epoch 215/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1012 - acc: 0.9711 - val_loss: 0.0979 - val_acc: 0.9760\n",
      "Epoch 216/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.1007 - acc: 0.9721 - val_loss: 0.0973 - val_acc: 0.9760\n",
      "Epoch 217/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1002 - acc: 0.9691 - val_loss: 0.0956 - val_acc: 0.9701\n",
      "Epoch 218/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0995 - acc: 0.9731 - val_loss: 0.0945 - val_acc: 0.9760\n",
      "Epoch 219/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0987 - acc: 0.9741 - val_loss: 0.0952 - val_acc: 0.9760\n",
      "Epoch 220/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0978 - acc: 0.9711 - val_loss: 0.0936 - val_acc: 0.9701\n",
      "Epoch 221/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0972 - acc: 0.9711 - val_loss: 0.0935 - val_acc: 0.9701\n",
      "Epoch 222/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0963 - acc: 0.9701 - val_loss: 0.0922 - val_acc: 0.9760\n",
      "Epoch 223/1500\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0960 - acc: 0.9721 - val_loss: 0.0916 - val_acc: 0.9731\n",
      "Epoch 224/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0945 - acc: 0.9721 - val_loss: 0.0911 - val_acc: 0.9760\n",
      "Epoch 225/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0945 - acc: 0.9721 - val_loss: 0.0896 - val_acc: 0.9760\n",
      "Epoch 226/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0933 - acc: 0.9711 - val_loss: 0.0898 - val_acc: 0.9731\n",
      "Epoch 227/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0932 - acc: 0.9721 - val_loss: 0.0894 - val_acc: 0.9701\n",
      "Epoch 228/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0933 - acc: 0.9731 - val_loss: 0.0879 - val_acc: 0.9701\n",
      "Epoch 229/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0920 - acc: 0.9750 - val_loss: 0.0871 - val_acc: 0.9760\n",
      "Epoch 230/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0924 - acc: 0.9721 - val_loss: 0.0874 - val_acc: 0.9701\n",
      "Epoch 231/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0909 - acc: 0.9721 - val_loss: 0.0862 - val_acc: 0.9731\n",
      "Epoch 232/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0902 - acc: 0.9731 - val_loss: 0.0852 - val_acc: 0.9701\n",
      "Epoch 233/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0900 - acc: 0.9760 - val_loss: 0.0858 - val_acc: 0.9731\n",
      "Epoch 234/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0891 - acc: 0.9741 - val_loss: 0.0855 - val_acc: 0.9760\n",
      "Epoch 235/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0889 - acc: 0.9750 - val_loss: 0.0839 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0880 - acc: 0.9741 - val_loss: 0.0833 - val_acc: 0.9701\n",
      "Epoch 237/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0873 - acc: 0.9741 - val_loss: 0.0840 - val_acc: 0.9731\n",
      "Epoch 238/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0870 - acc: 0.9721 - val_loss: 0.0827 - val_acc: 0.9701\n",
      "Epoch 239/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0871 - acc: 0.9731 - val_loss: 0.0816 - val_acc: 0.9701\n",
      "Epoch 240/1500\n",
      "1002/1002 [==============================] - 0s 43us/step - loss: 0.0868 - acc: 0.9731 - val_loss: 0.0824 - val_acc: 0.9701\n",
      "Epoch 241/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0858 - acc: 0.9741 - val_loss: 0.0818 - val_acc: 0.9760\n",
      "Epoch 242/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0852 - acc: 0.9741 - val_loss: 0.0802 - val_acc: 0.9701\n",
      "Epoch 243/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0853 - acc: 0.9731 - val_loss: 0.0799 - val_acc: 0.9731\n",
      "Epoch 244/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0845 - acc: 0.9721 - val_loss: 0.0798 - val_acc: 0.9701\n",
      "Epoch 245/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0842 - acc: 0.9741 - val_loss: 0.0787 - val_acc: 0.9701\n",
      "Epoch 246/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0834 - acc: 0.9760 - val_loss: 0.0790 - val_acc: 0.9731\n",
      "Epoch 247/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0829 - acc: 0.9750 - val_loss: 0.0791 - val_acc: 0.9731\n",
      "Epoch 248/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0831 - acc: 0.9731 - val_loss: 0.0785 - val_acc: 0.9731\n",
      "Epoch 249/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0822 - acc: 0.9741 - val_loss: 0.0775 - val_acc: 0.9701\n",
      "Epoch 250/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0818 - acc: 0.9741 - val_loss: 0.0772 - val_acc: 0.9731\n",
      "Epoch 251/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0814 - acc: 0.9750 - val_loss: 0.0770 - val_acc: 0.9701\n",
      "Epoch 252/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0815 - acc: 0.9750 - val_loss: 0.0766 - val_acc: 0.9731\n",
      "Epoch 253/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0808 - acc: 0.9741 - val_loss: 0.0765 - val_acc: 0.9731\n",
      "Epoch 254/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0807 - acc: 0.9741 - val_loss: 0.0760 - val_acc: 0.9701\n",
      "Epoch 255/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0802 - acc: 0.9760 - val_loss: 0.0757 - val_acc: 0.9701\n",
      "Epoch 256/1500\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0803 - acc: 0.9760 - val_loss: 0.0746 - val_acc: 0.9731\n",
      "Epoch 257/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0797 - acc: 0.9741 - val_loss: 0.0746 - val_acc: 0.9701\n",
      "Epoch 258/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0794 - acc: 0.9750 - val_loss: 0.0737 - val_acc: 0.9701\n",
      "Epoch 259/1500\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0787 - acc: 0.9770 - val_loss: 0.0738 - val_acc: 0.9731\n",
      "Epoch 260/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0785 - acc: 0.9760 - val_loss: 0.0742 - val_acc: 0.9731\n",
      "Epoch 261/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0796 - acc: 0.9790 - val_loss: 0.0731 - val_acc: 0.9731\n",
      "Epoch 262/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0785 - acc: 0.9750 - val_loss: 0.0726 - val_acc: 0.9701\n",
      "Epoch 263/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0778 - acc: 0.9731 - val_loss: 0.0725 - val_acc: 0.9701\n",
      "Epoch 264/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0773 - acc: 0.9760 - val_loss: 0.0721 - val_acc: 0.9701\n",
      "Epoch 265/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0770 - acc: 0.9750 - val_loss: 0.0725 - val_acc: 0.9731\n",
      "Epoch 266/1500\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0765 - acc: 0.9760 - val_loss: 0.0714 - val_acc: 0.9701\n",
      "Epoch 267/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0770 - acc: 0.9750 - val_loss: 0.0718 - val_acc: 0.9731\n",
      "Epoch 268/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0762 - acc: 0.9741 - val_loss: 0.0715 - val_acc: 0.9731\n",
      "Epoch 269/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0757 - acc: 0.9750 - val_loss: 0.0714 - val_acc: 0.9701\n",
      "Epoch 270/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0755 - acc: 0.9760 - val_loss: 0.0702 - val_acc: 0.9701\n",
      "Epoch 271/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0755 - acc: 0.9760 - val_loss: 0.0708 - val_acc: 0.9731\n",
      "Epoch 272/1500\n",
      "1002/1002 [==============================] - 0s 133us/step - loss: 0.0753 - acc: 0.9750 - val_loss: 0.0695 - val_acc: 0.9701\n",
      "Epoch 273/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0747 - acc: 0.9760 - val_loss: 0.0699 - val_acc: 0.9701\n",
      "Epoch 274/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0747 - acc: 0.9750 - val_loss: 0.0702 - val_acc: 0.9701\n",
      "Epoch 275/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0747 - acc: 0.9760 - val_loss: 0.0692 - val_acc: 0.9731\n",
      "Epoch 276/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0741 - acc: 0.9750 - val_loss: 0.0697 - val_acc: 0.9701\n",
      "Epoch 277/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0740 - acc: 0.9770 - val_loss: 0.0685 - val_acc: 0.9701\n",
      "Epoch 278/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0735 - acc: 0.9750 - val_loss: 0.0690 - val_acc: 0.9701\n",
      "Epoch 279/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0739 - acc: 0.9750 - val_loss: 0.0681 - val_acc: 0.9701\n",
      "Epoch 280/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0737 - acc: 0.9780 - val_loss: 0.0677 - val_acc: 0.9701\n",
      "Epoch 281/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0733 - acc: 0.9790 - val_loss: 0.0676 - val_acc: 0.9701\n",
      "Epoch 282/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0733 - acc: 0.9731 - val_loss: 0.0693 - val_acc: 0.9701\n",
      "Epoch 283/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0731 - acc: 0.9760 - val_loss: 0.0669 - val_acc: 0.9701\n",
      "Epoch 284/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0724 - acc: 0.9741 - val_loss: 0.0679 - val_acc: 0.9701\n",
      "Epoch 285/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0720 - acc: 0.9760 - val_loss: 0.0669 - val_acc: 0.9701\n",
      "Epoch 286/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0721 - acc: 0.9741 - val_loss: 0.0668 - val_acc: 0.9701\n",
      "Epoch 287/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0715 - acc: 0.9770 - val_loss: 0.0659 - val_acc: 0.9701\n",
      "Epoch 288/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0716 - acc: 0.9780 - val_loss: 0.0665 - val_acc: 0.9731\n",
      "Epoch 289/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0713 - acc: 0.9780 - val_loss: 0.0659 - val_acc: 0.9701\n",
      "Epoch 290/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0711 - acc: 0.9760 - val_loss: 0.0666 - val_acc: 0.9701\n",
      "Epoch 291/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0716 - acc: 0.9770 - val_loss: 0.0660 - val_acc: 0.9731\n",
      "Epoch 292/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0714 - acc: 0.9770 - val_loss: 0.0658 - val_acc: 0.9701\n",
      "Epoch 293/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0709 - acc: 0.9760 - val_loss: 0.0656 - val_acc: 0.9701\n",
      "Epoch 294/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0707 - acc: 0.9770 - val_loss: 0.0643 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0702 - acc: 0.9741 - val_loss: 0.0654 - val_acc: 0.9701\n",
      "Epoch 296/1500\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0704 - acc: 0.9760 - val_loss: 0.0642 - val_acc: 0.9701\n",
      "Epoch 297/1500\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.0698 - acc: 0.9780 - val_loss: 0.0646 - val_acc: 0.9731\n",
      "Epoch 298/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0696 - acc: 0.9760 - val_loss: 0.0647 - val_acc: 0.9701\n",
      "Epoch 299/1500\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0697 - acc: 0.9760 - val_loss: 0.0647 - val_acc: 0.9701\n",
      "Epoch 300/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0697 - acc: 0.9760 - val_loss: 0.0643 - val_acc: 0.9731\n",
      "Epoch 301/1500\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0690 - acc: 0.9760 - val_loss: 0.0640 - val_acc: 0.9701\n",
      "Epoch 302/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0688 - acc: 0.9780 - val_loss: 0.0633 - val_acc: 0.9731\n",
      "Epoch 303/1500\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0692 - acc: 0.9770 - val_loss: 0.0640 - val_acc: 0.9701\n",
      "Epoch 304/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0691 - acc: 0.9750 - val_loss: 0.0636 - val_acc: 0.9701\n",
      "Epoch 305/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0694 - acc: 0.9760 - val_loss: 0.0640 - val_acc: 0.9701\n",
      "Epoch 306/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0683 - acc: 0.9760 - val_loss: 0.0634 - val_acc: 0.9701\n",
      "Epoch 307/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0680 - acc: 0.9770 - val_loss: 0.0631 - val_acc: 0.9701\n",
      "Epoch 308/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0682 - acc: 0.9780 - val_loss: 0.0626 - val_acc: 0.9701\n",
      "Epoch 309/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0679 - acc: 0.9770 - val_loss: 0.0629 - val_acc: 0.9701\n",
      "Epoch 310/1500\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0677 - acc: 0.9750 - val_loss: 0.0636 - val_acc: 0.9701\n",
      "Epoch 311/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0680 - acc: 0.9770 - val_loss: 0.0623 - val_acc: 0.9731\n",
      "Epoch 312/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0673 - acc: 0.9790 - val_loss: 0.0617 - val_acc: 0.9701\n",
      "Epoch 313/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0674 - acc: 0.9760 - val_loss: 0.0619 - val_acc: 0.9701\n",
      "Epoch 314/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0672 - acc: 0.9760 - val_loss: 0.0622 - val_acc: 0.9760\n",
      "Epoch 315/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0672 - acc: 0.9760 - val_loss: 0.0614 - val_acc: 0.9701\n",
      "Epoch 316/1500\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0672 - acc: 0.9780 - val_loss: 0.0614 - val_acc: 0.9701\n",
      "Epoch 317/1500\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0668 - acc: 0.9770 - val_loss: 0.0615 - val_acc: 0.9701\n",
      "Epoch 318/1500\n",
      "1002/1002 [==============================] - 0s 115us/step - loss: 0.0667 - acc: 0.9760 - val_loss: 0.0617 - val_acc: 0.9701\n",
      "Epoch 319/1500\n",
      "1002/1002 [==============================] - 0s 129us/step - loss: 0.0668 - acc: 0.9760 - val_loss: 0.0608 - val_acc: 0.9760\n",
      "Epoch 320/1500\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0667 - acc: 0.9790 - val_loss: 0.0601 - val_acc: 0.9731\n",
      "Epoch 321/1500\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0662 - acc: 0.9760 - val_loss: 0.0619 - val_acc: 0.9701\n",
      "Epoch 322/1500\n",
      "1002/1002 [==============================] - 0s 144us/step - loss: 0.0659 - acc: 0.9770 - val_loss: 0.0600 - val_acc: 0.9701\n",
      "Epoch 323/1500\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 0.0657 - acc: 0.9770 - val_loss: 0.0599 - val_acc: 0.9731\n",
      "Epoch 324/1500\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0659 - acc: 0.9790 - val_loss: 0.0593 - val_acc: 0.9731\n",
      "Epoch 325/1500\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 0.0658 - acc: 0.9750 - val_loss: 0.0603 - val_acc: 0.9731\n",
      "Epoch 326/1500\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 0.0653 - acc: 0.9790 - val_loss: 0.0590 - val_acc: 0.9701\n",
      "Epoch 327/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0652 - acc: 0.9770 - val_loss: 0.0612 - val_acc: 0.9701\n",
      "Epoch 328/1500\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0654 - acc: 0.9750 - val_loss: 0.0595 - val_acc: 0.9760\n",
      "Epoch 329/1500\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.0658 - acc: 0.9770 - val_loss: 0.0604 - val_acc: 0.9731\n",
      "Epoch 330/1500\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0654 - acc: 0.9770 - val_loss: 0.0591 - val_acc: 0.9731\n",
      "Epoch 331/1500\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0649 - acc: 0.9780 - val_loss: 0.0586 - val_acc: 0.9731\n",
      "Epoch 332/1500\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0649 - acc: 0.9770 - val_loss: 0.0593 - val_acc: 0.9731\n",
      "Epoch 333/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0648 - acc: 0.9760 - val_loss: 0.0588 - val_acc: 0.9731\n",
      "Epoch 334/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0645 - acc: 0.9770 - val_loss: 0.0585 - val_acc: 0.9760\n",
      "Epoch 335/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0645 - acc: 0.9800 - val_loss: 0.0583 - val_acc: 0.9760\n",
      "Epoch 336/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0649 - acc: 0.9770 - val_loss: 0.0586 - val_acc: 0.9731\n",
      "Epoch 337/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0646 - acc: 0.9760 - val_loss: 0.0585 - val_acc: 0.9731\n",
      "Epoch 338/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0639 - acc: 0.9780 - val_loss: 0.0571 - val_acc: 0.9731\n",
      "Epoch 339/1500\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0647 - acc: 0.9750 - val_loss: 0.0590 - val_acc: 0.9731\n",
      "Epoch 340/1500\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0639 - acc: 0.9770 - val_loss: 0.0575 - val_acc: 0.9760\n",
      "Epoch 341/1500\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0638 - acc: 0.9790 - val_loss: 0.0581 - val_acc: 0.9760\n",
      "Epoch 342/1500\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0636 - acc: 0.9760 - val_loss: 0.0582 - val_acc: 0.9731\n",
      "Epoch 343/1500\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0641 - acc: 0.9800 - val_loss: 0.0574 - val_acc: 0.9731\n",
      "Epoch 344/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0637 - acc: 0.9770 - val_loss: 0.0580 - val_acc: 0.9731\n",
      "Epoch 345/1500\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0636 - acc: 0.9760 - val_loss: 0.0571 - val_acc: 0.9731\n",
      "Epoch 346/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0631 - acc: 0.9760 - val_loss: 0.0576 - val_acc: 0.9731\n",
      "Epoch 347/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0631 - acc: 0.9770 - val_loss: 0.0572 - val_acc: 0.9731\n",
      "Epoch 348/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0631 - acc: 0.9760 - val_loss: 0.0573 - val_acc: 0.9731\n",
      "Epoch 349/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0630 - acc: 0.9780 - val_loss: 0.0565 - val_acc: 0.9731\n",
      "Epoch 350/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0630 - acc: 0.9770 - val_loss: 0.0574 - val_acc: 0.9731\n",
      "Epoch 351/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0629 - acc: 0.9780 - val_loss: 0.0565 - val_acc: 0.9731\n",
      "Epoch 352/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0630 - acc: 0.9750 - val_loss: 0.0567 - val_acc: 0.9731\n",
      "Epoch 353/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0633 - acc: 0.9780 - val_loss: 0.0561 - val_acc: 0.9760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0626 - acc: 0.9790 - val_loss: 0.0567 - val_acc: 0.9731\n",
      "Epoch 355/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0622 - acc: 0.9760 - val_loss: 0.0565 - val_acc: 0.9731\n",
      "Epoch 356/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0624 - acc: 0.9760 - val_loss: 0.0562 - val_acc: 0.9731\n",
      "Epoch 357/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0621 - acc: 0.9770 - val_loss: 0.0559 - val_acc: 0.9731\n",
      "Epoch 358/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0627 - acc: 0.9770 - val_loss: 0.0557 - val_acc: 0.9731\n",
      "Epoch 359/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0623 - acc: 0.9780 - val_loss: 0.0556 - val_acc: 0.9731\n",
      "Epoch 360/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0618 - acc: 0.9780 - val_loss: 0.0561 - val_acc: 0.9760\n",
      "Epoch 361/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0620 - acc: 0.9790 - val_loss: 0.0553 - val_acc: 0.9731\n",
      "Epoch 362/1500\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0619 - acc: 0.9770 - val_loss: 0.0556 - val_acc: 0.9731\n",
      "Epoch 363/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0618 - acc: 0.9770 - val_loss: 0.0555 - val_acc: 0.9731\n",
      "Epoch 364/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0616 - acc: 0.9780 - val_loss: 0.0545 - val_acc: 0.9731\n",
      "Epoch 365/1500\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.0616 - acc: 0.9760 - val_loss: 0.0562 - val_acc: 0.9731\n",
      "Epoch 366/1500\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0616 - acc: 0.9790 - val_loss: 0.0550 - val_acc: 0.9731\n",
      "Epoch 367/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0610 - acc: 0.9770 - val_loss: 0.0552 - val_acc: 0.9731\n",
      "Epoch 368/1500\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0614 - acc: 0.9750 - val_loss: 0.0552 - val_acc: 0.9731\n",
      "Epoch 369/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0615 - acc: 0.9780 - val_loss: 0.0544 - val_acc: 0.9731\n",
      "Epoch 370/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0611 - acc: 0.9770 - val_loss: 0.0551 - val_acc: 0.9731\n",
      "Epoch 371/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0608 - acc: 0.9760 - val_loss: 0.0544 - val_acc: 0.9731\n",
      "Epoch 372/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0614 - acc: 0.9780 - val_loss: 0.0551 - val_acc: 0.9731\n",
      "Epoch 373/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0609 - acc: 0.9790 - val_loss: 0.0545 - val_acc: 0.9731\n",
      "Epoch 374/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.0542 - val_acc: 0.9731\n",
      "Epoch 375/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0608 - acc: 0.9780 - val_loss: 0.0541 - val_acc: 0.9760\n",
      "Epoch 376/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0611 - acc: 0.9780 - val_loss: 0.0537 - val_acc: 0.9731\n",
      "Epoch 377/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0609 - acc: 0.9780 - val_loss: 0.0551 - val_acc: 0.9731\n",
      "Epoch 378/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0601 - acc: 0.9780 - val_loss: 0.0537 - val_acc: 0.9760\n",
      "Epoch 379/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0606 - acc: 0.9770 - val_loss: 0.0543 - val_acc: 0.9731\n",
      "Epoch 380/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0609 - acc: 0.9760 - val_loss: 0.0542 - val_acc: 0.9760\n",
      "Epoch 381/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0601 - acc: 0.9770 - val_loss: 0.0531 - val_acc: 0.9731\n",
      "Epoch 382/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0602 - acc: 0.9760 - val_loss: 0.0541 - val_acc: 0.9731\n",
      "Epoch 383/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0608 - acc: 0.9790 - val_loss: 0.0534 - val_acc: 0.9731\n",
      "Epoch 384/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0597 - acc: 0.9770 - val_loss: 0.0537 - val_acc: 0.9731\n",
      "Epoch 385/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0613 - acc: 0.9790 - val_loss: 0.0545 - val_acc: 0.9731\n",
      "Epoch 386/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0598 - acc: 0.9790 - val_loss: 0.0536 - val_acc: 0.9760\n",
      "Epoch 387/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0596 - acc: 0.9790 - val_loss: 0.0527 - val_acc: 0.9731\n",
      "Epoch 388/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0607 - acc: 0.9780 - val_loss: 0.0536 - val_acc: 0.9731\n",
      "Epoch 389/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0601 - acc: 0.9780 - val_loss: 0.0530 - val_acc: 0.9731\n",
      "Epoch 390/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0600 - acc: 0.9770 - val_loss: 0.0529 - val_acc: 0.9760\n",
      "Epoch 391/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0595 - acc: 0.9760 - val_loss: 0.0527 - val_acc: 0.9731\n",
      "Epoch 392/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0592 - acc: 0.9770 - val_loss: 0.0533 - val_acc: 0.9731\n",
      "Epoch 393/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0591 - acc: 0.9790 - val_loss: 0.0524 - val_acc: 0.9731\n",
      "Epoch 394/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0594 - acc: 0.9780 - val_loss: 0.0528 - val_acc: 0.9731\n",
      "Epoch 395/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0595 - acc: 0.9760 - val_loss: 0.0527 - val_acc: 0.9731\n",
      "Epoch 396/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0587 - acc: 0.9790 - val_loss: 0.0523 - val_acc: 0.9731\n",
      "Epoch 397/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0588 - acc: 0.9780 - val_loss: 0.0523 - val_acc: 0.9731\n",
      "Epoch 398/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0590 - acc: 0.9790 - val_loss: 0.0525 - val_acc: 0.9731\n",
      "Epoch 399/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0584 - acc: 0.9760 - val_loss: 0.0525 - val_acc: 0.9760\n",
      "Epoch 400/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0588 - acc: 0.9770 - val_loss: 0.0521 - val_acc: 0.9731\n",
      "Epoch 401/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0584 - acc: 0.9760 - val_loss: 0.0522 - val_acc: 0.9731\n",
      "Epoch 402/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0589 - acc: 0.9790 - val_loss: 0.0517 - val_acc: 0.9731\n",
      "Epoch 403/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0584 - acc: 0.9780 - val_loss: 0.0518 - val_acc: 0.9731\n",
      "Epoch 404/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0588 - acc: 0.9780 - val_loss: 0.0515 - val_acc: 0.9731\n",
      "Epoch 405/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0585 - acc: 0.9780 - val_loss: 0.0521 - val_acc: 0.9731\n",
      "Epoch 406/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0586 - acc: 0.9770 - val_loss: 0.0522 - val_acc: 0.9731\n",
      "Epoch 407/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0586 - acc: 0.9780 - val_loss: 0.0509 - val_acc: 0.9731\n",
      "Epoch 408/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0583 - acc: 0.9770 - val_loss: 0.0518 - val_acc: 0.9731\n",
      "Epoch 409/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0585 - acc: 0.9770 - val_loss: 0.0517 - val_acc: 0.9731\n",
      "Epoch 410/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0586 - acc: 0.9780 - val_loss: 0.0517 - val_acc: 0.9731\n",
      "Epoch 411/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0588 - acc: 0.9790 - val_loss: 0.0510 - val_acc: 0.9731\n",
      "Epoch 412/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0593 - acc: 0.9770 - val_loss: 0.0525 - val_acc: 0.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0574 - acc: 0.9790 - val_loss: 0.0508 - val_acc: 0.9731\n",
      "Epoch 414/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0583 - acc: 0.9780 - val_loss: 0.0512 - val_acc: 0.9760\n",
      "Epoch 415/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0520 - val_acc: 0.9760\n",
      "Epoch 416/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0583 - acc: 0.9780 - val_loss: 0.0513 - val_acc: 0.9731\n",
      "Epoch 417/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0576 - acc: 0.9780 - val_loss: 0.0511 - val_acc: 0.9731\n",
      "Epoch 418/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0579 - acc: 0.9780 - val_loss: 0.0506 - val_acc: 0.9731\n",
      "Epoch 419/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0580 - acc: 0.9750 - val_loss: 0.0508 - val_acc: 0.9731\n",
      "Epoch 420/1500\n",
      "1002/1002 [==============================] - 0s 49us/step - loss: 0.0581 - acc: 0.9800 - val_loss: 0.0503 - val_acc: 0.9731\n",
      "Epoch 421/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0578 - acc: 0.9790 - val_loss: 0.0512 - val_acc: 0.9731\n",
      "Epoch 422/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0578 - acc: 0.9770 - val_loss: 0.0506 - val_acc: 0.9731\n",
      "Epoch 423/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0572 - acc: 0.9760 - val_loss: 0.0506 - val_acc: 0.9731\n",
      "Epoch 424/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0512 - val_acc: 0.9731\n",
      "Epoch 425/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0569 - acc: 0.9790 - val_loss: 0.0504 - val_acc: 0.9731\n",
      "Epoch 426/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0570 - acc: 0.9770 - val_loss: 0.0499 - val_acc: 0.9731\n",
      "Epoch 427/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0574 - acc: 0.9770 - val_loss: 0.0504 - val_acc: 0.9731\n",
      "Epoch 428/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0570 - acc: 0.9770 - val_loss: 0.0510 - val_acc: 0.9731\n",
      "Epoch 429/1500\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0568 - acc: 0.9780 - val_loss: 0.0501 - val_acc: 0.9731\n",
      "Epoch 430/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0577 - acc: 0.9780 - val_loss: 0.0491 - val_acc: 0.9731\n",
      "Epoch 431/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0579 - acc: 0.9780 - val_loss: 0.0497 - val_acc: 0.9731\n",
      "Epoch 432/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0576 - acc: 0.9770 - val_loss: 0.0501 - val_acc: 0.9731\n",
      "Epoch 433/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0573 - acc: 0.9790 - val_loss: 0.0486 - val_acc: 0.9760\n",
      "Epoch 434/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0566 - acc: 0.9790 - val_loss: 0.0499 - val_acc: 0.9760\n",
      "Epoch 435/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0570 - acc: 0.9750 - val_loss: 0.0513 - val_acc: 0.9731\n",
      "Epoch 436/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0571 - acc: 0.9790 - val_loss: 0.0495 - val_acc: 0.9731\n",
      "Epoch 437/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0561 - acc: 0.9790 - val_loss: 0.0505 - val_acc: 0.9731\n",
      "Epoch 438/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0566 - acc: 0.9790 - val_loss: 0.0499 - val_acc: 0.9731\n",
      "Epoch 439/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0569 - acc: 0.9780 - val_loss: 0.0501 - val_acc: 0.9731\n",
      "Epoch 440/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0564 - acc: 0.9770 - val_loss: 0.0499 - val_acc: 0.9731\n",
      "Epoch 441/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0560 - acc: 0.9800 - val_loss: 0.0492 - val_acc: 0.9731\n",
      "Epoch 442/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0561 - acc: 0.9790 - val_loss: 0.0490 - val_acc: 0.9731\n",
      "Epoch 443/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0567 - acc: 0.9780 - val_loss: 0.0494 - val_acc: 0.9731\n",
      "Epoch 444/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0563 - acc: 0.9790 - val_loss: 0.0492 - val_acc: 0.9731\n",
      "Epoch 445/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0568 - acc: 0.9790 - val_loss: 0.0489 - val_acc: 0.9760\n",
      "Epoch 446/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0559 - acc: 0.9780 - val_loss: 0.0495 - val_acc: 0.9731\n",
      "Epoch 447/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0559 - acc: 0.9770 - val_loss: 0.0493 - val_acc: 0.9731\n",
      "Epoch 448/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0555 - acc: 0.9780 - val_loss: 0.0485 - val_acc: 0.9731\n",
      "Epoch 449/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0564 - acc: 0.9790 - val_loss: 0.0480 - val_acc: 0.9760\n",
      "Epoch 450/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0557 - acc: 0.9800 - val_loss: 0.0498 - val_acc: 0.9731\n",
      "Epoch 451/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0563 - acc: 0.9800 - val_loss: 0.0486 - val_acc: 0.9731\n",
      "Epoch 452/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0561 - acc: 0.9790 - val_loss: 0.0489 - val_acc: 0.9731\n",
      "Epoch 453/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0559 - acc: 0.9780 - val_loss: 0.0490 - val_acc: 0.9760\n",
      "Epoch 454/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0564 - acc: 0.9780 - val_loss: 0.0495 - val_acc: 0.9731\n",
      "Epoch 455/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0557 - acc: 0.9780 - val_loss: 0.0495 - val_acc: 0.9731\n",
      "Epoch 456/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0560 - acc: 0.9770 - val_loss: 0.0488 - val_acc: 0.9731\n",
      "Epoch 457/1500\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0557 - acc: 0.9780 - val_loss: 0.0490 - val_acc: 0.9731\n",
      "Epoch 458/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0554 - acc: 0.9800 - val_loss: 0.0484 - val_acc: 0.9731\n",
      "Epoch 459/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0562 - acc: 0.9790 - val_loss: 0.0498 - val_acc: 0.9731\n",
      "Epoch 460/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0550 - acc: 0.9790 - val_loss: 0.0486 - val_acc: 0.9760\n",
      "Epoch 461/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0549 - acc: 0.9800 - val_loss: 0.0495 - val_acc: 0.9731\n",
      "Epoch 462/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0552 - acc: 0.9780 - val_loss: 0.0480 - val_acc: 0.9731\n",
      "Epoch 463/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0551 - acc: 0.9800 - val_loss: 0.0479 - val_acc: 0.9760\n",
      "Epoch 464/1500\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0555 - acc: 0.9780 - val_loss: 0.0484 - val_acc: 0.9731\n",
      "Epoch 465/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0549 - acc: 0.9780 - val_loss: 0.0479 - val_acc: 0.9760\n",
      "Epoch 466/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0547 - acc: 0.9770 - val_loss: 0.0488 - val_acc: 0.9731\n",
      "Epoch 467/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0550 - acc: 0.9810 - val_loss: 0.0476 - val_acc: 0.9790\n",
      "Epoch 468/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0551 - acc: 0.9780 - val_loss: 0.0481 - val_acc: 0.9731\n",
      "Epoch 469/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0547 - acc: 0.9810 - val_loss: 0.0479 - val_acc: 0.9731\n",
      "Epoch 470/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0544 - acc: 0.9790 - val_loss: 0.0471 - val_acc: 0.9760\n",
      "Epoch 471/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0548 - acc: 0.9790 - val_loss: 0.0481 - val_acc: 0.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0543 - acc: 0.9790 - val_loss: 0.0471 - val_acc: 0.9760\n",
      "Epoch 473/1500\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.0556 - acc: 0.9760 - val_loss: 0.0482 - val_acc: 0.9760\n",
      "Epoch 474/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0546 - acc: 0.9790 - val_loss: 0.0470 - val_acc: 0.9760\n",
      "Epoch 475/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0546 - acc: 0.9790 - val_loss: 0.0475 - val_acc: 0.9760\n",
      "Epoch 476/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0545 - acc: 0.9820 - val_loss: 0.0472 - val_acc: 0.9790\n",
      "Epoch 477/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0547 - acc: 0.9780 - val_loss: 0.0478 - val_acc: 0.9731\n",
      "Epoch 478/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0548 - acc: 0.9790 - val_loss: 0.0471 - val_acc: 0.9760\n",
      "Epoch 479/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0538 - acc: 0.9800 - val_loss: 0.0479 - val_acc: 0.9731\n",
      "Epoch 480/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0543 - acc: 0.9780 - val_loss: 0.0475 - val_acc: 0.9731\n",
      "Epoch 481/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0545 - acc: 0.9780 - val_loss: 0.0468 - val_acc: 0.9760\n",
      "Epoch 482/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0537 - acc: 0.9810 - val_loss: 0.0470 - val_acc: 0.9760\n",
      "Epoch 483/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0537 - acc: 0.9790 - val_loss: 0.0474 - val_acc: 0.9760\n",
      "Epoch 484/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0538 - acc: 0.9800 - val_loss: 0.0476 - val_acc: 0.9760\n",
      "Epoch 485/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0540 - acc: 0.9810 - val_loss: 0.0467 - val_acc: 0.9760\n",
      "Epoch 486/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0546 - acc: 0.9790 - val_loss: 0.0471 - val_acc: 0.9760\n",
      "Epoch 487/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0535 - acc: 0.9790 - val_loss: 0.0467 - val_acc: 0.9760\n",
      "Epoch 488/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0538 - acc: 0.9800 - val_loss: 0.0466 - val_acc: 0.9760\n",
      "Epoch 489/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0535 - acc: 0.9790 - val_loss: 0.0467 - val_acc: 0.9760\n",
      "Epoch 490/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0536 - acc: 0.9800 - val_loss: 0.0468 - val_acc: 0.9790\n",
      "Epoch 491/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0535 - acc: 0.9810 - val_loss: 0.0465 - val_acc: 0.9760\n",
      "Epoch 492/1500\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0536 - acc: 0.9790 - val_loss: 0.0466 - val_acc: 0.9760\n",
      "Epoch 493/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0532 - acc: 0.9810 - val_loss: 0.0464 - val_acc: 0.9760\n",
      "Epoch 494/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0537 - acc: 0.9800 - val_loss: 0.0471 - val_acc: 0.9760\n",
      "Epoch 495/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0543 - acc: 0.9800 - val_loss: 0.0460 - val_acc: 0.9790\n",
      "Epoch 496/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0535 - acc: 0.9790 - val_loss: 0.0462 - val_acc: 0.9760\n",
      "Epoch 497/1500\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0532 - acc: 0.9800 - val_loss: 0.0464 - val_acc: 0.9760\n",
      "Epoch 498/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0532 - acc: 0.9790 - val_loss: 0.0478 - val_acc: 0.9731\n",
      "Epoch 499/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0529 - acc: 0.9800 - val_loss: 0.0461 - val_acc: 0.9760\n",
      "Epoch 500/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0531 - acc: 0.9780 - val_loss: 0.0467 - val_acc: 0.9760\n",
      "Epoch 501/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0537 - acc: 0.9790 - val_loss: 0.0461 - val_acc: 0.9760\n",
      "Epoch 502/1500\n",
      "1002/1002 [==============================] - 0s 111us/step - loss: 0.0538 - acc: 0.9770 - val_loss: 0.0464 - val_acc: 0.9790\n",
      "Epoch 503/1500\n",
      "1002/1002 [==============================] - 0s 112us/step - loss: 0.0532 - acc: 0.9800 - val_loss: 0.0448 - val_acc: 0.9790\n",
      "Epoch 504/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0532 - acc: 0.9820 - val_loss: 0.0459 - val_acc: 0.9760\n",
      "Epoch 505/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0529 - acc: 0.9810 - val_loss: 0.0446 - val_acc: 0.9790\n",
      "Epoch 506/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0528 - acc: 0.9810 - val_loss: 0.0458 - val_acc: 0.9760\n",
      "Epoch 507/1500\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0525 - acc: 0.9820 - val_loss: 0.0457 - val_acc: 0.9760\n",
      "Epoch 508/1500\n",
      "1002/1002 [==============================] - 0s 115us/step - loss: 0.0536 - acc: 0.9800 - val_loss: 0.0458 - val_acc: 0.9760\n",
      "Epoch 509/1500\n",
      "1002/1002 [==============================] - 0s 128us/step - loss: 0.0527 - acc: 0.9800 - val_loss: 0.0456 - val_acc: 0.9760\n",
      "Epoch 510/1500\n",
      "1002/1002 [==============================] - 0s 112us/step - loss: 0.0522 - acc: 0.9790 - val_loss: 0.0460 - val_acc: 0.9760\n",
      "Epoch 511/1500\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0526 - acc: 0.9790 - val_loss: 0.0458 - val_acc: 0.9760\n",
      "Epoch 512/1500\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0524 - acc: 0.9800 - val_loss: 0.0447 - val_acc: 0.9760\n",
      "Epoch 513/1500\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0525 - acc: 0.9800 - val_loss: 0.0461 - val_acc: 0.9760\n",
      "Epoch 514/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0459 - val_acc: 0.9760\n",
      "Epoch 515/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0521 - acc: 0.9790 - val_loss: 0.0460 - val_acc: 0.9790\n",
      "Epoch 516/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0532 - acc: 0.9790 - val_loss: 0.0441 - val_acc: 0.9790\n",
      "Epoch 517/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0526 - acc: 0.9810 - val_loss: 0.0457 - val_acc: 0.9760\n",
      "Epoch 518/1500\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0522 - acc: 0.9790 - val_loss: 0.0456 - val_acc: 0.9760\n",
      "Epoch 519/1500\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0448 - val_acc: 0.9760\n",
      "Epoch 520/1500\n",
      "1002/1002 [==============================] - 0s 123us/step - loss: 0.0517 - acc: 0.9820 - val_loss: 0.0456 - val_acc: 0.9760\n",
      "Epoch 521/1500\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0448 - val_acc: 0.9760\n",
      "Epoch 522/1500\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0524 - acc: 0.9800 - val_loss: 0.0455 - val_acc: 0.9760\n",
      "Epoch 523/1500\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0522 - acc: 0.9830 - val_loss: 0.0442 - val_acc: 0.9790\n",
      "Epoch 524/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0517 - acc: 0.9810 - val_loss: 0.0450 - val_acc: 0.9760\n",
      "Epoch 525/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0449 - val_acc: 0.9760\n",
      "Epoch 526/1500\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0514 - acc: 0.9820 - val_loss: 0.0448 - val_acc: 0.9760\n",
      "Epoch 527/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0514 - acc: 0.9800 - val_loss: 0.0452 - val_acc: 0.9760\n",
      "Epoch 528/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0519 - acc: 0.9810 - val_loss: 0.0448 - val_acc: 0.9760\n",
      "Epoch 529/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0517 - acc: 0.9810 - val_loss: 0.0444 - val_acc: 0.9760\n",
      "Epoch 530/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0511 - acc: 0.9810 - val_loss: 0.0451 - val_acc: 0.9790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0515 - acc: 0.9800 - val_loss: 0.0450 - val_acc: 0.9760\n",
      "Epoch 532/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0512 - acc: 0.9810 - val_loss: 0.0447 - val_acc: 0.9760\n",
      "Epoch 533/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0513 - acc: 0.9800 - val_loss: 0.0446 - val_acc: 0.9760\n",
      "Epoch 534/1500\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 0.0522 - acc: 0.9820 - val_loss: 0.0438 - val_acc: 0.9790\n",
      "Epoch 535/1500\n",
      "1002/1002 [==============================] - 0s 165us/step - loss: 0.0514 - acc: 0.9800 - val_loss: 0.0445 - val_acc: 0.9760\n",
      "Epoch 536/1500\n",
      "1002/1002 [==============================] - 0s 147us/step - loss: 0.0511 - acc: 0.9830 - val_loss: 0.0439 - val_acc: 0.9760\n",
      "Epoch 537/1500\n",
      "1002/1002 [==============================] - 0s 155us/step - loss: 0.0516 - acc: 0.9810 - val_loss: 0.0439 - val_acc: 0.9760\n",
      "Epoch 538/1500\n",
      "1002/1002 [==============================] - 0s 138us/step - loss: 0.0525 - acc: 0.9810 - val_loss: 0.0435 - val_acc: 0.9790\n",
      "Epoch 539/1500\n",
      "1002/1002 [==============================] - 0s 151us/step - loss: 0.0510 - acc: 0.9810 - val_loss: 0.0450 - val_acc: 0.9760\n",
      "Epoch 540/1500\n",
      "1002/1002 [==============================] - 0s 144us/step - loss: 0.0507 - acc: 0.9810 - val_loss: 0.0443 - val_acc: 0.9760\n",
      "Epoch 541/1500\n",
      "1002/1002 [==============================] - 0s 122us/step - loss: 0.0515 - acc: 0.9800 - val_loss: 0.0439 - val_acc: 0.9790\n",
      "Epoch 542/1500\n",
      "1002/1002 [==============================] - 0s 136us/step - loss: 0.0509 - acc: 0.9810 - val_loss: 0.0441 - val_acc: 0.9760\n",
      "Epoch 543/1500\n",
      "1002/1002 [==============================] - 0s 124us/step - loss: 0.0508 - acc: 0.9800 - val_loss: 0.0441 - val_acc: 0.9760\n",
      "Epoch 544/1500\n",
      "1002/1002 [==============================] - 0s 123us/step - loss: 0.0506 - acc: 0.9810 - val_loss: 0.0439 - val_acc: 0.9760\n",
      "Epoch 545/1500\n",
      "1002/1002 [==============================] - 0s 140us/step - loss: 0.0509 - acc: 0.9800 - val_loss: 0.0442 - val_acc: 0.9760\n",
      "Epoch 546/1500\n",
      "1002/1002 [==============================] - 0s 123us/step - loss: 0.0508 - acc: 0.9810 - val_loss: 0.0442 - val_acc: 0.9760\n",
      "Epoch 547/1500\n",
      "1002/1002 [==============================] - 0s 141us/step - loss: 0.0506 - acc: 0.9830 - val_loss: 0.0430 - val_acc: 0.9790\n",
      "Epoch 548/1500\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0509 - acc: 0.9830 - val_loss: 0.0443 - val_acc: 0.9760\n",
      "Epoch 549/1500\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0505 - acc: 0.9800 - val_loss: 0.0441 - val_acc: 0.9760\n",
      "Epoch 550/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0503 - acc: 0.9810 - val_loss: 0.0437 - val_acc: 0.9760\n",
      "Epoch 551/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0510 - acc: 0.9820 - val_loss: 0.0434 - val_acc: 0.9760\n",
      "Epoch 552/1500\n",
      "1002/1002 [==============================] - 0s 116us/step - loss: 0.0508 - acc: 0.9790 - val_loss: 0.0436 - val_acc: 0.9760\n",
      "Epoch 553/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0509 - acc: 0.9840 - val_loss: 0.0432 - val_acc: 0.9790\n",
      "Epoch 554/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0501 - acc: 0.9810 - val_loss: 0.0435 - val_acc: 0.9760\n",
      "Epoch 555/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0505 - acc: 0.9830 - val_loss: 0.0433 - val_acc: 0.9790\n",
      "Epoch 556/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0500 - acc: 0.9820 - val_loss: 0.0430 - val_acc: 0.9790\n",
      "Epoch 557/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0501 - acc: 0.9820 - val_loss: 0.0436 - val_acc: 0.9760\n",
      "Epoch 558/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0500 - acc: 0.9810 - val_loss: 0.0434 - val_acc: 0.9760\n",
      "Epoch 559/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0498 - acc: 0.9840 - val_loss: 0.0429 - val_acc: 0.9790\n",
      "Epoch 560/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0502 - acc: 0.9820 - val_loss: 0.0435 - val_acc: 0.9760\n",
      "Epoch 561/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0501 - acc: 0.9820 - val_loss: 0.0437 - val_acc: 0.9790\n",
      "Epoch 562/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0499 - acc: 0.9820 - val_loss: 0.0433 - val_acc: 0.9760\n",
      "Epoch 563/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0494 - acc: 0.9830 - val_loss: 0.0422 - val_acc: 0.9790\n",
      "Epoch 564/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0498 - acc: 0.9810 - val_loss: 0.0431 - val_acc: 0.9790\n",
      "Epoch 565/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0499 - acc: 0.9840 - val_loss: 0.0425 - val_acc: 0.9790\n",
      "Epoch 566/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0494 - acc: 0.9830 - val_loss: 0.0423 - val_acc: 0.9820\n",
      "Epoch 567/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0495 - acc: 0.9810 - val_loss: 0.0433 - val_acc: 0.9760\n",
      "Epoch 568/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0498 - acc: 0.9820 - val_loss: 0.0433 - val_acc: 0.9760\n",
      "Epoch 569/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0494 - acc: 0.9800 - val_loss: 0.0430 - val_acc: 0.9760\n",
      "Epoch 570/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0493 - acc: 0.9840 - val_loss: 0.0425 - val_acc: 0.9790\n",
      "Epoch 571/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0498 - acc: 0.9820 - val_loss: 0.0429 - val_acc: 0.9760\n",
      "Epoch 572/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0499 - acc: 0.9840 - val_loss: 0.0419 - val_acc: 0.9820\n",
      "Epoch 573/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0496 - acc: 0.9820 - val_loss: 0.0431 - val_acc: 0.9760\n",
      "Epoch 574/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0489 - acc: 0.9830 - val_loss: 0.0426 - val_acc: 0.9790\n",
      "Epoch 575/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0494 - acc: 0.9820 - val_loss: 0.0424 - val_acc: 0.9790\n",
      "Epoch 576/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0490 - acc: 0.9830 - val_loss: 0.0423 - val_acc: 0.9790\n",
      "Epoch 577/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0496 - acc: 0.9830 - val_loss: 0.0421 - val_acc: 0.9790\n",
      "Epoch 578/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0501 - acc: 0.9840 - val_loss: 0.0420 - val_acc: 0.9820\n",
      "Epoch 579/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0494 - acc: 0.9820 - val_loss: 0.0433 - val_acc: 0.9760\n",
      "Epoch 580/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0487 - acc: 0.9840 - val_loss: 0.0416 - val_acc: 0.9820\n",
      "Epoch 581/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0494 - acc: 0.9820 - val_loss: 0.0423 - val_acc: 0.9790\n",
      "Epoch 582/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0486 - acc: 0.9840 - val_loss: 0.0425 - val_acc: 0.9820\n",
      "Epoch 583/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0488 - acc: 0.9830 - val_loss: 0.0420 - val_acc: 0.9790\n",
      "Epoch 584/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0491 - acc: 0.9820 - val_loss: 0.0421 - val_acc: 0.9790\n",
      "Epoch 585/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0494 - acc: 0.9800 - val_loss: 0.0418 - val_acc: 0.9790\n",
      "Epoch 586/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0486 - acc: 0.9840 - val_loss: 0.0416 - val_acc: 0.9820\n",
      "Epoch 587/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0487 - acc: 0.9830 - val_loss: 0.0425 - val_acc: 0.9760\n",
      "Epoch 588/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0420 - val_acc: 0.9790\n",
      "Epoch 589/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0491 - acc: 0.9820 - val_loss: 0.0416 - val_acc: 0.9820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1500\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0493 - acc: 0.9840 - val_loss: 0.0420 - val_acc: 0.9820\n",
      "Epoch 591/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0478 - acc: 0.9830 - val_loss: 0.0410 - val_acc: 0.9820\n",
      "Epoch 592/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0494 - acc: 0.9820 - val_loss: 0.0418 - val_acc: 0.9820\n",
      "Epoch 593/1500\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0479 - acc: 0.9820 - val_loss: 0.0418 - val_acc: 0.9790\n",
      "Epoch 594/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0481 - acc: 0.9840 - val_loss: 0.0414 - val_acc: 0.9820\n",
      "Epoch 595/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0484 - acc: 0.9820 - val_loss: 0.0420 - val_acc: 0.9790\n",
      "Epoch 596/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0479 - acc: 0.9820 - val_loss: 0.0412 - val_acc: 0.9820\n",
      "Epoch 597/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0476 - acc: 0.9840 - val_loss: 0.0413 - val_acc: 0.9820\n",
      "Epoch 598/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0482 - acc: 0.9820 - val_loss: 0.0420 - val_acc: 0.9790\n",
      "Epoch 599/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0483 - acc: 0.9840 - val_loss: 0.0410 - val_acc: 0.9820\n",
      "Epoch 600/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0489 - acc: 0.9820 - val_loss: 0.0410 - val_acc: 0.9820\n",
      "Epoch 601/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0488 - acc: 0.9780 - val_loss: 0.0411 - val_acc: 0.9820\n",
      "Epoch 602/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0477 - acc: 0.9840 - val_loss: 0.0410 - val_acc: 0.9820\n",
      "Epoch 603/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0474 - acc: 0.9830 - val_loss: 0.0409 - val_acc: 0.9820\n",
      "Epoch 604/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0486 - acc: 0.9810 - val_loss: 0.0406 - val_acc: 0.9820\n",
      "Epoch 605/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0473 - acc: 0.9840 - val_loss: 0.0407 - val_acc: 0.9880\n",
      "Epoch 606/1500\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0485 - acc: 0.9820 - val_loss: 0.0426 - val_acc: 0.9760\n",
      "Epoch 607/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0479 - acc: 0.9810 - val_loss: 0.0397 - val_acc: 0.9820\n",
      "Epoch 608/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0474 - acc: 0.9830 - val_loss: 0.0408 - val_acc: 0.9820\n",
      "Epoch 609/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0476 - acc: 0.9850 - val_loss: 0.0417 - val_acc: 0.9790\n",
      "Epoch 610/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0473 - acc: 0.9830 - val_loss: 0.0404 - val_acc: 0.9820\n",
      "Epoch 611/1500\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0477 - acc: 0.9820 - val_loss: 0.0411 - val_acc: 0.9820\n",
      "Epoch 612/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0470 - acc: 0.9860 - val_loss: 0.0406 - val_acc: 0.9820\n",
      "Epoch 613/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0471 - acc: 0.9840 - val_loss: 0.0396 - val_acc: 0.9820\n",
      "Epoch 614/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0471 - acc: 0.9840 - val_loss: 0.0409 - val_acc: 0.9820\n",
      "Epoch 615/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0470 - acc: 0.9860 - val_loss: 0.0405 - val_acc: 0.9820\n",
      "Epoch 616/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0470 - acc: 0.9820 - val_loss: 0.0405 - val_acc: 0.9820\n",
      "Epoch 617/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0471 - acc: 0.9840 - val_loss: 0.0404 - val_acc: 0.9820\n",
      "Epoch 618/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0473 - acc: 0.9820 - val_loss: 0.0406 - val_acc: 0.9820\n",
      "Epoch 619/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0470 - acc: 0.9850 - val_loss: 0.0401 - val_acc: 0.9820\n",
      "Epoch 620/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0467 - acc: 0.9830 - val_loss: 0.0405 - val_acc: 0.9850\n",
      "Epoch 621/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0473 - acc: 0.9840 - val_loss: 0.0401 - val_acc: 0.9820\n",
      "Epoch 622/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0473 - acc: 0.9840 - val_loss: 0.0397 - val_acc: 0.9820\n",
      "Epoch 623/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0468 - acc: 0.9850 - val_loss: 0.0406 - val_acc: 0.9820\n",
      "Epoch 624/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0473 - acc: 0.9840 - val_loss: 0.0400 - val_acc: 0.9820\n",
      "Epoch 625/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0467 - acc: 0.9830 - val_loss: 0.0402 - val_acc: 0.9820\n",
      "Epoch 626/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0460 - acc: 0.9840 - val_loss: 0.0405 - val_acc: 0.9820\n",
      "Epoch 627/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0466 - acc: 0.9850 - val_loss: 0.0399 - val_acc: 0.9820\n",
      "Epoch 628/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0462 - acc: 0.9860 - val_loss: 0.0400 - val_acc: 0.9850\n",
      "Epoch 629/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0461 - acc: 0.9840 - val_loss: 0.0396 - val_acc: 0.9820\n",
      "Epoch 630/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0465 - acc: 0.9860 - val_loss: 0.0395 - val_acc: 0.9820\n",
      "Epoch 631/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0463 - acc: 0.9830 - val_loss: 0.0399 - val_acc: 0.9820\n",
      "Epoch 632/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0467 - acc: 0.9860 - val_loss: 0.0392 - val_acc: 0.9820\n",
      "Epoch 633/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0463 - acc: 0.9840 - val_loss: 0.0391 - val_acc: 0.9820\n",
      "Epoch 634/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0465 - acc: 0.9840 - val_loss: 0.0398 - val_acc: 0.9820\n",
      "Epoch 635/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0464 - acc: 0.9830 - val_loss: 0.0392 - val_acc: 0.9820\n",
      "Epoch 636/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0462 - acc: 0.9840 - val_loss: 0.0395 - val_acc: 0.9820\n",
      "Epoch 637/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0463 - acc: 0.9850 - val_loss: 0.0397 - val_acc: 0.9820\n",
      "Epoch 638/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0465 - acc: 0.9830 - val_loss: 0.0395 - val_acc: 0.9820\n",
      "Epoch 639/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0457 - acc: 0.9850 - val_loss: 0.0389 - val_acc: 0.9820\n",
      "Epoch 640/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0456 - acc: 0.9830 - val_loss: 0.0389 - val_acc: 0.9820\n",
      "Epoch 641/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0460 - acc: 0.9840 - val_loss: 0.0393 - val_acc: 0.9820\n",
      "Epoch 642/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0464 - acc: 0.9830 - val_loss: 0.0394 - val_acc: 0.9820\n",
      "Epoch 643/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0465 - acc: 0.9840 - val_loss: 0.0382 - val_acc: 0.9820\n",
      "Epoch 644/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0455 - acc: 0.9840 - val_loss: 0.0391 - val_acc: 0.9820\n",
      "Epoch 645/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0456 - acc: 0.9850 - val_loss: 0.0401 - val_acc: 0.9820\n",
      "Epoch 646/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0459 - acc: 0.9830 - val_loss: 0.0386 - val_acc: 0.9820\n",
      "Epoch 647/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0454 - acc: 0.9850 - val_loss: 0.0388 - val_acc: 0.9820\n",
      "Epoch 648/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0459 - acc: 0.9850 - val_loss: 0.0386 - val_acc: 0.9820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0455 - acc: 0.9850 - val_loss: 0.0389 - val_acc: 0.9850\n",
      "Epoch 650/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0459 - acc: 0.9840 - val_loss: 0.0383 - val_acc: 0.9820\n",
      "Epoch 651/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0451 - acc: 0.9860 - val_loss: 0.0389 - val_acc: 0.9820\n",
      "Epoch 652/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0457 - acc: 0.9840 - val_loss: 0.0384 - val_acc: 0.9820\n",
      "Epoch 653/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0455 - acc: 0.9850 - val_loss: 0.0387 - val_acc: 0.9820\n",
      "Epoch 654/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0453 - acc: 0.9840 - val_loss: 0.0390 - val_acc: 0.9850\n",
      "Epoch 655/1500\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0447 - acc: 0.9860 - val_loss: 0.0388 - val_acc: 0.9820\n",
      "Epoch 656/1500\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0452 - acc: 0.9830 - val_loss: 0.0387 - val_acc: 0.9820\n",
      "Epoch 657/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0449 - acc: 0.9840 - val_loss: 0.0391 - val_acc: 0.9850\n",
      "Epoch 658/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0452 - acc: 0.9850 - val_loss: 0.0382 - val_acc: 0.9820\n",
      "Epoch 659/1500\n",
      "1002/1002 [==============================] - 0s 50us/step - loss: 0.0450 - acc: 0.9840 - val_loss: 0.0388 - val_acc: 0.9820\n",
      "Epoch 660/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0446 - acc: 0.9860 - val_loss: 0.0382 - val_acc: 0.9820\n",
      "Epoch 661/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0448 - acc: 0.9850 - val_loss: 0.0384 - val_acc: 0.9820\n",
      "Epoch 662/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0450 - acc: 0.9840 - val_loss: 0.0390 - val_acc: 0.9820\n",
      "Epoch 663/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0444 - acc: 0.9850 - val_loss: 0.0381 - val_acc: 0.9820\n",
      "Epoch 664/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0456 - acc: 0.9830 - val_loss: 0.0391 - val_acc: 0.9880\n",
      "Epoch 665/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0443 - acc: 0.9860 - val_loss: 0.0382 - val_acc: 0.9820\n",
      "Epoch 666/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0442 - acc: 0.9850 - val_loss: 0.0382 - val_acc: 0.9820\n",
      "Epoch 667/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0444 - acc: 0.9860 - val_loss: 0.0382 - val_acc: 0.9820\n",
      "Epoch 668/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0450 - acc: 0.9860 - val_loss: 0.0382 - val_acc: 0.9880\n",
      "Epoch 669/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0446 - acc: 0.9840 - val_loss: 0.0385 - val_acc: 0.9820\n",
      "Epoch 670/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0444 - acc: 0.9850 - val_loss: 0.0380 - val_acc: 0.9820\n",
      "Epoch 671/1500\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.0444 - acc: 0.9850 - val_loss: 0.0390 - val_acc: 0.9820\n",
      "Epoch 672/1500\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0441 - acc: 0.9840 - val_loss: 0.0388 - val_acc: 0.9820\n",
      "Epoch 673/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0437 - acc: 0.9860 - val_loss: 0.0375 - val_acc: 0.9820\n",
      "Epoch 674/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0440 - acc: 0.9830 - val_loss: 0.0380 - val_acc: 0.9820\n",
      "Epoch 675/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0435 - acc: 0.9860 - val_loss: 0.0375 - val_acc: 0.9820\n",
      "Epoch 676/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0443 - acc: 0.9860 - val_loss: 0.0377 - val_acc: 0.9820\n",
      "Epoch 677/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0433 - acc: 0.9860 - val_loss: 0.0377 - val_acc: 0.9820\n",
      "Epoch 678/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0436 - acc: 0.9860 - val_loss: 0.0377 - val_acc: 0.9820\n",
      "Epoch 679/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0432 - acc: 0.9860 - val_loss: 0.0376 - val_acc: 0.9850\n",
      "Epoch 680/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0432 - acc: 0.9860 - val_loss: 0.0378 - val_acc: 0.9850\n",
      "Epoch 681/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0432 - acc: 0.9860 - val_loss: 0.0371 - val_acc: 0.9820\n",
      "Epoch 682/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0434 - acc: 0.9850 - val_loss: 0.0378 - val_acc: 0.9820\n",
      "Epoch 683/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0430 - acc: 0.9860 - val_loss: 0.0376 - val_acc: 0.9820\n",
      "Epoch 684/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0429 - acc: 0.9860 - val_loss: 0.0379 - val_acc: 0.9820\n",
      "Epoch 685/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0431 - acc: 0.9860 - val_loss: 0.0375 - val_acc: 0.9820\n",
      "Epoch 686/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0427 - acc: 0.9860 - val_loss: 0.0376 - val_acc: 0.9820\n",
      "Epoch 687/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0428 - acc: 0.9860 - val_loss: 0.0374 - val_acc: 0.9820\n",
      "Epoch 688/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0431 - acc: 0.9860 - val_loss: 0.0384 - val_acc: 0.9850\n",
      "Epoch 689/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0431 - acc: 0.9860 - val_loss: 0.0378 - val_acc: 0.9820\n",
      "Epoch 690/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0426 - acc: 0.9860 - val_loss: 0.0375 - val_acc: 0.9850\n",
      "Epoch 691/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0428 - acc: 0.9850 - val_loss: 0.0371 - val_acc: 0.9820\n",
      "Epoch 692/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0430 - acc: 0.9830 - val_loss: 0.0390 - val_acc: 0.9820\n",
      "Epoch 693/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0430 - acc: 0.9850 - val_loss: 0.0366 - val_acc: 0.9820\n",
      "Epoch 694/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0427 - acc: 0.9860 - val_loss: 0.0375 - val_acc: 0.9850\n",
      "Epoch 695/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0428 - acc: 0.9830 - val_loss: 0.0378 - val_acc: 0.9820\n",
      "Epoch 696/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0423 - acc: 0.9860 - val_loss: 0.0372 - val_acc: 0.9820\n",
      "Epoch 697/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0428 - acc: 0.9850 - val_loss: 0.0374 - val_acc: 0.9820\n",
      "Epoch 698/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0421 - acc: 0.9850 - val_loss: 0.0372 - val_acc: 0.9820\n",
      "Epoch 699/1500\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0421 - acc: 0.9860 - val_loss: 0.0367 - val_acc: 0.9820\n",
      "Epoch 700/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0419 - acc: 0.9860 - val_loss: 0.0374 - val_acc: 0.9850\n",
      "Epoch 701/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0418 - acc: 0.9860 - val_loss: 0.0377 - val_acc: 0.9850\n",
      "Epoch 702/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0416 - acc: 0.9860 - val_loss: 0.0373 - val_acc: 0.9820\n",
      "Epoch 703/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0427 - acc: 0.9840 - val_loss: 0.0365 - val_acc: 0.9820\n",
      "Epoch 704/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0419 - acc: 0.9860 - val_loss: 0.0369 - val_acc: 0.9880\n",
      "Epoch 705/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0418 - acc: 0.9860 - val_loss: 0.0368 - val_acc: 0.9820\n",
      "Epoch 706/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0420 - acc: 0.9860 - val_loss: 0.0370 - val_acc: 0.9850\n",
      "Epoch 707/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0422 - acc: 0.9850 - val_loss: 0.0364 - val_acc: 0.9850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0420 - acc: 0.9850 - val_loss: 0.0366 - val_acc: 0.9850\n",
      "Epoch 709/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0419 - acc: 0.9860 - val_loss: 0.0376 - val_acc: 0.9820\n",
      "Epoch 710/1500\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.0424 - acc: 0.9850 - val_loss: 0.0361 - val_acc: 0.9850\n",
      "Epoch 711/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0424 - acc: 0.9850 - val_loss: 0.0368 - val_acc: 0.9820\n",
      "Epoch 712/1500\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0420 - acc: 0.9860 - val_loss: 0.0366 - val_acc: 0.9850\n",
      "Epoch 713/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0417 - acc: 0.9860 - val_loss: 0.0370 - val_acc: 0.9850\n",
      "Epoch 714/1500\n",
      "1002/1002 [==============================] - 0s 42us/step - loss: 0.0412 - acc: 0.9860 - val_loss: 0.0363 - val_acc: 0.9850\n",
      "Epoch 715/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0417 - acc: 0.9860 - val_loss: 0.0360 - val_acc: 0.9850\n",
      "Epoch 716/1500\n",
      "1002/1002 [==============================] - 0s 45us/step - loss: 0.0410 - acc: 0.9860 - val_loss: 0.0363 - val_acc: 0.9850\n",
      "Epoch 717/1500\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0413 - acc: 0.9860 - val_loss: 0.0360 - val_acc: 0.9850\n",
      "Epoch 718/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0409 - acc: 0.9860 - val_loss: 0.0361 - val_acc: 0.9850\n",
      "Epoch 719/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0411 - acc: 0.9860 - val_loss: 0.0368 - val_acc: 0.9850\n",
      "Epoch 720/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0411 - acc: 0.9860 - val_loss: 0.0364 - val_acc: 0.9820\n",
      "Epoch 721/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0415 - acc: 0.9850 - val_loss: 0.0358 - val_acc: 0.9850\n",
      "Epoch 722/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0409 - acc: 0.9860 - val_loss: 0.0372 - val_acc: 0.9850\n",
      "Epoch 723/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0412 - acc: 0.9860 - val_loss: 0.0364 - val_acc: 0.9850\n",
      "Epoch 724/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0405 - acc: 0.9860 - val_loss: 0.0357 - val_acc: 0.9880\n",
      "Epoch 725/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0409 - acc: 0.9860 - val_loss: 0.0362 - val_acc: 0.9850\n",
      "Epoch 726/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0412 - acc: 0.9870 - val_loss: 0.0369 - val_acc: 0.9820\n",
      "Epoch 727/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0408 - acc: 0.9860 - val_loss: 0.0360 - val_acc: 0.9850\n",
      "Epoch 728/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0409 - acc: 0.9860 - val_loss: 0.0363 - val_acc: 0.9850\n",
      "Epoch 729/1500\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.0404 - acc: 0.9860 - val_loss: 0.0361 - val_acc: 0.9850\n",
      "Epoch 730/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0413 - acc: 0.9870 - val_loss: 0.0362 - val_acc: 0.9850\n",
      "Epoch 731/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0409 - acc: 0.9860 - val_loss: 0.0367 - val_acc: 0.9880\n",
      "Epoch 732/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0412 - acc: 0.9860 - val_loss: 0.0361 - val_acc: 0.9850\n",
      "Epoch 733/1500\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.0406 - acc: 0.9860 - val_loss: 0.0357 - val_acc: 0.9850\n",
      "Epoch 734/1500\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0399 - acc: 0.9860 - val_loss: 0.0362 - val_acc: 0.9850\n",
      "Epoch 735/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0398 - acc: 0.9860 - val_loss: 0.0356 - val_acc: 0.9850\n",
      "Epoch 736/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0397 - acc: 0.9860 - val_loss: 0.0354 - val_acc: 0.9850\n",
      "Epoch 737/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0397 - acc: 0.9860 - val_loss: 0.0355 - val_acc: 0.9850\n",
      "Epoch 738/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0398 - acc: 0.9860 - val_loss: 0.0359 - val_acc: 0.9850\n",
      "Epoch 739/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0400 - acc: 0.9860 - val_loss: 0.0353 - val_acc: 0.9850\n",
      "Epoch 740/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0401 - acc: 0.9860 - val_loss: 0.0355 - val_acc: 0.9850\n",
      "Epoch 741/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0399 - acc: 0.9880 - val_loss: 0.0366 - val_acc: 0.9850\n",
      "Epoch 742/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0399 - acc: 0.9860 - val_loss: 0.0357 - val_acc: 0.9850\n",
      "Epoch 743/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0398 - acc: 0.9860 - val_loss: 0.0358 - val_acc: 0.9880\n",
      "Epoch 744/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0394 - acc: 0.9850 - val_loss: 0.0359 - val_acc: 0.9880\n",
      "Epoch 745/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0393 - acc: 0.9860 - val_loss: 0.0356 - val_acc: 0.9850\n",
      "Epoch 746/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0399 - acc: 0.9850 - val_loss: 0.0354 - val_acc: 0.9880\n",
      "Epoch 747/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0396 - acc: 0.9860 - val_loss: 0.0353 - val_acc: 0.9850\n",
      "Epoch 748/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0391 - acc: 0.9860 - val_loss: 0.0355 - val_acc: 0.9880\n",
      "Epoch 749/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0394 - acc: 0.9860 - val_loss: 0.0355 - val_acc: 0.9850\n",
      "Epoch 750/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0389 - acc: 0.9860 - val_loss: 0.0355 - val_acc: 0.9850\n",
      "Epoch 751/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0393 - acc: 0.9860 - val_loss: 0.0353 - val_acc: 0.9850\n",
      "Epoch 752/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0390 - acc: 0.9860 - val_loss: 0.0356 - val_acc: 0.9820\n",
      "Epoch 753/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0397 - acc: 0.9860 - val_loss: 0.0354 - val_acc: 0.9820\n",
      "Epoch 754/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0395 - acc: 0.9850 - val_loss: 0.0352 - val_acc: 0.9850\n",
      "Epoch 755/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0390 - acc: 0.9880 - val_loss: 0.0361 - val_acc: 0.9850\n",
      "Epoch 756/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0352 - val_acc: 0.9880\n",
      "Epoch 757/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0385 - acc: 0.9860 - val_loss: 0.0353 - val_acc: 0.9880\n",
      "Epoch 758/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0384 - acc: 0.9860 - val_loss: 0.0354 - val_acc: 0.9850\n",
      "Epoch 759/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0386 - acc: 0.9860 - val_loss: 0.0352 - val_acc: 0.9850\n",
      "Epoch 760/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0393 - acc: 0.9860 - val_loss: 0.0349 - val_acc: 0.9850\n",
      "Epoch 761/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0390 - acc: 0.9860 - val_loss: 0.0357 - val_acc: 0.9880\n",
      "Epoch 762/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0379 - acc: 0.9860 - val_loss: 0.0345 - val_acc: 0.9850\n",
      "Epoch 763/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0382 - acc: 0.9860 - val_loss: 0.0347 - val_acc: 0.9850\n",
      "Epoch 764/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0383 - acc: 0.9860 - val_loss: 0.0349 - val_acc: 0.9850\n",
      "Epoch 765/1500\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0380 - acc: 0.9860 - val_loss: 0.0352 - val_acc: 0.9880\n",
      "Epoch 766/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0387 - acc: 0.9860 - val_loss: 0.0345 - val_acc: 0.9850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1500\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0386 - acc: 0.9870 - val_loss: 0.0351 - val_acc: 0.9850\n",
      "Epoch 768/1500\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0384 - acc: 0.9870 - val_loss: 0.0349 - val_acc: 0.9850\n",
      "Epoch 769/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0378 - acc: 0.9860 - val_loss: 0.0351 - val_acc: 0.9850\n",
      "Epoch 770/1500\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.0381 - acc: 0.9860 - val_loss: 0.0349 - val_acc: 0.9850\n",
      "Epoch 771/1500\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0381 - acc: 0.9880 - val_loss: 0.0347 - val_acc: 0.9850\n",
      "Epoch 772/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0384 - acc: 0.9880 - val_loss: 0.0352 - val_acc: 0.9850\n",
      "Epoch 773/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0347 - val_acc: 0.9880\n",
      "Epoch 774/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0379 - acc: 0.9870 - val_loss: 0.0348 - val_acc: 0.9850\n",
      "Epoch 775/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0378 - acc: 0.9860 - val_loss: 0.0349 - val_acc: 0.9880\n",
      "Epoch 776/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0373 - acc: 0.9860 - val_loss: 0.0345 - val_acc: 0.9850\n",
      "Epoch 777/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0380 - acc: 0.9880 - val_loss: 0.0346 - val_acc: 0.9850\n",
      "Epoch 778/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0374 - acc: 0.9880 - val_loss: 0.0344 - val_acc: 0.9850\n",
      "Epoch 779/1500\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0378 - acc: 0.9870 - val_loss: 0.0347 - val_acc: 0.9880\n",
      "Epoch 780/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0375 - acc: 0.9860 - val_loss: 0.0347 - val_acc: 0.9880\n",
      "Epoch 781/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0376 - acc: 0.9860 - val_loss: 0.0349 - val_acc: 0.9880\n",
      "Epoch 782/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0372 - acc: 0.9880 - val_loss: 0.0343 - val_acc: 0.9850\n",
      "Epoch 783/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0382 - acc: 0.9870 - val_loss: 0.0340 - val_acc: 0.9850\n",
      "Epoch 784/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0382 - acc: 0.9860 - val_loss: 0.0350 - val_acc: 0.9850\n",
      "Epoch 785/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0370 - acc: 0.9880 - val_loss: 0.0342 - val_acc: 0.9850\n",
      "Epoch 786/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0371 - acc: 0.9880 - val_loss: 0.0342 - val_acc: 0.9850\n",
      "Epoch 787/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0383 - acc: 0.9870 - val_loss: 0.0342 - val_acc: 0.9850\n",
      "Epoch 788/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0372 - acc: 0.9860 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 789/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0370 - acc: 0.9890 - val_loss: 0.0349 - val_acc: 0.9850\n",
      "Epoch 790/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0369 - acc: 0.9870 - val_loss: 0.0341 - val_acc: 0.9850\n",
      "Epoch 791/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.0341 - val_acc: 0.9850\n",
      "Epoch 792/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.0341 - val_acc: 0.9880\n",
      "Epoch 793/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0377 - acc: 0.9880 - val_loss: 0.0348 - val_acc: 0.9880\n",
      "Epoch 794/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0370 - acc: 0.9880 - val_loss: 0.0343 - val_acc: 0.9850\n",
      "Epoch 795/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0365 - acc: 0.9890 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 796/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0364 - acc: 0.9870 - val_loss: 0.0338 - val_acc: 0.9850\n",
      "Epoch 797/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0364 - acc: 0.9870 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 798/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0363 - acc: 0.9860 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 799/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0364 - acc: 0.9880 - val_loss: 0.0341 - val_acc: 0.9880\n",
      "Epoch 800/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0363 - acc: 0.9870 - val_loss: 0.0339 - val_acc: 0.9880\n",
      "Epoch 801/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0368 - acc: 0.9870 - val_loss: 0.0341 - val_acc: 0.9880\n",
      "Epoch 802/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0360 - acc: 0.9890 - val_loss: 0.0339 - val_acc: 0.9880\n",
      "Epoch 803/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0360 - acc: 0.9870 - val_loss: 0.0340 - val_acc: 0.9880\n",
      "Epoch 804/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0360 - acc: 0.9860 - val_loss: 0.0338 - val_acc: 0.9880\n",
      "Epoch 805/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0370 - acc: 0.9900 - val_loss: 0.0339 - val_acc: 0.9850\n",
      "Epoch 806/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0362 - acc: 0.9880 - val_loss: 0.0340 - val_acc: 0.9880\n",
      "Epoch 807/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0361 - acc: 0.9910 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 808/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0366 - acc: 0.9850 - val_loss: 0.0338 - val_acc: 0.9880\n",
      "Epoch 809/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0370 - acc: 0.9890 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 810/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0366 - acc: 0.9890 - val_loss: 0.0342 - val_acc: 0.9880\n",
      "Epoch 811/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0369 - acc: 0.9850 - val_loss: 0.0341 - val_acc: 0.9880\n",
      "Epoch 812/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0359 - acc: 0.9890 - val_loss: 0.0341 - val_acc: 0.9850\n",
      "Epoch 813/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0361 - acc: 0.9900 - val_loss: 0.0337 - val_acc: 0.9880\n",
      "Epoch 814/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0358 - acc: 0.9880 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 815/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0359 - acc: 0.9900 - val_loss: 0.0341 - val_acc: 0.9850\n",
      "Epoch 816/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0357 - acc: 0.9890 - val_loss: 0.0336 - val_acc: 0.9850\n",
      "Epoch 817/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0355 - acc: 0.9890 - val_loss: 0.0340 - val_acc: 0.9880\n",
      "Epoch 818/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0355 - acc: 0.9890 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 819/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0353 - acc: 0.9890 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 820/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0356 - acc: 0.9890 - val_loss: 0.0338 - val_acc: 0.9880\n",
      "Epoch 821/1500\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0355 - acc: 0.9900 - val_loss: 0.0337 - val_acc: 0.9880\n",
      "Epoch 822/1500\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.0294 - acc: 1.000 - 0s 64us/step - loss: 0.0355 - acc: 0.9890 - val_loss: 0.0335 - val_acc: 0.9880\n",
      "Epoch 823/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0356 - acc: 0.9880 - val_loss: 0.0339 - val_acc: 0.9850\n",
      "Epoch 824/1500\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0348 - acc: 0.9900 - val_loss: 0.0335 - val_acc: 0.9880\n",
      "Epoch 825/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0357 - acc: 0.9870 - val_loss: 0.0338 - val_acc: 0.9880\n",
      "Epoch 826/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0359 - acc: 0.9910 - val_loss: 0.0339 - val_acc: 0.9880\n",
      "Epoch 827/1500\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0358 - acc: 0.9910 - val_loss: 0.0337 - val_acc: 0.9880\n",
      "Epoch 828/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0355 - acc: 0.9870 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "Epoch 829/1500\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "Epoch 830/1500\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0346 - acc: 0.9900 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 831/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0350 - acc: 0.9880 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 832/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0347 - acc: 0.9870 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 833/1500\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0348 - acc: 0.9910 - val_loss: 0.0337 - val_acc: 0.9850\n",
      "Epoch 834/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0347 - acc: 0.9910 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 835/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0348 - acc: 0.9870 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "Epoch 836/1500\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0346 - acc: 0.9910 - val_loss: 0.0335 - val_acc: 0.9880\n",
      "Epoch 837/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0351 - acc: 0.9880 - val_loss: 0.0337 - val_acc: 0.9850\n",
      "Epoch 838/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0352 - acc: 0.9900 - val_loss: 0.0335 - val_acc: 0.9880\n",
      "Epoch 839/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0348 - acc: 0.9880 - val_loss: 0.0335 - val_acc: 0.9880\n",
      "Epoch 840/1500\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0343 - acc: 0.9910 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 841/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0343 - acc: 0.9880 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "Epoch 842/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0344 - acc: 0.9910 - val_loss: 0.0336 - val_acc: 0.9880\n",
      "Epoch 843/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0342 - acc: 0.9890 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "Epoch 844/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0340 - acc: 0.9920 - val_loss: 0.0337 - val_acc: 0.9850\n",
      "Epoch 845/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0347 - acc: 0.9920 - val_loss: 0.0340 - val_acc: 0.9880\n",
      "Epoch 846/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0340 - acc: 0.9900 - val_loss: 0.0333 - val_acc: 0.9880\n",
      "Epoch 847/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0338 - acc: 0.9890 - val_loss: 0.0338 - val_acc: 0.9880\n",
      "Epoch 848/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0338 - acc: 0.9900 - val_loss: 0.0345 - val_acc: 0.9850\n",
      "Epoch 849/1500\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0336 - acc: 0.9920 - val_loss: 0.0341 - val_acc: 0.9880\n",
      "Epoch 850/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0341 - acc: 0.9900 - val_loss: 0.0338 - val_acc: 0.9880\n",
      "Epoch 851/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0345 - acc: 0.9900 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 852/1500\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0337 - acc: 0.9920 - val_loss: 0.0342 - val_acc: 0.9880\n",
      "Epoch 853/1500\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0338 - acc: 0.9910 - val_loss: 0.0342 - val_acc: 0.9880\n",
      "Epoch 854/1500\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0334 - acc: 0.9890 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 855/1500\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0336 - acc: 0.9900 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 856/1500\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0334 - acc: 0.9920 - val_loss: 0.0341 - val_acc: 0.9880\n",
      "Epoch 857/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0339 - acc: 0.9880 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 858/1500\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 859/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0335 - acc: 0.9930 - val_loss: 0.0341 - val_acc: 0.9850\n",
      "Epoch 860/1500\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0338 - acc: 0.9930 - val_loss: 0.0349 - val_acc: 0.9880\n",
      "Epoch 861/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0330 - acc: 0.9910 - val_loss: 0.0345 - val_acc: 0.9880\n",
      "Epoch 862/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0331 - acc: 0.9910 - val_loss: 0.0345 - val_acc: 0.9880\n",
      "Epoch 863/1500\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0333 - acc: 0.9910 - val_loss: 0.0343 - val_acc: 0.9850\n",
      "Epoch 864/1500\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0328 - acc: 0.9910 - val_loss: 0.0342 - val_acc: 0.9880\n",
      "Epoch 865/1500\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0331 - acc: 0.9920 - val_loss: 0.0342 - val_acc: 0.9880\n",
      "Epoch 866/1500\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0328 - acc: 0.9920 - val_loss: 0.0344 - val_acc: 0.9880\n",
      "Epoch 00866: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(29, input_shape=(dims,)))\n",
    "m.add(Activation('softmax'))\n",
    "m.add(Dense(20))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adam,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1500, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/3000\n",
      "1002/1002 [==============================] - 1s 541us/step - loss: 1.8280 - acc: 0.3184 - val_loss: 1.7046 - val_acc: 0.2994\n",
      "Epoch 2/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 1.6689 - acc: 0.3403 - val_loss: 1.5747 - val_acc: 0.5359\n",
      "Epoch 3/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 1.5143 - acc: 0.5329 - val_loss: 1.3982 - val_acc: 0.5509\n",
      "Epoch 4/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 1.3285 - acc: 0.5329 - val_loss: 1.2221 - val_acc: 0.5509\n",
      "Epoch 5/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 1.1771 - acc: 0.5339 - val_loss: 1.0961 - val_acc: 0.5509\n",
      "Epoch 6/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 1.0582 - acc: 0.5339 - val_loss: 0.9897 - val_acc: 0.5539\n",
      "Epoch 7/3000\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 0.9634 - acc: 0.5369 - val_loss: 0.8982 - val_acc: 0.5838\n",
      "Epoch 8/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.8715 - acc: 0.6218 - val_loss: 0.8199 - val_acc: 0.6228\n",
      "Epoch 9/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.7801 - acc: 0.6467 - val_loss: 0.7288 - val_acc: 0.7305\n",
      "Epoch 10/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.6883 - acc: 0.7216 - val_loss: 0.6413 - val_acc: 0.8623\n",
      "Epoch 11/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.6114 - acc: 0.7864 - val_loss: 0.5689 - val_acc: 0.7964\n",
      "Epoch 12/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.5327 - acc: 0.8513 - val_loss: 0.5013 - val_acc: 0.8862\n",
      "Epoch 13/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.4694 - acc: 0.8892 - val_loss: 0.4361 - val_acc: 0.9461\n",
      "Epoch 14/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.4127 - acc: 0.9242 - val_loss: 0.3775 - val_acc: 0.9551\n",
      "Epoch 15/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.3648 - acc: 0.9451 - val_loss: 0.3450 - val_acc: 0.9371\n",
      "Epoch 16/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.3278 - acc: 0.9331 - val_loss: 0.2999 - val_acc: 0.9671\n",
      "Epoch 17/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.2952 - acc: 0.9431 - val_loss: 0.2722 - val_acc: 0.9581\n",
      "Epoch 18/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.2697 - acc: 0.9461 - val_loss: 0.2432 - val_acc: 0.9551\n",
      "Epoch 19/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.2436 - acc: 0.9571 - val_loss: 0.2238 - val_acc: 0.9641\n",
      "Epoch 20/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.2284 - acc: 0.9581 - val_loss: 0.2161 - val_acc: 0.9611\n",
      "Epoch 21/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.2105 - acc: 0.9631 - val_loss: 0.2079 - val_acc: 0.9551\n",
      "Epoch 22/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.1952 - acc: 0.9631 - val_loss: 0.1872 - val_acc: 0.9551\n",
      "Epoch 23/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.1889 - acc: 0.9581 - val_loss: 0.1803 - val_acc: 0.9581\n",
      "Epoch 24/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.1686 - acc: 0.964 - 0s 92us/step - loss: 0.1787 - acc: 0.9561 - val_loss: 0.1704 - val_acc: 0.9551\n",
      "Epoch 25/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1700 - acc: 0.9601 - val_loss: 0.1556 - val_acc: 0.9581\n",
      "Epoch 26/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1593 - acc: 0.9611 - val_loss: 0.1476 - val_acc: 0.9581\n",
      "Epoch 27/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.1548 - acc: 0.9591 - val_loss: 0.1503 - val_acc: 0.9611\n",
      "Epoch 28/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.1452 - acc: 0.9611 - val_loss: 0.1441 - val_acc: 0.9611\n",
      "Epoch 29/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.1432 - acc: 0.9631 - val_loss: 0.1381 - val_acc: 0.9581\n",
      "Epoch 30/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.1377 - acc: 0.9651 - val_loss: 0.1301 - val_acc: 0.9671\n",
      "Epoch 31/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.1321 - acc: 0.9641 - val_loss: 0.1273 - val_acc: 0.9581\n",
      "Epoch 32/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1260 - acc: 0.9651 - val_loss: 0.1225 - val_acc: 0.9641\n",
      "Epoch 33/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.1251 - acc: 0.9631 - val_loss: 0.1198 - val_acc: 0.9611\n",
      "Epoch 34/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.1188 - acc: 0.9671 - val_loss: 0.1104 - val_acc: 0.9760\n",
      "Epoch 35/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.1178 - acc: 0.9661 - val_loss: 0.1141 - val_acc: 0.9641\n",
      "Epoch 36/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1126 - acc: 0.9671 - val_loss: 0.1045 - val_acc: 0.9731\n",
      "Epoch 37/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.1148 - acc: 0.9601 - val_loss: 0.1091 - val_acc: 0.9641\n",
      "Epoch 38/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1115 - acc: 0.9641 - val_loss: 0.1117 - val_acc: 0.9641\n",
      "Epoch 39/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.1088 - acc: 0.9651 - val_loss: 0.1070 - val_acc: 0.9521\n",
      "Epoch 40/3000\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.1085 - acc: 0.9611 - val_loss: 0.1049 - val_acc: 0.9671\n",
      "Epoch 41/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1040 - acc: 0.9701 - val_loss: 0.1005 - val_acc: 0.9671\n",
      "Epoch 42/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.1022 - acc: 0.9711 - val_loss: 0.1064 - val_acc: 0.9701\n",
      "Epoch 43/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0999 - acc: 0.9641 - val_loss: 0.0980 - val_acc: 0.9581\n",
      "Epoch 44/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.1004 - acc: 0.9681 - val_loss: 0.0961 - val_acc: 0.9671\n",
      "Epoch 45/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0965 - acc: 0.9681 - val_loss: 0.0899 - val_acc: 0.9641\n",
      "Epoch 46/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0939 - acc: 0.9681 - val_loss: 0.0998 - val_acc: 0.9731\n",
      "Epoch 47/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0929 - acc: 0.9701 - val_loss: 0.0958 - val_acc: 0.9641\n",
      "Epoch 48/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.0927 - acc: 0.9681 - val_loss: 0.0882 - val_acc: 0.9731\n",
      "Epoch 49/3000\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0870 - acc: 0.9711 - val_loss: 0.0894 - val_acc: 0.9701\n",
      "Epoch 50/3000\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0918 - acc: 0.9721 - val_loss: 0.0825 - val_acc: 0.9701\n",
      "Epoch 51/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0915 - acc: 0.9701 - val_loss: 0.0870 - val_acc: 0.9581\n",
      "Epoch 52/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0915 - acc: 0.9671 - val_loss: 0.0861 - val_acc: 0.9611\n",
      "Epoch 53/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0840 - acc: 0.9760 - val_loss: 0.0826 - val_acc: 0.9701\n",
      "Epoch 54/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0821 - acc: 0.9790 - val_loss: 0.1004 - val_acc: 0.9671\n",
      "Epoch 55/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0875 - acc: 0.9681 - val_loss: 0.0929 - val_acc: 0.9701\n",
      "Epoch 56/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0877 - acc: 0.9701 - val_loss: 0.0856 - val_acc: 0.9701\n",
      "Epoch 57/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.0877 - acc: 0.9701 - val_loss: 0.0859 - val_acc: 0.9701\n",
      "Epoch 58/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0831 - acc: 0.9711 - val_loss: 0.0923 - val_acc: 0.9671\n",
      "Epoch 59/3000\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0823 - acc: 0.9721 - val_loss: 0.0769 - val_acc: 0.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.0845 - acc: 0.9711 - val_loss: 0.0769 - val_acc: 0.9641\n",
      "Epoch 61/3000\n",
      "1002/1002 [==============================] - 0s 50us/step - loss: 0.0828 - acc: 0.9721 - val_loss: 0.0848 - val_acc: 0.9671\n",
      "Epoch 62/3000\n",
      "1002/1002 [==============================] - 0s 40us/step - loss: 0.0815 - acc: 0.9671 - val_loss: 0.0783 - val_acc: 0.9611\n",
      "Epoch 63/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0819 - acc: 0.9681 - val_loss: 0.0799 - val_acc: 0.9671\n",
      "Epoch 64/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0785 - acc: 0.9741 - val_loss: 0.0877 - val_acc: 0.9671\n",
      "Epoch 65/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0798 - acc: 0.9760 - val_loss: 0.0792 - val_acc: 0.9701\n",
      "Epoch 66/3000\n",
      "1002/1002 [==============================] - 0s 41us/step - loss: 0.0818 - acc: 0.9731 - val_loss: 0.0866 - val_acc: 0.9701\n",
      "Epoch 67/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0777 - acc: 0.9770 - val_loss: 0.0736 - val_acc: 0.9581\n",
      "Epoch 68/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0786 - acc: 0.9651 - val_loss: 0.0738 - val_acc: 0.9671\n",
      "Epoch 69/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0773 - acc: 0.9750 - val_loss: 0.0764 - val_acc: 0.9671\n",
      "Epoch 70/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.0806 - acc: 0.9701 - val_loss: 0.0750 - val_acc: 0.9731\n",
      "Epoch 71/3000\n",
      "1002/1002 [==============================] - 0s 49us/step - loss: 0.0754 - acc: 0.9731 - val_loss: 0.0914 - val_acc: 0.9641\n",
      "Epoch 72/3000\n",
      "1002/1002 [==============================] - 0s 39us/step - loss: 0.0814 - acc: 0.9691 - val_loss: 0.0725 - val_acc: 0.9671\n",
      "Epoch 73/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0748 - acc: 0.9711 - val_loss: 0.0770 - val_acc: 0.9581\n",
      "Epoch 74/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0785 - acc: 0.9741 - val_loss: 0.0718 - val_acc: 0.9701\n",
      "Epoch 75/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0799 - acc: 0.9711 - val_loss: 0.0708 - val_acc: 0.9701\n",
      "Epoch 76/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0781 - acc: 0.9701 - val_loss: 0.0877 - val_acc: 0.9671\n",
      "Epoch 77/3000\n",
      "1002/1002 [==============================] - 0s 45us/step - loss: 0.0802 - acc: 0.9721 - val_loss: 0.0880 - val_acc: 0.9641\n",
      "Epoch 78/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0780 - acc: 0.9741 - val_loss: 0.0738 - val_acc: 0.9611\n",
      "Epoch 79/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0749 - acc: 0.9770 - val_loss: 0.0674 - val_acc: 0.9731\n",
      "Epoch 80/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0725 - acc: 0.9741 - val_loss: 0.0854 - val_acc: 0.9671\n",
      "Epoch 81/3000\n",
      "1002/1002 [==============================] - 0s 35us/step - loss: 0.0784 - acc: 0.9711 - val_loss: 0.0749 - val_acc: 0.9731\n",
      "Epoch 82/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0743 - acc: 0.9731 - val_loss: 0.0844 - val_acc: 0.9701\n",
      "Epoch 83/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0734 - acc: 0.9721 - val_loss: 0.0747 - val_acc: 0.9731\n",
      "Epoch 84/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0766 - acc: 0.9731 - val_loss: 0.0718 - val_acc: 0.9701\n",
      "Epoch 85/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0744 - acc: 0.9731 - val_loss: 0.0848 - val_acc: 0.9641\n",
      "Epoch 86/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0700 - acc: 0.9721 - val_loss: 0.0847 - val_acc: 0.9731\n",
      "Epoch 87/3000\n",
      "1002/1002 [==============================] - 0s 38us/step - loss: 0.0768 - acc: 0.9760 - val_loss: 0.0734 - val_acc: 0.9671\n",
      "Epoch 88/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0710 - acc: 0.9731 - val_loss: 0.0638 - val_acc: 0.9731\n",
      "Epoch 89/3000\n",
      "1002/1002 [==============================] - 0s 50us/step - loss: 0.0762 - acc: 0.9691 - val_loss: 0.0775 - val_acc: 0.9701\n",
      "Epoch 90/3000\n",
      "1002/1002 [==============================] - 0s 36us/step - loss: 0.0731 - acc: 0.9731 - val_loss: 0.0699 - val_acc: 0.9731\n",
      "Epoch 91/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0698 - acc: 0.9770 - val_loss: 0.1045 - val_acc: 0.9551\n",
      "Epoch 92/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0789 - acc: 0.9681 - val_loss: 0.0643 - val_acc: 0.9760\n",
      "Epoch 93/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0733 - acc: 0.9721 - val_loss: 0.0661 - val_acc: 0.9671\n",
      "Epoch 94/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0714 - acc: 0.9741 - val_loss: 0.0902 - val_acc: 0.9731\n",
      "Epoch 95/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0763 - acc: 0.9701 - val_loss: 0.0706 - val_acc: 0.9731\n",
      "Epoch 96/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0762 - acc: 0.9691 - val_loss: 0.0862 - val_acc: 0.9701\n",
      "Epoch 97/3000\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0720 - acc: 0.9760 - val_loss: 0.0704 - val_acc: 0.9701\n",
      "Epoch 98/3000\n",
      "1002/1002 [==============================] - 0s 43us/step - loss: 0.0729 - acc: 0.9741 - val_loss: 0.0815 - val_acc: 0.9671\n",
      "Epoch 99/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0716 - acc: 0.9731 - val_loss: 0.0712 - val_acc: 0.9731\n",
      "Epoch 100/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0682 - acc: 0.9731 - val_loss: 0.0749 - val_acc: 0.9671\n",
      "Epoch 101/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0671 - acc: 0.9770 - val_loss: 0.0939 - val_acc: 0.9731\n",
      "Epoch 102/3000\n",
      "1002/1002 [==============================] - 0s 34us/step - loss: 0.0737 - acc: 0.9721 - val_loss: 0.0745 - val_acc: 0.9671\n",
      "Epoch 103/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0677 - acc: 0.9750 - val_loss: 0.0728 - val_acc: 0.9641\n",
      "Epoch 104/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.0768 - acc: 0.9701 - val_loss: 0.0681 - val_acc: 0.9731\n",
      "Epoch 105/3000\n",
      "1002/1002 [==============================] - 0s 41us/step - loss: 0.0695 - acc: 0.9770 - val_loss: 0.0804 - val_acc: 0.9611\n",
      "Epoch 106/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0660 - acc: 0.9741 - val_loss: 0.0703 - val_acc: 0.9701\n",
      "Epoch 107/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0677 - acc: 0.9760 - val_loss: 0.0611 - val_acc: 0.9731\n",
      "Epoch 108/3000\n",
      "1002/1002 [==============================] - 0s 49us/step - loss: 0.0699 - acc: 0.9701 - val_loss: 0.0637 - val_acc: 0.9731\n",
      "Epoch 109/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0683 - acc: 0.9760 - val_loss: 0.0716 - val_acc: 0.9731\n",
      "Epoch 110/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0652 - acc: 0.9731 - val_loss: 0.0660 - val_acc: 0.9760\n",
      "Epoch 111/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0682 - acc: 0.9770 - val_loss: 0.0759 - val_acc: 0.9701\n",
      "Epoch 112/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0701 - acc: 0.9760 - val_loss: 0.0700 - val_acc: 0.9731\n",
      "Epoch 113/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0693 - acc: 0.9741 - val_loss: 0.0638 - val_acc: 0.9671\n",
      "Epoch 114/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0711 - acc: 0.9760 - val_loss: 0.0664 - val_acc: 0.9701\n",
      "Epoch 115/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0642 - acc: 0.9760 - val_loss: 0.0787 - val_acc: 0.9731\n",
      "Epoch 116/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.0710 - val_acc: 0.9701\n",
      "Epoch 117/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0705 - acc: 0.9731 - val_loss: 0.0844 - val_acc: 0.9641\n",
      "Epoch 118/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0676 - acc: 0.9701 - val_loss: 0.0698 - val_acc: 0.9671\n",
      "Epoch 119/3000\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.0654 - acc: 0.9780 - val_loss: 0.0946 - val_acc: 0.9671\n",
      "Epoch 120/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0702 - acc: 0.9760 - val_loss: 0.0656 - val_acc: 0.9701\n",
      "Epoch 121/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0644 - acc: 0.9780 - val_loss: 0.0822 - val_acc: 0.9611\n",
      "Epoch 122/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0670 - acc: 0.9810 - val_loss: 0.0736 - val_acc: 0.9731\n",
      "Epoch 123/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0710 - acc: 0.9770 - val_loss: 0.0690 - val_acc: 0.9671\n",
      "Epoch 124/3000\n",
      "1002/1002 [==============================] - 0s 40us/step - loss: 0.0685 - acc: 0.9721 - val_loss: 0.0650 - val_acc: 0.9701\n",
      "Epoch 125/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0650 - acc: 0.9790 - val_loss: 0.0617 - val_acc: 0.9731\n",
      "Epoch 126/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0632 - acc: 0.9760 - val_loss: 0.1003 - val_acc: 0.9521\n",
      "Epoch 127/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0890 - acc: 0.9671 - val_loss: 0.0763 - val_acc: 0.9701\n",
      "Epoch 00127: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist2=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/3000\n",
      "1002/1002 [==============================] - 1s 718us/step - loss: 1.7960 - acc: 0.2934 - val_loss: 1.7030 - val_acc: 0.2994\n",
      "Epoch 2/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 1.6666 - acc: 0.2924 - val_loss: 1.6068 - val_acc: 0.2994\n",
      "Epoch 3/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 1.5776 - acc: 0.2974 - val_loss: 1.5192 - val_acc: 0.4192\n",
      "Epoch 4/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 1.4760 - acc: 0.4790 - val_loss: 1.4060 - val_acc: 0.5449\n",
      "Epoch 5/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 1.3590 - acc: 0.5309 - val_loss: 1.2917 - val_acc: 0.5539\n",
      "Epoch 6/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 1.2440 - acc: 0.5379 - val_loss: 1.1810 - val_acc: 0.5569\n",
      "Epoch 7/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 1.1307 - acc: 0.5399 - val_loss: 1.0776 - val_acc: 0.5599\n",
      "Epoch 8/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 1.0267 - acc: 0.5469 - val_loss: 0.9780 - val_acc: 0.5599\n",
      "Epoch 9/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.9308 - acc: 0.5579 - val_loss: 0.8919 - val_acc: 0.6198\n",
      "Epoch 10/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.8397 - acc: 0.6148 - val_loss: 0.8102 - val_acc: 0.6407\n",
      "Epoch 11/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.7579 - acc: 0.6826 - val_loss: 0.7291 - val_acc: 0.6617\n",
      "Epoch 12/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.6817 - acc: 0.7385 - val_loss: 0.6602 - val_acc: 0.7246\n",
      "Epoch 13/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.6134 - acc: 0.8134 - val_loss: 0.5936 - val_acc: 0.8982\n",
      "Epoch 14/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.5526 - acc: 0.8912 - val_loss: 0.5349 - val_acc: 0.9401\n",
      "Epoch 15/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.4961 - acc: 0.9351 - val_loss: 0.4794 - val_acc: 0.9641\n",
      "Epoch 16/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.4471 - acc: 0.9491 - val_loss: 0.4356 - val_acc: 0.9431\n",
      "Epoch 17/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.4036 - acc: 0.9561 - val_loss: 0.3931 - val_acc: 0.9581\n",
      "Epoch 18/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.3657 - acc: 0.9581 - val_loss: 0.3557 - val_acc: 0.9611\n",
      "Epoch 19/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.3311 - acc: 0.9621 - val_loss: 0.3247 - val_acc: 0.9701\n",
      "Epoch 20/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.3020 - acc: 0.9691 - val_loss: 0.2985 - val_acc: 0.9701\n",
      "Epoch 21/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.2764 - acc: 0.9651 - val_loss: 0.2739 - val_acc: 0.9701\n",
      "Epoch 22/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.2536 - acc: 0.9671 - val_loss: 0.2548 - val_acc: 0.9641\n",
      "Epoch 23/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.2334 - acc: 0.9681 - val_loss: 0.2367 - val_acc: 0.9701\n",
      "Epoch 24/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.2157 - acc: 0.9750 - val_loss: 0.2243 - val_acc: 0.9641\n",
      "Epoch 25/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.2010 - acc: 0.9731 - val_loss: 0.2019 - val_acc: 0.9641\n",
      "Epoch 26/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1863 - acc: 0.9711 - val_loss: 0.1884 - val_acc: 0.9641\n",
      "Epoch 27/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1757 - acc: 0.9770 - val_loss: 0.1833 - val_acc: 0.9641\n",
      "Epoch 28/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1647 - acc: 0.9741 - val_loss: 0.1752 - val_acc: 0.9611\n",
      "Epoch 29/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.1559 - acc: 0.9691 - val_loss: 0.1657 - val_acc: 0.9671\n",
      "Epoch 30/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1486 - acc: 0.9731 - val_loss: 0.1538 - val_acc: 0.9641\n",
      "Epoch 31/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.1405 - acc: 0.9721 - val_loss: 0.1488 - val_acc: 0.9641\n",
      "Epoch 32/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.1339 - acc: 0.9731 - val_loss: 0.1420 - val_acc: 0.9731\n",
      "Epoch 33/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.1275 - acc: 0.9760 - val_loss: 0.1363 - val_acc: 0.9701\n",
      "Epoch 34/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.1219 - acc: 0.9750 - val_loss: 0.1308 - val_acc: 0.9641\n",
      "Epoch 35/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.1173 - acc: 0.9741 - val_loss: 0.1291 - val_acc: 0.9701\n",
      "Epoch 36/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1131 - acc: 0.9750 - val_loss: 0.1278 - val_acc: 0.9671\n",
      "Epoch 37/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1089 - acc: 0.9741 - val_loss: 0.1191 - val_acc: 0.9581\n",
      "Epoch 38/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1055 - acc: 0.9750 - val_loss: 0.1143 - val_acc: 0.9641\n",
      "Epoch 39/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.1010 - acc: 0.9770 - val_loss: 0.1175 - val_acc: 0.9611\n",
      "Epoch 40/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0989 - acc: 0.9760 - val_loss: 0.1126 - val_acc: 0.9611\n",
      "Epoch 41/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0949 - acc: 0.9770 - val_loss: 0.1123 - val_acc: 0.9671\n",
      "Epoch 42/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0945 - acc: 0.9760 - val_loss: 0.1106 - val_acc: 0.9671\n",
      "Epoch 43/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0917 - acc: 0.9750 - val_loss: 0.1066 - val_acc: 0.9671\n",
      "Epoch 44/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0911 - acc: 0.9760 - val_loss: 0.1011 - val_acc: 0.9641\n",
      "Epoch 45/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0868 - acc: 0.9770 - val_loss: 0.1047 - val_acc: 0.9611\n",
      "Epoch 46/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0851 - acc: 0.9800 - val_loss: 0.1040 - val_acc: 0.9701\n",
      "Epoch 47/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0857 - acc: 0.9790 - val_loss: 0.0968 - val_acc: 0.9671\n",
      "Epoch 48/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0828 - acc: 0.9770 - val_loss: 0.0998 - val_acc: 0.9701\n",
      "Epoch 49/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0824 - acc: 0.9731 - val_loss: 0.0937 - val_acc: 0.9731\n",
      "Epoch 50/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0810 - acc: 0.9760 - val_loss: 0.0935 - val_acc: 0.9671\n",
      "Epoch 51/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0788 - acc: 0.9741 - val_loss: 0.0939 - val_acc: 0.9641\n",
      "Epoch 52/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0789 - acc: 0.9780 - val_loss: 0.0927 - val_acc: 0.9641\n",
      "Epoch 53/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0768 - acc: 0.9780 - val_loss: 0.0894 - val_acc: 0.9731\n",
      "Epoch 54/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0765 - acc: 0.9770 - val_loss: 0.0895 - val_acc: 0.9641\n",
      "Epoch 55/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0748 - acc: 0.9750 - val_loss: 0.0902 - val_acc: 0.9701\n",
      "Epoch 56/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0744 - acc: 0.9780 - val_loss: 0.0872 - val_acc: 0.9701\n",
      "Epoch 57/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0730 - acc: 0.9780 - val_loss: 0.0955 - val_acc: 0.9671\n",
      "Epoch 58/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0733 - acc: 0.9770 - val_loss: 0.0884 - val_acc: 0.9731\n",
      "Epoch 59/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0708 - acc: 0.9750 - val_loss: 0.0918 - val_acc: 0.9701\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0714 - acc: 0.9790 - val_loss: 0.0907 - val_acc: 0.9731\n",
      "Epoch 61/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0710 - acc: 0.9750 - val_loss: 0.0870 - val_acc: 0.9611\n",
      "Epoch 62/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0703 - acc: 0.9760 - val_loss: 0.0843 - val_acc: 0.9701\n",
      "Epoch 63/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0693 - acc: 0.9790 - val_loss: 0.0835 - val_acc: 0.9701\n",
      "Epoch 64/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0677 - acc: 0.9770 - val_loss: 0.0900 - val_acc: 0.9731\n",
      "Epoch 65/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0668 - acc: 0.9800 - val_loss: 0.0891 - val_acc: 0.9731\n",
      "Epoch 66/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0667 - acc: 0.9770 - val_loss: 0.0882 - val_acc: 0.9701\n",
      "Epoch 67/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0665 - acc: 0.9800 - val_loss: 0.0855 - val_acc: 0.9701\n",
      "Epoch 68/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0660 - acc: 0.9770 - val_loss: 0.0888 - val_acc: 0.9731\n",
      "Epoch 69/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0647 - acc: 0.9810 - val_loss: 0.0898 - val_acc: 0.9671\n",
      "Epoch 70/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0657 - acc: 0.9741 - val_loss: 0.0889 - val_acc: 0.9641\n",
      "Epoch 71/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0644 - acc: 0.9790 - val_loss: 0.0875 - val_acc: 0.9790\n",
      "Epoch 72/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0632 - acc: 0.9790 - val_loss: 0.0898 - val_acc: 0.9731\n",
      "Epoch 73/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0643 - acc: 0.9780 - val_loss: 0.0876 - val_acc: 0.9701\n",
      "Epoch 74/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0630 - acc: 0.9790 - val_loss: 0.0898 - val_acc: 0.9671\n",
      "Epoch 75/3000\n",
      "1002/1002 [==============================] - 0s 37us/step - loss: 0.0630 - acc: 0.9790 - val_loss: 0.0922 - val_acc: 0.9611\n",
      "Epoch 76/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0630 - acc: 0.9770 - val_loss: 0.0895 - val_acc: 0.9671\n",
      "Epoch 77/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0590 - acc: 0.9770 - val_loss: 0.1037 - val_acc: 0.9731\n",
      "Epoch 78/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0651 - acc: 0.9790 - val_loss: 0.0959 - val_acc: 0.9731\n",
      "Epoch 79/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0618 - acc: 0.9810 - val_loss: 0.0949 - val_acc: 0.9641\n",
      "Epoch 80/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0610 - acc: 0.9780 - val_loss: 0.0934 - val_acc: 0.9731\n",
      "Epoch 81/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0599 - acc: 0.9800 - val_loss: 0.0977 - val_acc: 0.9671\n",
      "Epoch 82/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0604 - acc: 0.9780 - val_loss: 0.0976 - val_acc: 0.9760\n",
      "Epoch 83/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0609 - acc: 0.9790 - val_loss: 0.1021 - val_acc: 0.9760\n",
      "Epoch 00083: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=rmsp,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist3=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/3000\n",
      "1002/1002 [==============================] - 1s 750us/step - loss: 1.7224 - acc: 0.2824 - val_loss: 1.5745 - val_acc: 0.3084\n",
      "Epoch 2/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 1.4951 - acc: 0.4750 - val_loss: 1.3802 - val_acc: 0.5479\n",
      "Epoch 3/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 1.3035 - acc: 0.5369 - val_loss: 1.2142 - val_acc: 0.5539\n",
      "Epoch 4/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 1.1612 - acc: 0.5379 - val_loss: 1.1028 - val_acc: 0.5569\n",
      "Epoch 5/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 1.0580 - acc: 0.5479 - val_loss: 1.0157 - val_acc: 0.5719\n",
      "Epoch 6/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.9781 - acc: 0.5838 - val_loss: 0.9473 - val_acc: 0.5868\n",
      "Epoch 7/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.9144 - acc: 0.6018 - val_loss: 0.8953 - val_acc: 0.5958\n",
      "Epoch 8/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.8599 - acc: 0.6367 - val_loss: 0.8415 - val_acc: 0.6317\n",
      "Epoch 9/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.8095 - acc: 0.6487 - val_loss: 0.7961 - val_acc: 0.6557\n",
      "Epoch 10/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.7636 - acc: 0.6886 - val_loss: 0.7515 - val_acc: 0.6527\n",
      "Epoch 11/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.7273 - acc: 0.6856 - val_loss: 0.7140 - val_acc: 0.6826\n",
      "Epoch 12/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.6899 - acc: 0.7425 - val_loss: 0.6784 - val_acc: 0.7275\n",
      "Epoch 13/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.6571 - acc: 0.7635 - val_loss: 0.6495 - val_acc: 0.8084\n",
      "Epoch 14/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.6272 - acc: 0.8114 - val_loss: 0.6183 - val_acc: 0.8024\n",
      "Epoch 15/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.5995 - acc: 0.8174 - val_loss: 0.5922 - val_acc: 0.8174\n",
      "Epoch 16/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.5731 - acc: 0.8283 - val_loss: 0.5662 - val_acc: 0.8174\n",
      "Epoch 17/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.5496 - acc: 0.8363 - val_loss: 0.5425 - val_acc: 0.8293\n",
      "Epoch 18/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.5267 - acc: 0.8493 - val_loss: 0.5199 - val_acc: 0.8323\n",
      "Epoch 19/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.5053 - acc: 0.8543 - val_loss: 0.5009 - val_acc: 0.9251\n",
      "Epoch 20/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.4862 - acc: 0.8922 - val_loss: 0.4783 - val_acc: 0.9132\n",
      "Epoch 21/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.4671 - acc: 0.9002 - val_loss: 0.4603 - val_acc: 0.9311\n",
      "Epoch 22/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.4497 - acc: 0.9102 - val_loss: 0.4433 - val_acc: 0.9341\n",
      "Epoch 23/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.4332 - acc: 0.9222 - val_loss: 0.4279 - val_acc: 0.9431\n",
      "Epoch 24/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.4177 - acc: 0.9301 - val_loss: 0.4118 - val_acc: 0.9461\n",
      "Epoch 25/3000\n",
      "1002/1002 [==============================] - 0s 126us/step - loss: 0.4027 - acc: 0.9371 - val_loss: 0.3976 - val_acc: 0.9641\n",
      "Epoch 26/3000\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.3889 - acc: 0.9421 - val_loss: 0.3835 - val_acc: 0.9581\n",
      "Epoch 27/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.3757 - acc: 0.9411 - val_loss: 0.3705 - val_acc: 0.9641\n",
      "Epoch 28/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.3639 - acc: 0.9561 - val_loss: 0.3588 - val_acc: 0.9641\n",
      "Epoch 29/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.3518 - acc: 0.9501 - val_loss: 0.3462 - val_acc: 0.9641\n",
      "Epoch 30/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.3409 - acc: 0.9501 - val_loss: 0.3355 - val_acc: 0.9641\n",
      "Epoch 31/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.3302 - acc: 0.9591 - val_loss: 0.3262 - val_acc: 0.9611\n",
      "Epoch 32/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.3204 - acc: 0.9601 - val_loss: 0.3153 - val_acc: 0.9641\n",
      "Epoch 33/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.3113 - acc: 0.9611 - val_loss: 0.3064 - val_acc: 0.9641\n",
      "Epoch 34/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.3024 - acc: 0.9621 - val_loss: 0.2980 - val_acc: 0.9581\n",
      "Epoch 35/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.2945 - acc: 0.9701 - val_loss: 0.2894 - val_acc: 0.9611\n",
      "Epoch 36/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.2870 - acc: 0.9601 - val_loss: 0.2819 - val_acc: 0.9611\n",
      "Epoch 37/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.2789 - acc: 0.9641 - val_loss: 0.2745 - val_acc: 0.9581\n",
      "Epoch 38/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.2715 - acc: 0.9651 - val_loss: 0.2693 - val_acc: 0.9641\n",
      "Epoch 39/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.2655 - acc: 0.9661 - val_loss: 0.2608 - val_acc: 0.9611\n",
      "Epoch 40/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.2589 - acc: 0.9631 - val_loss: 0.2544 - val_acc: 0.9581\n",
      "Epoch 41/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.2528 - acc: 0.9691 - val_loss: 0.2481 - val_acc: 0.9611\n",
      "Epoch 42/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.2474 - acc: 0.9661 - val_loss: 0.2428 - val_acc: 0.9611\n",
      "Epoch 43/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.2418 - acc: 0.9691 - val_loss: 0.2385 - val_acc: 0.9611\n",
      "Epoch 44/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.2365 - acc: 0.9711 - val_loss: 0.2334 - val_acc: 0.9671\n",
      "Epoch 45/3000\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.2317 - acc: 0.9641 - val_loss: 0.2282 - val_acc: 0.9641\n",
      "Epoch 46/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.2271 - acc: 0.9701 - val_loss: 0.2242 - val_acc: 0.9641\n",
      "Epoch 47/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.2224 - acc: 0.9671 - val_loss: 0.2194 - val_acc: 0.9641\n",
      "Epoch 48/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.2178 - acc: 0.9671 - val_loss: 0.2155 - val_acc: 0.9671\n",
      "Epoch 49/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.2144 - acc: 0.9711 - val_loss: 0.2112 - val_acc: 0.9641\n",
      "Epoch 50/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.2101 - acc: 0.9711 - val_loss: 0.2071 - val_acc: 0.9671\n",
      "Epoch 51/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.2064 - acc: 0.9691 - val_loss: 0.2044 - val_acc: 0.9641\n",
      "Epoch 52/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.2031 - acc: 0.9691 - val_loss: 0.1997 - val_acc: 0.9671\n",
      "Epoch 53/3000\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.1997 - acc: 0.9701 - val_loss: 0.1961 - val_acc: 0.9671\n",
      "Epoch 54/3000\n",
      "1002/1002 [==============================] - 0s 39us/step - loss: 0.1962 - acc: 0.9711 - val_loss: 0.1930 - val_acc: 0.9641\n",
      "Epoch 55/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1931 - acc: 0.9701 - val_loss: 0.1912 - val_acc: 0.9671\n",
      "Epoch 56/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1909 - acc: 0.9671 - val_loss: 0.1873 - val_acc: 0.9671\n",
      "Epoch 57/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.1875 - acc: 0.9691 - val_loss: 0.1839 - val_acc: 0.9701\n",
      "Epoch 58/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.1849 - acc: 0.9691 - val_loss: 0.1816 - val_acc: 0.9701\n",
      "Epoch 59/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.1820 - acc: 0.9701 - val_loss: 0.1791 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/3000\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.1797 - acc: 0.9721 - val_loss: 0.1765 - val_acc: 0.9671\n",
      "Epoch 61/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.1767 - acc: 0.974 - 0s 95us/step - loss: 0.1773 - acc: 0.9721 - val_loss: 0.1741 - val_acc: 0.9671\n",
      "Epoch 62/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.1753 - acc: 0.9721 - val_loss: 0.1721 - val_acc: 0.9671\n",
      "Epoch 63/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.1727 - acc: 0.9711 - val_loss: 0.1702 - val_acc: 0.9671\n",
      "Epoch 64/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1706 - acc: 0.9691 - val_loss: 0.1679 - val_acc: 0.9671\n",
      "Epoch 65/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.1687 - acc: 0.9711 - val_loss: 0.1655 - val_acc: 0.9671\n",
      "Epoch 66/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.1668 - acc: 0.9691 - val_loss: 0.1635 - val_acc: 0.9671\n",
      "Epoch 67/3000\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.1647 - acc: 0.9721 - val_loss: 0.1615 - val_acc: 0.9671\n",
      "Epoch 68/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.1629 - acc: 0.9711 - val_loss: 0.1597 - val_acc: 0.9671\n",
      "Epoch 69/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.1609 - acc: 0.9741 - val_loss: 0.1577 - val_acc: 0.9671\n",
      "Epoch 70/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.1591 - acc: 0.9711 - val_loss: 0.1575 - val_acc: 0.9671\n",
      "Epoch 71/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.1577 - acc: 0.9691 - val_loss: 0.1538 - val_acc: 0.9701\n",
      "Epoch 72/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1555 - acc: 0.9721 - val_loss: 0.1520 - val_acc: 0.9701\n",
      "Epoch 73/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.1538 - acc: 0.9701 - val_loss: 0.1505 - val_acc: 0.9671\n",
      "Epoch 74/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1522 - acc: 0.9731 - val_loss: 0.1488 - val_acc: 0.9701\n",
      "Epoch 75/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1506 - acc: 0.9681 - val_loss: 0.1471 - val_acc: 0.9671\n",
      "Epoch 76/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.1522 - acc: 0.970 - 0s 86us/step - loss: 0.1490 - acc: 0.9721 - val_loss: 0.1458 - val_acc: 0.9671\n",
      "Epoch 77/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.1475 - acc: 0.9731 - val_loss: 0.1446 - val_acc: 0.9671\n",
      "Epoch 78/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1460 - acc: 0.9711 - val_loss: 0.1428 - val_acc: 0.9671\n",
      "Epoch 79/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.1447 - acc: 0.9731 - val_loss: 0.1414 - val_acc: 0.9671\n",
      "Epoch 80/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1430 - acc: 0.9731 - val_loss: 0.1404 - val_acc: 0.9701\n",
      "Epoch 81/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1421 - acc: 0.9691 - val_loss: 0.1382 - val_acc: 0.9671\n",
      "Epoch 82/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1408 - acc: 0.9741 - val_loss: 0.1370 - val_acc: 0.9671\n",
      "Epoch 83/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1394 - acc: 0.9701 - val_loss: 0.1360 - val_acc: 0.9671\n",
      "Epoch 84/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1381 - acc: 0.9731 - val_loss: 0.1347 - val_acc: 0.9701\n",
      "Epoch 85/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1370 - acc: 0.9711 - val_loss: 0.1334 - val_acc: 0.9671\n",
      "Epoch 86/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1357 - acc: 0.9731 - val_loss: 0.1331 - val_acc: 0.9701\n",
      "Epoch 87/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.1347 - acc: 0.9701 - val_loss: 0.1314 - val_acc: 0.9671\n",
      "Epoch 88/3000\n",
      "1002/1002 [==============================] - 0s 50us/step - loss: 0.1336 - acc: 0.9731 - val_loss: 0.1304 - val_acc: 0.9671\n",
      "Epoch 89/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1324 - acc: 0.9741 - val_loss: 0.1296 - val_acc: 0.9671\n",
      "Epoch 90/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1317 - acc: 0.9731 - val_loss: 0.1287 - val_acc: 0.9671\n",
      "Epoch 91/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1306 - acc: 0.9711 - val_loss: 0.1279 - val_acc: 0.9701\n",
      "Epoch 92/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1297 - acc: 0.9731 - val_loss: 0.1264 - val_acc: 0.9701\n",
      "Epoch 93/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1289 - acc: 0.9701 - val_loss: 0.1251 - val_acc: 0.9701\n",
      "Epoch 94/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1275 - acc: 0.9731 - val_loss: 0.1242 - val_acc: 0.9671\n",
      "Epoch 95/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1269 - acc: 0.9731 - val_loss: 0.1237 - val_acc: 0.9671\n",
      "Epoch 96/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.1261 - acc: 0.9731 - val_loss: 0.1227 - val_acc: 0.9671\n",
      "Epoch 97/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1250 - acc: 0.9721 - val_loss: 0.1218 - val_acc: 0.9671\n",
      "Epoch 98/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.1242 - acc: 0.9731 - val_loss: 0.1210 - val_acc: 0.9671\n",
      "Epoch 99/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1235 - acc: 0.9721 - val_loss: 0.1200 - val_acc: 0.9701\n",
      "Epoch 100/3000\n",
      "1002/1002 [==============================] - 0s 39us/step - loss: 0.1225 - acc: 0.9711 - val_loss: 0.1190 - val_acc: 0.9701\n",
      "Epoch 101/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1216 - acc: 0.9731 - val_loss: 0.1183 - val_acc: 0.9701\n",
      "Epoch 102/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1210 - acc: 0.9721 - val_loss: 0.1171 - val_acc: 0.9701\n",
      "Epoch 103/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1203 - acc: 0.9741 - val_loss: 0.1168 - val_acc: 0.9731\n",
      "Epoch 104/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1197 - acc: 0.9721 - val_loss: 0.1164 - val_acc: 0.9731\n",
      "Epoch 105/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1189 - acc: 0.9701 - val_loss: 0.1151 - val_acc: 0.9701\n",
      "Epoch 106/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.1182 - acc: 0.9741 - val_loss: 0.1145 - val_acc: 0.9701\n",
      "Epoch 107/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.1173 - acc: 0.9741 - val_loss: 0.1143 - val_acc: 0.9701\n",
      "Epoch 108/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1166 - acc: 0.9721 - val_loss: 0.1135 - val_acc: 0.9701\n",
      "Epoch 109/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1160 - acc: 0.9721 - val_loss: 0.1129 - val_acc: 0.9701\n",
      "Epoch 110/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1152 - acc: 0.9721 - val_loss: 0.1121 - val_acc: 0.9701\n",
      "Epoch 111/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1148 - acc: 0.9721 - val_loss: 0.1117 - val_acc: 0.9701\n",
      "Epoch 112/3000\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.1143 - acc: 0.9721 - val_loss: 0.1107 - val_acc: 0.9701\n",
      "Epoch 113/3000\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.1137 - acc: 0.9741 - val_loss: 0.1100 - val_acc: 0.9701\n",
      "Epoch 114/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1128 - acc: 0.9721 - val_loss: 0.1096 - val_acc: 0.9701\n",
      "Epoch 115/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.1124 - acc: 0.9731 - val_loss: 0.1092 - val_acc: 0.9701\n",
      "Epoch 116/3000\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.1114 - acc: 0.9711 - val_loss: 0.1085 - val_acc: 0.9701\n",
      "Epoch 117/3000\n",
      "1002/1002 [==============================] - 0s 39us/step - loss: 0.1112 - acc: 0.9741 - val_loss: 0.1080 - val_acc: 0.9701\n",
      "Epoch 118/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.1106 - acc: 0.9731 - val_loss: 0.1078 - val_acc: 0.9701\n",
      "Epoch 119/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1100 - acc: 0.9721 - val_loss: 0.1072 - val_acc: 0.9701\n",
      "Epoch 120/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.1094 - acc: 0.9711 - val_loss: 0.1063 - val_acc: 0.9701\n",
      "Epoch 121/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.1088 - acc: 0.9731 - val_loss: 0.1059 - val_acc: 0.9701\n",
      "Epoch 122/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.1085 - acc: 0.9731 - val_loss: 0.1053 - val_acc: 0.9701\n",
      "Epoch 123/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.1078 - acc: 0.9721 - val_loss: 0.1044 - val_acc: 0.9701\n",
      "Epoch 124/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.1074 - acc: 0.9731 - val_loss: 0.1037 - val_acc: 0.9701\n",
      "Epoch 125/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.1067 - acc: 0.9741 - val_loss: 0.1038 - val_acc: 0.9701\n",
      "Epoch 126/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.1065 - acc: 0.9731 - val_loss: 0.1034 - val_acc: 0.9701\n",
      "Epoch 127/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.0633 - acc: 1.000 - 0s 61us/step - loss: 0.1057 - acc: 0.9731 - val_loss: 0.1030 - val_acc: 0.9701\n",
      "Epoch 128/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.1056 - acc: 0.9731 - val_loss: 0.1024 - val_acc: 0.9701\n",
      "Epoch 129/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.1050 - acc: 0.9721 - val_loss: 0.1018 - val_acc: 0.9701\n",
      "Epoch 130/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.1046 - acc: 0.9731 - val_loss: 0.1021 - val_acc: 0.9671\n",
      "Epoch 131/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.1041 - acc: 0.9741 - val_loss: 0.1011 - val_acc: 0.9701\n",
      "Epoch 132/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.1037 - acc: 0.9750 - val_loss: 0.1009 - val_acc: 0.9701\n",
      "Epoch 133/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.1033 - acc: 0.9721 - val_loss: 0.1001 - val_acc: 0.9701\n",
      "Epoch 134/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.1028 - acc: 0.9750 - val_loss: 0.0997 - val_acc: 0.9701\n",
      "Epoch 135/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.1024 - acc: 0.9721 - val_loss: 0.0999 - val_acc: 0.9701\n",
      "Epoch 136/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.1019 - acc: 0.9741 - val_loss: 0.0995 - val_acc: 0.9701\n",
      "Epoch 137/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.1014 - acc: 0.9721 - val_loss: 0.0990 - val_acc: 0.9701\n",
      "Epoch 138/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.1010 - acc: 0.9731 - val_loss: 0.0985 - val_acc: 0.9701\n",
      "Epoch 139/3000\n",
      "1002/1002 [==============================] - 0s 118us/step - loss: 0.1006 - acc: 0.9731 - val_loss: 0.0979 - val_acc: 0.9701\n",
      "Epoch 140/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.1004 - acc: 0.9750 - val_loss: 0.0979 - val_acc: 0.9701\n",
      "Epoch 141/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0999 - acc: 0.9741 - val_loss: 0.0970 - val_acc: 0.9701\n",
      "Epoch 142/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0995 - acc: 0.9731 - val_loss: 0.0963 - val_acc: 0.9671\n",
      "Epoch 143/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0993 - acc: 0.9741 - val_loss: 0.0961 - val_acc: 0.9671\n",
      "Epoch 144/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0988 - acc: 0.9760 - val_loss: 0.0958 - val_acc: 0.9701\n",
      "Epoch 145/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0985 - acc: 0.9731 - val_loss: 0.0955 - val_acc: 0.9701\n",
      "Epoch 146/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0980 - acc: 0.9750 - val_loss: 0.0953 - val_acc: 0.9701\n",
      "Epoch 147/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0977 - acc: 0.9741 - val_loss: 0.0950 - val_acc: 0.9701\n",
      "Epoch 148/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0974 - acc: 0.9741 - val_loss: 0.0947 - val_acc: 0.9701\n",
      "Epoch 149/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0970 - acc: 0.9741 - val_loss: 0.0945 - val_acc: 0.9701\n",
      "Epoch 150/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0968 - acc: 0.9731 - val_loss: 0.0939 - val_acc: 0.9701\n",
      "Epoch 151/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0966 - acc: 0.9741 - val_loss: 0.0933 - val_acc: 0.9701\n",
      "Epoch 152/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0961 - acc: 0.9750 - val_loss: 0.0934 - val_acc: 0.9701\n",
      "Epoch 153/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0959 - acc: 0.9741 - val_loss: 0.0930 - val_acc: 0.9701\n",
      "Epoch 154/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0955 - acc: 0.9750 - val_loss: 0.0928 - val_acc: 0.9701\n",
      "Epoch 155/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0952 - acc: 0.9741 - val_loss: 0.0922 - val_acc: 0.9701\n",
      "Epoch 156/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0948 - acc: 0.9741 - val_loss: 0.0916 - val_acc: 0.9701\n",
      "Epoch 157/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0946 - acc: 0.9741 - val_loss: 0.0915 - val_acc: 0.9701\n",
      "Epoch 158/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0942 - acc: 0.9760 - val_loss: 0.0912 - val_acc: 0.9701\n",
      "Epoch 159/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0939 - acc: 0.9741 - val_loss: 0.0910 - val_acc: 0.9701\n",
      "Epoch 160/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0937 - acc: 0.9741 - val_loss: 0.0905 - val_acc: 0.9701\n",
      "Epoch 161/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0933 - acc: 0.9741 - val_loss: 0.0901 - val_acc: 0.9671\n",
      "Epoch 162/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0932 - acc: 0.9750 - val_loss: 0.0905 - val_acc: 0.9701\n",
      "Epoch 163/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0928 - acc: 0.9750 - val_loss: 0.0898 - val_acc: 0.9701\n",
      "Epoch 164/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0927 - acc: 0.9750 - val_loss: 0.0897 - val_acc: 0.9701\n",
      "Epoch 165/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0922 - acc: 0.9760 - val_loss: 0.0895 - val_acc: 0.9701\n",
      "Epoch 166/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0922 - acc: 0.9731 - val_loss: 0.0893 - val_acc: 0.9701\n",
      "Epoch 167/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0917 - acc: 0.9741 - val_loss: 0.0891 - val_acc: 0.9701\n",
      "Epoch 168/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0915 - acc: 0.9741 - val_loss: 0.0891 - val_acc: 0.9701\n",
      "Epoch 169/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0913 - acc: 0.9731 - val_loss: 0.0883 - val_acc: 0.9701\n",
      "Epoch 170/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0909 - acc: 0.9760 - val_loss: 0.0876 - val_acc: 0.9701\n",
      "Epoch 171/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0908 - acc: 0.9750 - val_loss: 0.0876 - val_acc: 0.9671\n",
      "Epoch 172/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0907 - acc: 0.9770 - val_loss: 0.0876 - val_acc: 0.9671\n",
      "Epoch 173/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0901 - acc: 0.9760 - val_loss: 0.0873 - val_acc: 0.9701\n",
      "Epoch 174/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0900 - acc: 0.9731 - val_loss: 0.0868 - val_acc: 0.9671\n",
      "Epoch 175/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0898 - acc: 0.9760 - val_loss: 0.0867 - val_acc: 0.9671\n",
      "Epoch 176/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0895 - acc: 0.9760 - val_loss: 0.0863 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0894 - acc: 0.9770 - val_loss: 0.0858 - val_acc: 0.9701\n",
      "Epoch 178/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0891 - acc: 0.9760 - val_loss: 0.0857 - val_acc: 0.9701\n",
      "Epoch 179/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0889 - acc: 0.9741 - val_loss: 0.0859 - val_acc: 0.9701\n",
      "Epoch 180/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0887 - acc: 0.9770 - val_loss: 0.0854 - val_acc: 0.9701\n",
      "Epoch 181/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0886 - acc: 0.9760 - val_loss: 0.0852 - val_acc: 0.9701\n",
      "Epoch 182/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0881 - acc: 0.9760 - val_loss: 0.0851 - val_acc: 0.9701\n",
      "Epoch 183/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0880 - acc: 0.9750 - val_loss: 0.0850 - val_acc: 0.9671\n",
      "Epoch 184/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0880 - acc: 0.9770 - val_loss: 0.0843 - val_acc: 0.9671\n",
      "Epoch 185/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0876 - acc: 0.9760 - val_loss: 0.0844 - val_acc: 0.9671\n",
      "Epoch 186/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0873 - acc: 0.9760 - val_loss: 0.0843 - val_acc: 0.9671\n",
      "Epoch 187/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0871 - acc: 0.9750 - val_loss: 0.0839 - val_acc: 0.9671\n",
      "Epoch 188/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0869 - acc: 0.9770 - val_loss: 0.0832 - val_acc: 0.9701\n",
      "Epoch 189/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0867 - acc: 0.9780 - val_loss: 0.0833 - val_acc: 0.9701\n",
      "Epoch 190/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0867 - acc: 0.9770 - val_loss: 0.0832 - val_acc: 0.9671\n",
      "Epoch 191/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0865 - acc: 0.9770 - val_loss: 0.0831 - val_acc: 0.9701\n",
      "Epoch 192/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0862 - acc: 0.9770 - val_loss: 0.0832 - val_acc: 0.9701\n",
      "Epoch 193/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0859 - acc: 0.9770 - val_loss: 0.0829 - val_acc: 0.9701\n",
      "Epoch 194/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0859 - acc: 0.9760 - val_loss: 0.0830 - val_acc: 0.9701\n",
      "Epoch 195/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0856 - acc: 0.9770 - val_loss: 0.0834 - val_acc: 0.9671\n",
      "Epoch 196/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0854 - acc: 0.9770 - val_loss: 0.0828 - val_acc: 0.9671\n",
      "Epoch 197/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0852 - acc: 0.9760 - val_loss: 0.0830 - val_acc: 0.9731\n",
      "Epoch 198/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0851 - acc: 0.9750 - val_loss: 0.0823 - val_acc: 0.9701\n",
      "Epoch 199/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0849 - acc: 0.9760 - val_loss: 0.0824 - val_acc: 0.9701\n",
      "Epoch 200/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0847 - acc: 0.9780 - val_loss: 0.0821 - val_acc: 0.9731\n",
      "Epoch 201/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0844 - acc: 0.9770 - val_loss: 0.0817 - val_acc: 0.9701\n",
      "Epoch 202/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0843 - acc: 0.9770 - val_loss: 0.0813 - val_acc: 0.9701\n",
      "Epoch 203/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0841 - acc: 0.9780 - val_loss: 0.0812 - val_acc: 0.9701\n",
      "Epoch 204/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0839 - acc: 0.9770 - val_loss: 0.0810 - val_acc: 0.9701\n",
      "Epoch 205/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0837 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9701\n",
      "Epoch 206/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0836 - acc: 0.9780 - val_loss: 0.0813 - val_acc: 0.9731\n",
      "Epoch 207/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0835 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9731\n",
      "Epoch 208/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0833 - acc: 0.9770 - val_loss: 0.0804 - val_acc: 0.9701\n",
      "Epoch 209/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0830 - acc: 0.9750 - val_loss: 0.0800 - val_acc: 0.9701\n",
      "Epoch 210/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0830 - acc: 0.9770 - val_loss: 0.0801 - val_acc: 0.9731\n",
      "Epoch 211/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0828 - acc: 0.9770 - val_loss: 0.0799 - val_acc: 0.9731\n",
      "Epoch 212/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0827 - acc: 0.9760 - val_loss: 0.0798 - val_acc: 0.9701\n",
      "Epoch 213/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0826 - acc: 0.9780 - val_loss: 0.0797 - val_acc: 0.9731\n",
      "Epoch 214/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0823 - acc: 0.9770 - val_loss: 0.0797 - val_acc: 0.9731\n",
      "Epoch 215/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0821 - acc: 0.9770 - val_loss: 0.0793 - val_acc: 0.9731\n",
      "Epoch 216/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0822 - acc: 0.9770 - val_loss: 0.0791 - val_acc: 0.9731\n",
      "Epoch 217/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0818 - acc: 0.9760 - val_loss: 0.0787 - val_acc: 0.9731\n",
      "Epoch 218/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0818 - acc: 0.9770 - val_loss: 0.0787 - val_acc: 0.9731\n",
      "Epoch 219/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0816 - acc: 0.9770 - val_loss: 0.0788 - val_acc: 0.9731\n",
      "Epoch 220/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0814 - acc: 0.9780 - val_loss: 0.0793 - val_acc: 0.9731\n",
      "Epoch 221/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0812 - acc: 0.9760 - val_loss: 0.0790 - val_acc: 0.9701\n",
      "Epoch 222/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0812 - acc: 0.9760 - val_loss: 0.0786 - val_acc: 0.9701\n",
      "Epoch 223/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0810 - acc: 0.9770 - val_loss: 0.0783 - val_acc: 0.9701\n",
      "Epoch 224/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0808 - acc: 0.9780 - val_loss: 0.0783 - val_acc: 0.9701\n",
      "Epoch 225/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0807 - acc: 0.9780 - val_loss: 0.0782 - val_acc: 0.9701\n",
      "Epoch 226/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0807 - acc: 0.9770 - val_loss: 0.0779 - val_acc: 0.9701\n",
      "Epoch 227/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0804 - acc: 0.9780 - val_loss: 0.0782 - val_acc: 0.9731\n",
      "Epoch 228/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0802 - acc: 0.9770 - val_loss: 0.0777 - val_acc: 0.9701\n",
      "Epoch 229/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0801 - acc: 0.9780 - val_loss: 0.0774 - val_acc: 0.9701\n",
      "Epoch 230/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0801 - acc: 0.9780 - val_loss: 0.0773 - val_acc: 0.9701\n",
      "Epoch 231/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0800 - acc: 0.9790 - val_loss: 0.0772 - val_acc: 0.9701\n",
      "Epoch 232/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0798 - acc: 0.9780 - val_loss: 0.0771 - val_acc: 0.9701\n",
      "Epoch 233/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0796 - acc: 0.9770 - val_loss: 0.0772 - val_acc: 0.9731\n",
      "Epoch 234/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0796 - acc: 0.9770 - val_loss: 0.0773 - val_acc: 0.9731\n",
      "Epoch 235/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0795 - acc: 0.9770 - val_loss: 0.0775 - val_acc: 0.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0794 - acc: 0.9770 - val_loss: 0.0769 - val_acc: 0.9701\n",
      "Epoch 237/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0791 - acc: 0.9770 - val_loss: 0.0768 - val_acc: 0.9701\n",
      "Epoch 238/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0790 - acc: 0.9770 - val_loss: 0.0764 - val_acc: 0.9701\n",
      "Epoch 239/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0789 - acc: 0.9780 - val_loss: 0.0763 - val_acc: 0.9701\n",
      "Epoch 240/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0788 - acc: 0.9780 - val_loss: 0.0762 - val_acc: 0.9701\n",
      "Epoch 241/3000\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0786 - acc: 0.9780 - val_loss: 0.0758 - val_acc: 0.9701\n",
      "Epoch 242/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0785 - acc: 0.9790 - val_loss: 0.0758 - val_acc: 0.9701\n",
      "Epoch 243/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.0783 - acc: 0.9780 - val_loss: 0.0757 - val_acc: 0.9701\n",
      "Epoch 244/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0784 - acc: 0.9790 - val_loss: 0.0760 - val_acc: 0.9731\n",
      "Epoch 245/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.0781 - acc: 0.9780 - val_loss: 0.0758 - val_acc: 0.9701\n",
      "Epoch 246/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0780 - acc: 0.9780 - val_loss: 0.0756 - val_acc: 0.9701\n",
      "Epoch 247/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0779 - acc: 0.9780 - val_loss: 0.0754 - val_acc: 0.9701\n",
      "Epoch 248/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0778 - acc: 0.9790 - val_loss: 0.0752 - val_acc: 0.9701\n",
      "Epoch 249/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0777 - acc: 0.9780 - val_loss: 0.0759 - val_acc: 0.9671\n",
      "Epoch 250/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0776 - acc: 0.9790 - val_loss: 0.0753 - val_acc: 0.9671\n",
      "Epoch 251/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0775 - acc: 0.9780 - val_loss: 0.0750 - val_acc: 0.9701\n",
      "Epoch 252/3000\n",
      "1002/1002 [==============================] - 0s 117us/step - loss: 0.0776 - acc: 0.9770 - val_loss: 0.0749 - val_acc: 0.9701\n",
      "Epoch 253/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0772 - acc: 0.9780 - val_loss: 0.0749 - val_acc: 0.9701\n",
      "Epoch 254/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0772 - acc: 0.9780 - val_loss: 0.0749 - val_acc: 0.9701\n",
      "Epoch 255/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0771 - acc: 0.9790 - val_loss: 0.0749 - val_acc: 0.9731\n",
      "Epoch 256/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0770 - acc: 0.9780 - val_loss: 0.0747 - val_acc: 0.9731\n",
      "Epoch 257/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0769 - acc: 0.9780 - val_loss: 0.0743 - val_acc: 0.9701\n",
      "Epoch 258/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0768 - acc: 0.9780 - val_loss: 0.0740 - val_acc: 0.9701\n",
      "Epoch 259/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0767 - acc: 0.9790 - val_loss: 0.0740 - val_acc: 0.9701\n",
      "Epoch 260/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0766 - acc: 0.9780 - val_loss: 0.0741 - val_acc: 0.9701\n",
      "Epoch 261/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0765 - acc: 0.9780 - val_loss: 0.0739 - val_acc: 0.9701\n",
      "Epoch 262/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0763 - acc: 0.9790 - val_loss: 0.0739 - val_acc: 0.9701\n",
      "Epoch 263/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0762 - acc: 0.9770 - val_loss: 0.0739 - val_acc: 0.9701\n",
      "Epoch 264/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0761 - acc: 0.9780 - val_loss: 0.0737 - val_acc: 0.9701\n",
      "Epoch 265/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0759 - acc: 0.9790 - val_loss: 0.0741 - val_acc: 0.9701\n",
      "Epoch 266/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0759 - acc: 0.9770 - val_loss: 0.0736 - val_acc: 0.9701\n",
      "Epoch 267/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0757 - acc: 0.9780 - val_loss: 0.0734 - val_acc: 0.9701\n",
      "Epoch 268/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0757 - acc: 0.9780 - val_loss: 0.0732 - val_acc: 0.9701\n",
      "Epoch 269/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0756 - acc: 0.9780 - val_loss: 0.0731 - val_acc: 0.9701\n",
      "Epoch 270/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0754 - acc: 0.9780 - val_loss: 0.0734 - val_acc: 0.9701\n",
      "Epoch 271/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0754 - acc: 0.9770 - val_loss: 0.0729 - val_acc: 0.9701\n",
      "Epoch 272/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0754 - acc: 0.9780 - val_loss: 0.0728 - val_acc: 0.9701\n",
      "Epoch 273/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0752 - acc: 0.9780 - val_loss: 0.0725 - val_acc: 0.9701\n",
      "Epoch 274/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0752 - acc: 0.9780 - val_loss: 0.0724 - val_acc: 0.9701\n",
      "Epoch 275/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0750 - acc: 0.9800 - val_loss: 0.0726 - val_acc: 0.9731\n",
      "Epoch 276/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0749 - acc: 0.9770 - val_loss: 0.0725 - val_acc: 0.9701\n",
      "Epoch 277/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0749 - acc: 0.9780 - val_loss: 0.0725 - val_acc: 0.9701\n",
      "Epoch 278/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0747 - acc: 0.9790 - val_loss: 0.0725 - val_acc: 0.9731\n",
      "Epoch 279/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0747 - acc: 0.9770 - val_loss: 0.0718 - val_acc: 0.9731\n",
      "Epoch 280/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0748 - acc: 0.9790 - val_loss: 0.0724 - val_acc: 0.9731\n",
      "Epoch 281/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0745 - acc: 0.9780 - val_loss: 0.0721 - val_acc: 0.9731\n",
      "Epoch 282/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0744 - acc: 0.9790 - val_loss: 0.0719 - val_acc: 0.9701\n",
      "Epoch 283/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0743 - acc: 0.9780 - val_loss: 0.0718 - val_acc: 0.9701\n",
      "Epoch 284/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0743 - acc: 0.9770 - val_loss: 0.0718 - val_acc: 0.9701\n",
      "Epoch 285/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0741 - acc: 0.9780 - val_loss: 0.0720 - val_acc: 0.9701\n",
      "Epoch 286/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0740 - acc: 0.9770 - val_loss: 0.0718 - val_acc: 0.9701\n",
      "Epoch 287/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0740 - acc: 0.9790 - val_loss: 0.0717 - val_acc: 0.9701\n",
      "Epoch 288/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0739 - acc: 0.9770 - val_loss: 0.0714 - val_acc: 0.9701\n",
      "Epoch 289/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0737 - acc: 0.9760 - val_loss: 0.0715 - val_acc: 0.9671\n",
      "Epoch 290/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0737 - acc: 0.9790 - val_loss: 0.0714 - val_acc: 0.9701\n",
      "Epoch 291/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0737 - acc: 0.9780 - val_loss: 0.0711 - val_acc: 0.9701\n",
      "Epoch 292/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0736 - acc: 0.9770 - val_loss: 0.0714 - val_acc: 0.9701\n",
      "Epoch 293/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0734 - acc: 0.9780 - val_loss: 0.0712 - val_acc: 0.9701\n",
      "Epoch 294/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0733 - acc: 0.9780 - val_loss: 0.0709 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0733 - acc: 0.9790 - val_loss: 0.0709 - val_acc: 0.9701\n",
      "Epoch 296/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0731 - acc: 0.9780 - val_loss: 0.0710 - val_acc: 0.9701\n",
      "Epoch 297/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0731 - acc: 0.9780 - val_loss: 0.0708 - val_acc: 0.9701\n",
      "Epoch 298/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0730 - acc: 0.9780 - val_loss: 0.0712 - val_acc: 0.9701\n",
      "Epoch 299/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0731 - acc: 0.9780 - val_loss: 0.0711 - val_acc: 0.9701\n",
      "Epoch 300/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0729 - acc: 0.9770 - val_loss: 0.0708 - val_acc: 0.9701\n",
      "Epoch 301/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0727 - acc: 0.9780 - val_loss: 0.0707 - val_acc: 0.9701\n",
      "Epoch 302/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0728 - acc: 0.9780 - val_loss: 0.0705 - val_acc: 0.9701\n",
      "Epoch 303/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0728 - acc: 0.9780 - val_loss: 0.0706 - val_acc: 0.9671\n",
      "Epoch 304/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0725 - acc: 0.9780 - val_loss: 0.0705 - val_acc: 0.9701\n",
      "Epoch 305/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0726 - acc: 0.9780 - val_loss: 0.0705 - val_acc: 0.9701\n",
      "Epoch 306/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0724 - acc: 0.9780 - val_loss: 0.0703 - val_acc: 0.9701\n",
      "Epoch 307/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0723 - acc: 0.9780 - val_loss: 0.0701 - val_acc: 0.9701\n",
      "Epoch 308/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0723 - acc: 0.9790 - val_loss: 0.0700 - val_acc: 0.9701\n",
      "Epoch 309/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0721 - acc: 0.9780 - val_loss: 0.0699 - val_acc: 0.9701\n",
      "Epoch 310/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0722 - acc: 0.9770 - val_loss: 0.0699 - val_acc: 0.9701\n",
      "Epoch 311/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0722 - acc: 0.9780 - val_loss: 0.0700 - val_acc: 0.9701\n",
      "Epoch 312/3000\n",
      "1002/1002 [==============================] - 0s 149us/step - loss: 0.0720 - acc: 0.9790 - val_loss: 0.0700 - val_acc: 0.9731\n",
      "Epoch 313/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 0.0719 - acc: 0.9780 - val_loss: 0.0698 - val_acc: 0.9701\n",
      "Epoch 314/3000\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0719 - acc: 0.9780 - val_loss: 0.0699 - val_acc: 0.9701\n",
      "Epoch 315/3000\n",
      "1002/1002 [==============================] - 0s 137us/step - loss: 0.0719 - acc: 0.9780 - val_loss: 0.0698 - val_acc: 0.9701\n",
      "Epoch 316/3000\n",
      "1002/1002 [==============================] - 0s 128us/step - loss: 0.0718 - acc: 0.9790 - val_loss: 0.0698 - val_acc: 0.9701\n",
      "Epoch 317/3000\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0716 - acc: 0.9790 - val_loss: 0.0700 - val_acc: 0.9701\n",
      "Epoch 318/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0717 - acc: 0.9780 - val_loss: 0.0702 - val_acc: 0.9701\n",
      "Epoch 319/3000\n",
      "1002/1002 [==============================] - 0s 126us/step - loss: 0.0716 - acc: 0.9780 - val_loss: 0.0698 - val_acc: 0.9701\n",
      "Epoch 320/3000\n",
      "1002/1002 [==============================] - 0s 112us/step - loss: 0.0716 - acc: 0.9770 - val_loss: 0.0696 - val_acc: 0.9701\n",
      "Epoch 321/3000\n",
      "1002/1002 [==============================] - 0s 115us/step - loss: 0.0713 - acc: 0.9790 - val_loss: 0.0699 - val_acc: 0.9731\n",
      "Epoch 322/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 0.0714 - acc: 0.9770 - val_loss: 0.0696 - val_acc: 0.9701\n",
      "Epoch 323/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0712 - acc: 0.9780 - val_loss: 0.0693 - val_acc: 0.9701\n",
      "Epoch 324/3000\n",
      "1002/1002 [==============================] - 0s 112us/step - loss: 0.0712 - acc: 0.9780 - val_loss: 0.0691 - val_acc: 0.9701\n",
      "Epoch 325/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0712 - acc: 0.9780 - val_loss: 0.0695 - val_acc: 0.9701\n",
      "Epoch 326/3000\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 0.0712 - acc: 0.9780 - val_loss: 0.0691 - val_acc: 0.9701\n",
      "Epoch 327/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.0709 - acc: 0.9790 - val_loss: 0.0689 - val_acc: 0.9701\n",
      "Epoch 328/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0709 - acc: 0.9780 - val_loss: 0.0688 - val_acc: 0.9701\n",
      "Epoch 329/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0709 - acc: 0.9780 - val_loss: 0.0686 - val_acc: 0.9701\n",
      "Epoch 330/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0708 - acc: 0.9800 - val_loss: 0.0687 - val_acc: 0.9701\n",
      "Epoch 331/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0707 - acc: 0.9780 - val_loss: 0.0685 - val_acc: 0.9701\n",
      "Epoch 332/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0707 - acc: 0.9780 - val_loss: 0.0685 - val_acc: 0.9671\n",
      "Epoch 333/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0706 - acc: 0.9790 - val_loss: 0.0684 - val_acc: 0.9701\n",
      "Epoch 334/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0706 - acc: 0.9780 - val_loss: 0.0683 - val_acc: 0.9701\n",
      "Epoch 335/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0705 - acc: 0.9780 - val_loss: 0.0680 - val_acc: 0.9701\n",
      "Epoch 336/3000\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0704 - acc: 0.9790 - val_loss: 0.0681 - val_acc: 0.9701\n",
      "Epoch 337/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0703 - acc: 0.9780 - val_loss: 0.0680 - val_acc: 0.9701\n",
      "Epoch 338/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0703 - acc: 0.9790 - val_loss: 0.0679 - val_acc: 0.9701\n",
      "Epoch 339/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0703 - acc: 0.9790 - val_loss: 0.0678 - val_acc: 0.9701\n",
      "Epoch 340/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0702 - acc: 0.9780 - val_loss: 0.0678 - val_acc: 0.9701\n",
      "Epoch 341/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0700 - acc: 0.9800 - val_loss: 0.0680 - val_acc: 0.9701\n",
      "Epoch 342/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0701 - acc: 0.9780 - val_loss: 0.0680 - val_acc: 0.9701\n",
      "Epoch 343/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0700 - acc: 0.9790 - val_loss: 0.0679 - val_acc: 0.9701\n",
      "Epoch 344/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0700 - acc: 0.9780 - val_loss: 0.0679 - val_acc: 0.9701\n",
      "Epoch 345/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0699 - acc: 0.9780 - val_loss: 0.0678 - val_acc: 0.9701\n",
      "Epoch 346/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0699 - acc: 0.9780 - val_loss: 0.0677 - val_acc: 0.9701\n",
      "Epoch 347/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0698 - acc: 0.9770 - val_loss: 0.0675 - val_acc: 0.9701\n",
      "Epoch 348/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0698 - acc: 0.9780 - val_loss: 0.0675 - val_acc: 0.9701\n",
      "Epoch 349/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0697 - acc: 0.9790 - val_loss: 0.0675 - val_acc: 0.9701\n",
      "Epoch 350/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0698 - acc: 0.9780 - val_loss: 0.0675 - val_acc: 0.9701\n",
      "Epoch 351/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0696 - acc: 0.9790 - val_loss: 0.0675 - val_acc: 0.9701\n",
      "Epoch 352/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0695 - acc: 0.9790 - val_loss: 0.0677 - val_acc: 0.9701\n",
      "Epoch 353/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0694 - acc: 0.9780 - val_loss: 0.0677 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0694 - acc: 0.9780 - val_loss: 0.0672 - val_acc: 0.9701\n",
      "Epoch 355/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 0.0694 - acc: 0.9780 - val_loss: 0.0673 - val_acc: 0.9701\n",
      "Epoch 356/3000\n",
      "1002/1002 [==============================] - 0s 139us/step - loss: 0.0692 - acc: 0.9790 - val_loss: 0.0673 - val_acc: 0.9701\n",
      "Epoch 357/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 0.0691 - acc: 0.9790 - val_loss: 0.0671 - val_acc: 0.9701\n",
      "Epoch 358/3000\n",
      "1002/1002 [==============================] - 0s 159us/step - loss: 0.0691 - acc: 0.9790 - val_loss: 0.0672 - val_acc: 0.9701\n",
      "Epoch 359/3000\n",
      "1002/1002 [==============================] - 0s 144us/step - loss: 0.0691 - acc: 0.9790 - val_loss: 0.0672 - val_acc: 0.9701\n",
      "Epoch 360/3000\n",
      "1002/1002 [==============================] - 0s 123us/step - loss: 0.0690 - acc: 0.9780 - val_loss: 0.0673 - val_acc: 0.9701\n",
      "Epoch 361/3000\n",
      "1002/1002 [==============================] - 0s 146us/step - loss: 0.0689 - acc: 0.9780 - val_loss: 0.0672 - val_acc: 0.9701\n",
      "Epoch 362/3000\n",
      "1002/1002 [==============================] - 0s 145us/step - loss: 0.0688 - acc: 0.9780 - val_loss: 0.0670 - val_acc: 0.9701\n",
      "Epoch 363/3000\n",
      "1002/1002 [==============================] - 0s 129us/step - loss: 0.0689 - acc: 0.9790 - val_loss: 0.0669 - val_acc: 0.9701\n",
      "Epoch 364/3000\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 0.0688 - acc: 0.9790 - val_loss: 0.0670 - val_acc: 0.9701\n",
      "Epoch 365/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0687 - acc: 0.9790 - val_loss: 0.0668 - val_acc: 0.9701\n",
      "Epoch 366/3000\n",
      "1002/1002 [==============================] - 0s 133us/step - loss: 0.0687 - acc: 0.9780 - val_loss: 0.0669 - val_acc: 0.9701\n",
      "Epoch 367/3000\n",
      "1002/1002 [==============================] - 0s 132us/step - loss: 0.0688 - acc: 0.9790 - val_loss: 0.0670 - val_acc: 0.9701\n",
      "Epoch 368/3000\n",
      "1002/1002 [==============================] - 0s 142us/step - loss: 0.0686 - acc: 0.9780 - val_loss: 0.0665 - val_acc: 0.9701\n",
      "Epoch 369/3000\n",
      "1002/1002 [==============================] - 0s 141us/step - loss: 0.0685 - acc: 0.9790 - val_loss: 0.0663 - val_acc: 0.9701\n",
      "Epoch 370/3000\n",
      "1002/1002 [==============================] - 0s 118us/step - loss: 0.0684 - acc: 0.9790 - val_loss: 0.0664 - val_acc: 0.9701\n",
      "Epoch 371/3000\n",
      "1002/1002 [==============================] - 0s 153us/step - loss: 0.0685 - acc: 0.9780 - val_loss: 0.0663 - val_acc: 0.9701\n",
      "Epoch 372/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 0.0684 - acc: 0.9790 - val_loss: 0.0663 - val_acc: 0.9701\n",
      "Epoch 373/3000\n",
      "1002/1002 [==============================] - 0s 118us/step - loss: 0.0683 - acc: 0.9790 - val_loss: 0.0661 - val_acc: 0.9701\n",
      "Epoch 374/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0683 - acc: 0.9780 - val_loss: 0.0658 - val_acc: 0.9701\n",
      "Epoch 375/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0683 - acc: 0.9790 - val_loss: 0.0659 - val_acc: 0.9701\n",
      "Epoch 376/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0682 - acc: 0.9780 - val_loss: 0.0658 - val_acc: 0.9701\n",
      "Epoch 377/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0681 - acc: 0.9780 - val_loss: 0.0655 - val_acc: 0.9701\n",
      "Epoch 378/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0682 - acc: 0.9780 - val_loss: 0.0656 - val_acc: 0.9701\n",
      "Epoch 379/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0680 - acc: 0.9790 - val_loss: 0.0655 - val_acc: 0.9701\n",
      "Epoch 380/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0681 - acc: 0.9800 - val_loss: 0.0657 - val_acc: 0.9701\n",
      "Epoch 381/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0679 - acc: 0.9770 - val_loss: 0.0657 - val_acc: 0.9671\n",
      "Epoch 382/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0679 - acc: 0.9800 - val_loss: 0.0656 - val_acc: 0.9701\n",
      "Epoch 383/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0679 - acc: 0.9790 - val_loss: 0.0658 - val_acc: 0.9701\n",
      "Epoch 384/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0678 - acc: 0.9780 - val_loss: 0.0658 - val_acc: 0.9701\n",
      "Epoch 385/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0679 - acc: 0.9790 - val_loss: 0.0658 - val_acc: 0.9701\n",
      "Epoch 386/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0677 - acc: 0.9790 - val_loss: 0.0656 - val_acc: 0.9701\n",
      "Epoch 387/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0675 - acc: 0.9780 - val_loss: 0.0648 - val_acc: 0.9701\n",
      "Epoch 388/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.0650 - val_acc: 0.9701\n",
      "Epoch 389/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.0652 - val_acc: 0.9701\n",
      "Epoch 390/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0677 - acc: 0.9790 - val_loss: 0.0653 - val_acc: 0.9701\n",
      "Epoch 391/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0674 - acc: 0.9790 - val_loss: 0.0652 - val_acc: 0.9701\n",
      "Epoch 392/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0675 - acc: 0.9790 - val_loss: 0.0656 - val_acc: 0.9701\n",
      "Epoch 393/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0674 - acc: 0.9780 - val_loss: 0.0654 - val_acc: 0.9701\n",
      "Epoch 394/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0673 - acc: 0.9770 - val_loss: 0.0651 - val_acc: 0.9671\n",
      "Epoch 395/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0673 - acc: 0.9790 - val_loss: 0.0649 - val_acc: 0.9701\n",
      "Epoch 396/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0672 - acc: 0.9780 - val_loss: 0.0649 - val_acc: 0.9701\n",
      "Epoch 397/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0672 - acc: 0.9790 - val_loss: 0.0651 - val_acc: 0.9701\n",
      "Epoch 398/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0672 - acc: 0.9780 - val_loss: 0.0651 - val_acc: 0.9701\n",
      "Epoch 399/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0672 - acc: 0.9790 - val_loss: 0.0651 - val_acc: 0.9701\n",
      "Epoch 400/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0670 - acc: 0.9780 - val_loss: 0.0655 - val_acc: 0.9671\n",
      "Epoch 401/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0670 - acc: 0.9790 - val_loss: 0.0653 - val_acc: 0.9671\n",
      "Epoch 402/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.0670 - acc: 0.9790 - val_loss: 0.0652 - val_acc: 0.9701\n",
      "Epoch 403/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 0.0669 - acc: 0.9780 - val_loss: 0.0651 - val_acc: 0.9701\n",
      "Epoch 404/3000\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0668 - acc: 0.9790 - val_loss: 0.0647 - val_acc: 0.9701\n",
      "Epoch 405/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0670 - acc: 0.9790 - val_loss: 0.0646 - val_acc: 0.9671\n",
      "Epoch 406/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0669 - acc: 0.9800 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 407/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0668 - acc: 0.9790 - val_loss: 0.0647 - val_acc: 0.9701\n",
      "Epoch 408/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0667 - acc: 0.9790 - val_loss: 0.0649 - val_acc: 0.9701\n",
      "Epoch 409/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.0690 - acc: 0.976 - 0s 99us/step - loss: 0.0667 - acc: 0.9780 - val_loss: 0.0648 - val_acc: 0.9701\n",
      "Epoch 410/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0666 - acc: 0.9780 - val_loss: 0.0645 - val_acc: 0.9671\n",
      "Epoch 411/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0666 - acc: 0.9790 - val_loss: 0.0645 - val_acc: 0.9671\n",
      "Epoch 412/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0666 - acc: 0.9780 - val_loss: 0.0644 - val_acc: 0.9671\n",
      "Epoch 413/3000\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0665 - acc: 0.9780 - val_loss: 0.0646 - val_acc: 0.9641\n",
      "Epoch 414/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0664 - acc: 0.9790 - val_loss: 0.0650 - val_acc: 0.9701\n",
      "Epoch 415/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0663 - acc: 0.9780 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 416/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0664 - acc: 0.9790 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 417/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0663 - acc: 0.9790 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 418/3000\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0663 - acc: 0.9780 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 419/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0663 - acc: 0.9780 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 420/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0662 - acc: 0.9790 - val_loss: 0.0645 - val_acc: 0.9701\n",
      "Epoch 421/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0661 - acc: 0.9790 - val_loss: 0.0649 - val_acc: 0.9701\n",
      "Epoch 422/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0662 - acc: 0.9780 - val_loss: 0.0645 - val_acc: 0.9701\n",
      "Epoch 423/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0660 - acc: 0.9770 - val_loss: 0.0646 - val_acc: 0.9701\n",
      "Epoch 424/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0661 - acc: 0.9780 - val_loss: 0.0645 - val_acc: 0.9671\n",
      "Epoch 425/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0661 - acc: 0.9780 - val_loss: 0.0644 - val_acc: 0.9671\n",
      "Epoch 426/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0660 - acc: 0.9780 - val_loss: 0.0647 - val_acc: 0.9701\n",
      "Epoch 427/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0660 - acc: 0.9780 - val_loss: 0.0641 - val_acc: 0.9671\n",
      "Epoch 428/3000\n",
      "1002/1002 [==============================] - 0s 155us/step - loss: 0.0660 - acc: 0.9790 - val_loss: 0.0641 - val_acc: 0.9671\n",
      "Epoch 429/3000\n",
      "1002/1002 [==============================] - 0s 148us/step - loss: 0.0658 - acc: 0.9800 - val_loss: 0.0643 - val_acc: 0.9701\n",
      "Epoch 430/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0658 - acc: 0.9780 - val_loss: 0.0641 - val_acc: 0.9701\n",
      "Epoch 431/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.0657 - acc: 0.9780 - val_loss: 0.0642 - val_acc: 0.9701\n",
      "Epoch 432/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0657 - acc: 0.9780 - val_loss: 0.0640 - val_acc: 0.9701\n",
      "Epoch 433/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0657 - acc: 0.9780 - val_loss: 0.0640 - val_acc: 0.9701\n",
      "Epoch 434/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0657 - acc: 0.9780 - val_loss: 0.0638 - val_acc: 0.9671\n",
      "Epoch 435/3000\n",
      "1002/1002 [==============================] - 0s 148us/step - loss: 0.0657 - acc: 0.9780 - val_loss: 0.0636 - val_acc: 0.9671\n",
      "Epoch 436/3000\n",
      "1002/1002 [==============================] - 0s 139us/step - loss: 0.0656 - acc: 0.9790 - val_loss: 0.0636 - val_acc: 0.9671\n",
      "Epoch 437/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0655 - acc: 0.9800 - val_loss: 0.0637 - val_acc: 0.9701\n",
      "Epoch 438/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0656 - acc: 0.9780 - val_loss: 0.0637 - val_acc: 0.9701\n",
      "Epoch 439/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0655 - acc: 0.9780 - val_loss: 0.0637 - val_acc: 0.9701\n",
      "Epoch 440/3000\n",
      "1002/1002 [==============================] - 0s 123us/step - loss: 0.0654 - acc: 0.9780 - val_loss: 0.0635 - val_acc: 0.9701\n",
      "Epoch 441/3000\n",
      "1002/1002 [==============================] - 0s 117us/step - loss: 0.0654 - acc: 0.9790 - val_loss: 0.0639 - val_acc: 0.9701\n",
      "Epoch 442/3000\n",
      "1002/1002 [==============================] - 0s 111us/step - loss: 0.0654 - acc: 0.9780 - val_loss: 0.0638 - val_acc: 0.9701\n",
      "Epoch 443/3000\n",
      "1002/1002 [==============================] - 0s 130us/step - loss: 0.0653 - acc: 0.9780 - val_loss: 0.0637 - val_acc: 0.9701\n",
      "Epoch 444/3000\n",
      "1002/1002 [==============================] - 0s 126us/step - loss: 0.0654 - acc: 0.9780 - val_loss: 0.0635 - val_acc: 0.9701\n",
      "Epoch 445/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0653 - acc: 0.9780 - val_loss: 0.0635 - val_acc: 0.9701\n",
      "Epoch 446/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0652 - acc: 0.9780 - val_loss: 0.0633 - val_acc: 0.9701\n",
      "Epoch 447/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0653 - acc: 0.9790 - val_loss: 0.0633 - val_acc: 0.9701\n",
      "Epoch 448/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0651 - acc: 0.9790 - val_loss: 0.0634 - val_acc: 0.9671\n",
      "Epoch 449/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0651 - acc: 0.9790 - val_loss: 0.0629 - val_acc: 0.9701\n",
      "Epoch 450/3000\n",
      "1002/1002 [==============================] - 0s 116us/step - loss: 0.0653 - acc: 0.9780 - val_loss: 0.0628 - val_acc: 0.9671\n",
      "Epoch 451/3000\n",
      "1002/1002 [==============================] - 0s 120us/step - loss: 0.0651 - acc: 0.9790 - val_loss: 0.0629 - val_acc: 0.9671\n",
      "Epoch 452/3000\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0650 - acc: 0.9800 - val_loss: 0.0630 - val_acc: 0.9671\n",
      "Epoch 453/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0649 - acc: 0.9790 - val_loss: 0.0630 - val_acc: 0.9701\n",
      "Epoch 454/3000\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0650 - acc: 0.9790 - val_loss: 0.0630 - val_acc: 0.9701\n",
      "Epoch 455/3000\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0650 - acc: 0.9780 - val_loss: 0.0630 - val_acc: 0.9701\n",
      "Epoch 456/3000\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0648 - acc: 0.9790 - val_loss: 0.0629 - val_acc: 0.9701\n",
      "Epoch 457/3000\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0648 - acc: 0.9790 - val_loss: 0.0629 - val_acc: 0.9701\n",
      "Epoch 458/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0648 - acc: 0.9780 - val_loss: 0.0630 - val_acc: 0.9701\n",
      "Epoch 459/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0647 - acc: 0.9780 - val_loss: 0.0624 - val_acc: 0.9701\n",
      "Epoch 460/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0648 - acc: 0.9780 - val_loss: 0.0623 - val_acc: 0.9671\n",
      "Epoch 461/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0647 - acc: 0.9800 - val_loss: 0.0625 - val_acc: 0.9671\n",
      "Epoch 462/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0647 - acc: 0.9790 - val_loss: 0.0626 - val_acc: 0.9671\n",
      "Epoch 463/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0646 - acc: 0.9800 - val_loss: 0.0627 - val_acc: 0.9701\n",
      "Epoch 464/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0646 - acc: 0.9800 - val_loss: 0.0629 - val_acc: 0.9671\n",
      "Epoch 465/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0645 - acc: 0.9780 - val_loss: 0.0629 - val_acc: 0.9671\n",
      "Epoch 466/3000\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0633 - val_acc: 0.9641\n",
      "Epoch 467/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.0645 - acc: 0.9780 - val_loss: 0.0631 - val_acc: 0.9671\n",
      "Epoch 468/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0633 - val_acc: 0.9641\n",
      "Epoch 469/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0631 - val_acc: 0.9671\n",
      "Epoch 470/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0630 - val_acc: 0.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0630 - val_acc: 0.9701\n",
      "Epoch 472/3000\n",
      "1002/1002 [==============================] - 0s 138us/step - loss: 0.0644 - acc: 0.9790 - val_loss: 0.0628 - val_acc: 0.9701\n",
      "Epoch 473/3000\n",
      "1002/1002 [==============================] - 0s 166us/step - loss: 0.0642 - acc: 0.9780 - val_loss: 0.0626 - val_acc: 0.9671\n",
      "Epoch 474/3000\n",
      "1002/1002 [==============================] - 0s 138us/step - loss: 0.0642 - acc: 0.9790 - val_loss: 0.0629 - val_acc: 0.9671\n",
      "Epoch 475/3000\n",
      "1002/1002 [==============================] - 0s 155us/step - loss: 0.0643 - acc: 0.9790 - val_loss: 0.0627 - val_acc: 0.9671\n",
      "Epoch 476/3000\n",
      "1002/1002 [==============================] - 0s 158us/step - loss: 0.0642 - acc: 0.9790 - val_loss: 0.0630 - val_acc: 0.9701\n",
      "Epoch 477/3000\n",
      "1002/1002 [==============================] - 0s 144us/step - loss: 0.0644 - acc: 0.9780 - val_loss: 0.0629 - val_acc: 0.9671\n",
      "Epoch 478/3000\n",
      "1002/1002 [==============================] - 0s 144us/step - loss: 0.0642 - acc: 0.9770 - val_loss: 0.0626 - val_acc: 0.9671\n",
      "Epoch 479/3000\n",
      "1002/1002 [==============================] - 0s 163us/step - loss: 0.0641 - acc: 0.9780 - val_loss: 0.0621 - val_acc: 0.9671\n",
      "Epoch 480/3000\n",
      "1002/1002 [==============================] - 0s 131us/step - loss: 0.0640 - acc: 0.9790 - val_loss: 0.0625 - val_acc: 0.9701\n",
      "Epoch 481/3000\n",
      "1002/1002 [==============================] - 0s 129us/step - loss: 0.0642 - acc: 0.9780 - val_loss: 0.0623 - val_acc: 0.9671\n",
      "Epoch 482/3000\n",
      "1002/1002 [==============================] - 0s 124us/step - loss: 0.0641 - acc: 0.9780 - val_loss: 0.0621 - val_acc: 0.9671\n",
      "Epoch 483/3000\n",
      "1002/1002 [==============================] - 0s 140us/step - loss: 0.0640 - acc: 0.9790 - val_loss: 0.0622 - val_acc: 0.9671\n",
      "Epoch 484/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0640 - acc: 0.9790 - val_loss: 0.0622 - val_acc: 0.9701\n",
      "Epoch 485/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0639 - acc: 0.9790 - val_loss: 0.0625 - val_acc: 0.9701\n",
      "Epoch 486/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0639 - acc: 0.9770 - val_loss: 0.0620 - val_acc: 0.9701\n",
      "Epoch 487/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0639 - acc: 0.9780 - val_loss: 0.0617 - val_acc: 0.9671\n",
      "Epoch 488/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0638 - acc: 0.9790 - val_loss: 0.0617 - val_acc: 0.9701\n",
      "Epoch 489/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0637 - acc: 0.9790 - val_loss: 0.0616 - val_acc: 0.9671\n",
      "Epoch 490/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0638 - acc: 0.9790 - val_loss: 0.0614 - val_acc: 0.9671\n",
      "Epoch 491/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.0637 - acc: 0.9800 - val_loss: 0.0617 - val_acc: 0.9701\n",
      "Epoch 492/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0637 - acc: 0.9790 - val_loss: 0.0621 - val_acc: 0.9701\n",
      "Epoch 493/3000\n",
      "1002/1002 [==============================] - 0s 140us/step - loss: 0.0638 - acc: 0.9780 - val_loss: 0.0619 - val_acc: 0.9701\n",
      "Epoch 494/3000\n",
      "1002/1002 [==============================] - 0s 138us/step - loss: 0.0637 - acc: 0.9780 - val_loss: 0.0617 - val_acc: 0.9701\n",
      "Epoch 495/3000\n",
      "1002/1002 [==============================] - 0s 131us/step - loss: 0.0636 - acc: 0.9780 - val_loss: 0.0616 - val_acc: 0.9671\n",
      "Epoch 496/3000\n",
      "1002/1002 [==============================] - 0s 115us/step - loss: 0.0635 - acc: 0.9790 - val_loss: 0.0613 - val_acc: 0.9701\n",
      "Epoch 497/3000\n",
      "1002/1002 [==============================] - 0s 130us/step - loss: 0.0636 - acc: 0.9790 - val_loss: 0.0614 - val_acc: 0.9701\n",
      "Epoch 498/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.0704 - acc: 0.975 - 0s 121us/step - loss: 0.0635 - acc: 0.9790 - val_loss: 0.0614 - val_acc: 0.9671\n",
      "Epoch 499/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0635 - acc: 0.9790 - val_loss: 0.0614 - val_acc: 0.9671\n",
      "Epoch 500/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 0.0636 - acc: 0.9790 - val_loss: 0.0615 - val_acc: 0.9671\n",
      "Epoch 501/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.0634 - acc: 0.9790 - val_loss: 0.0616 - val_acc: 0.9701\n",
      "Epoch 502/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0634 - acc: 0.9780 - val_loss: 0.0616 - val_acc: 0.9701\n",
      "Epoch 503/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0634 - acc: 0.9790 - val_loss: 0.0617 - val_acc: 0.9701\n",
      "Epoch 504/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0633 - acc: 0.9780 - val_loss: 0.0615 - val_acc: 0.9671\n",
      "Epoch 505/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0632 - acc: 0.9790 - val_loss: 0.0615 - val_acc: 0.9671\n",
      "Epoch 506/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0632 - acc: 0.9790 - val_loss: 0.0614 - val_acc: 0.9671\n",
      "Epoch 507/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0632 - acc: 0.9790 - val_loss: 0.0615 - val_acc: 0.9671\n",
      "Epoch 508/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0633 - acc: 0.9790 - val_loss: 0.0618 - val_acc: 0.9701\n",
      "Epoch 509/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0631 - acc: 0.9780 - val_loss: 0.0615 - val_acc: 0.9671\n",
      "Epoch 510/3000\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0632 - acc: 0.9780 - val_loss: 0.0619 - val_acc: 0.9701\n",
      "Epoch 511/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0632 - acc: 0.9780 - val_loss: 0.0619 - val_acc: 0.9671\n",
      "Epoch 512/3000\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0631 - acc: 0.9780 - val_loss: 0.0615 - val_acc: 0.9701\n",
      "Epoch 513/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.0631 - acc: 0.9790 - val_loss: 0.0614 - val_acc: 0.9701\n",
      "Epoch 514/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0631 - acc: 0.9790 - val_loss: 0.0611 - val_acc: 0.9671\n",
      "Epoch 515/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0630 - acc: 0.9790 - val_loss: 0.0612 - val_acc: 0.9671\n",
      "Epoch 516/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0630 - acc: 0.9790 - val_loss: 0.0611 - val_acc: 0.9671\n",
      "Epoch 517/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0629 - acc: 0.9790 - val_loss: 0.0611 - val_acc: 0.9671\n",
      "Epoch 518/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0630 - acc: 0.9800 - val_loss: 0.0612 - val_acc: 0.9671\n",
      "Epoch 519/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0629 - acc: 0.9780 - val_loss: 0.0613 - val_acc: 0.9671\n",
      "Epoch 520/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0630 - acc: 0.9790 - val_loss: 0.0612 - val_acc: 0.9671\n",
      "Epoch 521/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0629 - acc: 0.9790 - val_loss: 0.0611 - val_acc: 0.9671\n",
      "Epoch 522/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0628 - acc: 0.9790 - val_loss: 0.0609 - val_acc: 0.9671\n",
      "Epoch 523/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0627 - acc: 0.9790 - val_loss: 0.0611 - val_acc: 0.9671\n",
      "Epoch 524/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0627 - acc: 0.9770 - val_loss: 0.0612 - val_acc: 0.9671\n",
      "Epoch 525/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0628 - acc: 0.9780 - val_loss: 0.0611 - val_acc: 0.9671\n",
      "Epoch 526/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0627 - acc: 0.9790 - val_loss: 0.0613 - val_acc: 0.9641\n",
      "Epoch 527/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0627 - acc: 0.9780 - val_loss: 0.0609 - val_acc: 0.9671\n",
      "Epoch 528/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0626 - acc: 0.9780 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 529/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0627 - acc: 0.9790 - val_loss: 0.0605 - val_acc: 0.9671\n",
      "Epoch 530/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0627 - acc: 0.9790 - val_loss: 0.0605 - val_acc: 0.9671\n",
      "Epoch 531/3000\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.0626 - acc: 0.9800 - val_loss: 0.0605 - val_acc: 0.9671\n",
      "Epoch 532/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0626 - acc: 0.9790 - val_loss: 0.0603 - val_acc: 0.9671\n",
      "Epoch 533/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0625 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 534/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0625 - acc: 0.9790 - val_loss: 0.0603 - val_acc: 0.9671\n",
      "Epoch 535/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0624 - acc: 0.9780 - val_loss: 0.0602 - val_acc: 0.9671\n",
      "Epoch 536/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0625 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 537/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0624 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9701\n",
      "Epoch 538/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0624 - acc: 0.9780 - val_loss: 0.0604 - val_acc: 0.9701\n",
      "Epoch 539/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0624 - acc: 0.9790 - val_loss: 0.0605 - val_acc: 0.9701\n",
      "Epoch 540/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0623 - acc: 0.9790 - val_loss: 0.0606 - val_acc: 0.9671\n",
      "Epoch 541/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0623 - acc: 0.9780 - val_loss: 0.0606 - val_acc: 0.9671\n",
      "Epoch 542/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0623 - acc: 0.9790 - val_loss: 0.0605 - val_acc: 0.9671\n",
      "Epoch 543/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0623 - acc: 0.9800 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 544/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0623 - acc: 0.9780 - val_loss: 0.0602 - val_acc: 0.9671\n",
      "Epoch 545/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0622 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 546/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0621 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 547/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0622 - acc: 0.9780 - val_loss: 0.0601 - val_acc: 0.9671\n",
      "Epoch 548/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0621 - acc: 0.9790 - val_loss: 0.0600 - val_acc: 0.9671\n",
      "Epoch 549/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0621 - acc: 0.9790 - val_loss: 0.0601 - val_acc: 0.9671\n",
      "Epoch 550/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0621 - acc: 0.9800 - val_loss: 0.0602 - val_acc: 0.9671\n",
      "Epoch 551/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0622 - acc: 0.9780 - val_loss: 0.0600 - val_acc: 0.9671\n",
      "Epoch 552/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0620 - acc: 0.9800 - val_loss: 0.0601 - val_acc: 0.9671\n",
      "Epoch 553/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0620 - acc: 0.9800 - val_loss: 0.0602 - val_acc: 0.9671\n",
      "Epoch 554/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0620 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 555/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0620 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9671\n",
      "Epoch 556/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0620 - acc: 0.9790 - val_loss: 0.0602 - val_acc: 0.9671\n",
      "Epoch 557/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0619 - acc: 0.9790 - val_loss: 0.0601 - val_acc: 0.9671\n",
      "Epoch 558/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0619 - acc: 0.9790 - val_loss: 0.0604 - val_acc: 0.9641\n",
      "Epoch 559/3000\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0619 - acc: 0.9800 - val_loss: 0.0599 - val_acc: 0.9671\n",
      "Epoch 560/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.0600 - val_acc: 0.9671\n",
      "Epoch 561/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.0597 - val_acc: 0.9671\n",
      "Epoch 562/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.0597 - val_acc: 0.9671\n",
      "Epoch 563/3000\n",
      "1002/1002 [==============================] - 0s 67us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.0596 - val_acc: 0.9671\n",
      "Epoch 564/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 565/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0618 - acc: 0.9790 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 566/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0617 - acc: 0.9800 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 567/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 568/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0616 - acc: 0.9790 - val_loss: 0.0595 - val_acc: 0.9671\n",
      "Epoch 569/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.0596 - val_acc: 0.9671\n",
      "Epoch 570/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0616 - acc: 0.9790 - val_loss: 0.0595 - val_acc: 0.9671\n",
      "Epoch 571/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0616 - acc: 0.9790 - val_loss: 0.0593 - val_acc: 0.9671\n",
      "Epoch 572/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0615 - acc: 0.9800 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 573/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.0595 - val_acc: 0.9701\n",
      "Epoch 574/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.0595 - val_acc: 0.9671\n",
      "Epoch 575/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.0596 - val_acc: 0.9671\n",
      "Epoch 576/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 577/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0614 - acc: 0.9800 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 578/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0614 - acc: 0.9800 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 579/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0614 - acc: 0.9800 - val_loss: 0.0591 - val_acc: 0.9671\n",
      "Epoch 580/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0614 - acc: 0.9800 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 581/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0612 - acc: 0.9800 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 582/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0613 - acc: 0.9790 - val_loss: 0.0593 - val_acc: 0.9671\n",
      "Epoch 583/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0613 - acc: 0.9790 - val_loss: 0.0593 - val_acc: 0.9671\n",
      "Epoch 584/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0612 - acc: 0.9790 - val_loss: 0.0591 - val_acc: 0.9671\n",
      "Epoch 585/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0613 - acc: 0.9790 - val_loss: 0.0591 - val_acc: 0.9671\n",
      "Epoch 586/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0612 - acc: 0.9790 - val_loss: 0.0590 - val_acc: 0.9671\n",
      "Epoch 587/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0613 - acc: 0.9800 - val_loss: 0.0590 - val_acc: 0.9671\n",
      "Epoch 588/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0612 - acc: 0.9790 - val_loss: 0.0590 - val_acc: 0.9671\n",
      "Epoch 589/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0611 - acc: 0.9790 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 590/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0611 - acc: 0.9800 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 591/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0611 - acc: 0.9800 - val_loss: 0.0593 - val_acc: 0.9701\n",
      "Epoch 592/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0611 - acc: 0.9780 - val_loss: 0.0592 - val_acc: 0.9701\n",
      "Epoch 593/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0611 - acc: 0.9790 - val_loss: 0.0596 - val_acc: 0.9641\n",
      "Epoch 594/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0611 - acc: 0.9790 - val_loss: 0.0595 - val_acc: 0.9641\n",
      "Epoch 595/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0610 - acc: 0.9790 - val_loss: 0.0593 - val_acc: 0.9671\n",
      "Epoch 596/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0609 - acc: 0.9800 - val_loss: 0.0593 - val_acc: 0.9671\n",
      "Epoch 597/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0609 - acc: 0.9790 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 598/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0609 - acc: 0.9800 - val_loss: 0.0591 - val_acc: 0.9671\n",
      "Epoch 599/3000\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0608 - acc: 0.9790 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 600/3000\n",
      "1002/1002 [==============================] - 0s 108us/step - loss: 0.0608 - acc: 0.9790 - val_loss: 0.0592 - val_acc: 0.9671\n",
      "Epoch 601/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0608 - acc: 0.9800 - val_loss: 0.0594 - val_acc: 0.9671\n",
      "Epoch 602/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0608 - acc: 0.9790 - val_loss: 0.0593 - val_acc: 0.9671\n",
      "Epoch 603/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0608 - acc: 0.9780 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 604/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0609 - acc: 0.9790 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 605/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0608 - acc: 0.9790 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 606/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0607 - acc: 0.9790 - val_loss: 0.0591 - val_acc: 0.9671\n",
      "Epoch 607/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0607 - acc: 0.9790 - val_loss: 0.0590 - val_acc: 0.9671\n",
      "Epoch 608/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0607 - acc: 0.9790 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 609/3000\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.0607 - acc: 0.9780 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 610/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0607 - acc: 0.9800 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 611/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0606 - acc: 0.9800 - val_loss: 0.0587 - val_acc: 0.9671\n",
      "Epoch 612/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0607 - acc: 0.9790 - val_loss: 0.0587 - val_acc: 0.9671\n",
      "Epoch 613/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0605 - acc: 0.9800 - val_loss: 0.0587 - val_acc: 0.9671\n",
      "Epoch 614/3000\n",
      "1002/1002 [==============================] - 0s 40us/step - loss: 0.0606 - acc: 0.9800 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 615/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0605 - acc: 0.9790 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 616/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0605 - acc: 0.9790 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 617/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0605 - acc: 0.9800 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 618/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0604 - acc: 0.9780 - val_loss: 0.0589 - val_acc: 0.9671\n",
      "Epoch 619/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0605 - acc: 0.9790 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 620/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0604 - acc: 0.9790 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 621/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0605 - acc: 0.9800 - val_loss: 0.0588 - val_acc: 0.9671\n",
      "Epoch 622/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0604 - acc: 0.9790 - val_loss: 0.0587 - val_acc: 0.9671\n",
      "Epoch 623/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0603 - acc: 0.9790 - val_loss: 0.0587 - val_acc: 0.9671\n",
      "Epoch 624/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0604 - acc: 0.9780 - val_loss: 0.0586 - val_acc: 0.9671\n",
      "Epoch 625/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0604 - acc: 0.9790 - val_loss: 0.0587 - val_acc: 0.9671\n",
      "Epoch 626/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0604 - acc: 0.9790 - val_loss: 0.0586 - val_acc: 0.9671\n",
      "Epoch 627/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0603 - acc: 0.9790 - val_loss: 0.0586 - val_acc: 0.9671\n",
      "Epoch 628/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0603 - acc: 0.9800 - val_loss: 0.0585 - val_acc: 0.9671\n",
      "Epoch 629/3000\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0602 - acc: 0.9800 - val_loss: 0.0584 - val_acc: 0.9671\n",
      "Epoch 630/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.0602 - acc: 0.9800 - val_loss: 0.0584 - val_acc: 0.9671\n",
      "Epoch 631/3000\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0603 - acc: 0.9790 - val_loss: 0.0584 - val_acc: 0.9671\n",
      "Epoch 632/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0602 - acc: 0.9790 - val_loss: 0.0584 - val_acc: 0.9671\n",
      "Epoch 633/3000\n",
      "1002/1002 [==============================] - 0s 88us/step - loss: 0.0602 - acc: 0.9790 - val_loss: 0.0583 - val_acc: 0.9671\n",
      "Epoch 634/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0602 - acc: 0.9800 - val_loss: 0.0583 - val_acc: 0.9671\n",
      "Epoch 635/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0601 - acc: 0.9790 - val_loss: 0.0583 - val_acc: 0.9671\n",
      "Epoch 636/3000\n",
      "1002/1002 [==============================] - 0s 38us/step - loss: 0.0602 - acc: 0.9790 - val_loss: 0.0583 - val_acc: 0.9671\n",
      "Epoch 637/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0602 - acc: 0.9790 - val_loss: 0.0582 - val_acc: 0.9671\n",
      "Epoch 638/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0601 - acc: 0.9800 - val_loss: 0.0582 - val_acc: 0.9671\n",
      "Epoch 639/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0601 - acc: 0.9790 - val_loss: 0.0581 - val_acc: 0.9671\n",
      "Epoch 640/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.0600 - acc: 0.9800 - val_loss: 0.0581 - val_acc: 0.9671\n",
      "Epoch 641/3000\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0600 - acc: 0.9790 - val_loss: 0.0582 - val_acc: 0.9671\n",
      "Epoch 642/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0600 - acc: 0.9800 - val_loss: 0.0582 - val_acc: 0.9671\n",
      "Epoch 643/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0600 - acc: 0.9790 - val_loss: 0.0581 - val_acc: 0.9671\n",
      "Epoch 644/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0600 - acc: 0.9800 - val_loss: 0.0581 - val_acc: 0.9671\n",
      "Epoch 645/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0599 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 646/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0599 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 647/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.0598 - acc: 0.9800 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 648/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0599 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 649/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0599 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 650/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0599 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 651/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0598 - acc: 0.9800 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 652/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0599 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 653/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0598 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 654/3000\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.0597 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 655/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0598 - acc: 0.9800 - val_loss: 0.0581 - val_acc: 0.9671\n",
      "Epoch 656/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0597 - acc: 0.9790 - val_loss: 0.0581 - val_acc: 0.9671\n",
      "Epoch 657/3000\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.0597 - acc: 0.9790 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 658/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0596 - acc: 0.9790 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 659/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0596 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 660/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0597 - acc: 0.9800 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 661/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0596 - acc: 0.9800 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 662/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0596 - acc: 0.9810 - val_loss: 0.0578 - val_acc: 0.9671\n",
      "Epoch 663/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0597 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 664/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0595 - acc: 0.9800 - val_loss: 0.0578 - val_acc: 0.9671\n",
      "Epoch 665/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0596 - acc: 0.9800 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 666/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0596 - acc: 0.9810 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 667/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0595 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 668/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0596 - acc: 0.9800 - val_loss: 0.0580 - val_acc: 0.9671\n",
      "Epoch 669/3000\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.0597 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9671\n",
      "Epoch 670/3000\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.0595 - acc: 0.9790 - val_loss: 0.0578 - val_acc: 0.9671\n",
      "Epoch 671/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0594 - acc: 0.9790 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 672/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0595 - acc: 0.9790 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 673/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0594 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 674/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0594 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 675/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0594 - acc: 0.9800 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 676/3000\n",
      "1002/1002 [==============================] - 0s 50us/step - loss: 0.0594 - acc: 0.9800 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 677/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 678/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 679/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0593 - acc: 0.9790 - val_loss: 0.0578 - val_acc: 0.9671\n",
      "Epoch 680/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0592 - acc: 0.9790 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 681/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0592 - acc: 0.9800 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 682/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0593 - acc: 0.9790 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 683/3000\n",
      "1002/1002 [==============================] - 0s 46us/step - loss: 0.0593 - acc: 0.9790 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 684/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0593 - acc: 0.9790 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 685/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0592 - acc: 0.9790 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 686/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0592 - acc: 0.9800 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 687/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0592 - acc: 0.9790 - val_loss: 0.0575 - val_acc: 0.9671\n",
      "Epoch 688/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0592 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 689/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0592 - acc: 0.9790 - val_loss: 0.0577 - val_acc: 0.9671\n",
      "Epoch 690/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0591 - acc: 0.9800 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 691/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0591 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 692/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0591 - acc: 0.9790 - val_loss: 0.0578 - val_acc: 0.9671\n",
      "Epoch 693/3000\n",
      "1002/1002 [==============================] - 0s 46us/step - loss: 0.0590 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 694/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0591 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9671\n",
      "Epoch 695/3000\n",
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0589 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 696/3000\n",
      "1002/1002 [==============================] - 0s 112us/step - loss: 0.0590 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9641\n",
      "Epoch 697/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 0.0589 - acc: 0.9800 - val_loss: 0.0578 - val_acc: 0.9641\n",
      "Epoch 698/3000\n",
      "1002/1002 [==============================] - 0s 111us/step - loss: 0.0590 - acc: 0.9800 - val_loss: 0.0578 - val_acc: 0.9641\n",
      "Epoch 699/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0589 - acc: 0.9790 - val_loss: 0.0576 - val_acc: 0.9641\n",
      "Epoch 700/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0590 - acc: 0.9790 - val_loss: 0.0574 - val_acc: 0.9671\n",
      "Epoch 701/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0589 - acc: 0.9800 - val_loss: 0.0579 - val_acc: 0.9641\n",
      "Epoch 702/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0589 - acc: 0.9800 - val_loss: 0.0577 - val_acc: 0.9641\n",
      "Epoch 703/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0588 - acc: 0.9790 - val_loss: 0.0574 - val_acc: 0.9641\n",
      "Epoch 704/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0589 - acc: 0.9800 - val_loss: 0.0574 - val_acc: 0.9671\n",
      "Epoch 705/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0588 - acc: 0.9800 - val_loss: 0.0574 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0587 - acc: 0.9800 - val_loss: 0.0574 - val_acc: 0.9671\n",
      "Epoch 707/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0588 - acc: 0.9790 - val_loss: 0.0572 - val_acc: 0.9671\n",
      "Epoch 708/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0587 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 709/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0588 - acc: 0.9790 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 710/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0586 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 711/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0586 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 712/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0587 - acc: 0.9810 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 713/3000\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.0587 - acc: 0.9790 - val_loss: 0.0570 - val_acc: 0.9671\n",
      "Epoch 714/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0587 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 715/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0587 - acc: 0.9790 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 716/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 717/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0586 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 718/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 719/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0585 - acc: 0.9790 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 720/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 721/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0586 - acc: 0.9790 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 722/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0585 - acc: 0.9790 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 723/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0570 - val_acc: 0.9671\n",
      "Epoch 724/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 725/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9641\n",
      "Epoch 726/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0584 - acc: 0.9800 - val_loss: 0.0570 - val_acc: 0.9671\n",
      "Epoch 727/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0584 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 728/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0585 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9671\n",
      "Epoch 729/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0584 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 730/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0584 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 731/3000\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 0.0583 - acc: 0.9790 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 732/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0584 - acc: 0.9790 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 733/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0583 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 734/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0583 - acc: 0.9800 - val_loss: 0.0570 - val_acc: 0.9671\n",
      "Epoch 735/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0582 - acc: 0.9790 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 736/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0583 - acc: 0.9790 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 737/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0582 - acc: 0.9800 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 738/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0582 - acc: 0.9800 - val_loss: 0.0568 - val_acc: 0.9671\n",
      "Epoch 739/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0583 - acc: 0.9800 - val_loss: 0.0570 - val_acc: 0.9641\n",
      "Epoch 740/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0583 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9641\n",
      "Epoch 741/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0582 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9641\n",
      "Epoch 742/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0582 - acc: 0.9800 - val_loss: 0.0571 - val_acc: 0.9641\n",
      "Epoch 743/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0581 - acc: 0.9800 - val_loss: 0.0570 - val_acc: 0.9671\n",
      "Epoch 744/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0583 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 745/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0581 - acc: 0.9800 - val_loss: 0.0569 - val_acc: 0.9671\n",
      "Epoch 746/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0580 - acc: 0.9790 - val_loss: 0.0567 - val_acc: 0.9671\n",
      "Epoch 747/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0580 - acc: 0.9790 - val_loss: 0.0566 - val_acc: 0.9671\n",
      "Epoch 748/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 749/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 750/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 751/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 752/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 753/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0580 - acc: 0.9790 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 754/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0581 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 755/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 756/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 757/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 758/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 759/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 760/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0579 - acc: 0.9790 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 761/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0578 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 762/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 763/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0578 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 764/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0578 - acc: 0.9790 - val_loss: 0.0562 - val_acc: 0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 765/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0578 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 766/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0578 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 767/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0577 - acc: 0.9800 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 768/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0578 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 769/3000\n",
      "1002/1002 [==============================] - 0s 58us/step - loss: 0.0577 - acc: 0.9810 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 770/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0577 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 771/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0577 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 772/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.0577 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 773/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 774/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 775/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0577 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 776/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 777/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0575 - acc: 0.9800 - val_loss: 0.0560 - val_acc: 0.9671\n",
      "Epoch 778/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 779/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0576 - acc: 0.9790 - val_loss: 0.0565 - val_acc: 0.9671\n",
      "Epoch 780/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 781/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0575 - acc: 0.9810 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 782/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0575 - acc: 0.9800 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 783/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0575 - acc: 0.9800 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 784/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 785/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0576 - acc: 0.9810 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 786/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0574 - acc: 0.9800 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 787/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0575 - acc: 0.9790 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 788/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0574 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9671\n",
      "Epoch 789/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0574 - acc: 0.9810 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 790/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0574 - acc: 0.9810 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 791/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 792/3000\n",
      "1002/1002 [==============================] - 0s 53us/step - loss: 0.0574 - acc: 0.9790 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 793/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0574 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 794/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 795/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 796/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 797/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 798/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0558 - val_acc: 0.9671\n",
      "Epoch 799/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0572 - acc: 0.9800 - val_loss: 0.0558 - val_acc: 0.9671\n",
      "Epoch 800/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0573 - acc: 0.9800 - val_loss: 0.0557 - val_acc: 0.9671\n",
      "Epoch 801/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0572 - acc: 0.9800 - val_loss: 0.0558 - val_acc: 0.9671\n",
      "Epoch 802/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0572 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 803/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 804/3000\n",
      "1002/1002 [==============================] - 0s 69us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0559 - val_acc: 0.9671\n",
      "Epoch 805/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0571 - acc: 0.9810 - val_loss: 0.0563 - val_acc: 0.9641\n",
      "Epoch 806/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0572 - acc: 0.9800 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 807/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.0502 - acc: 0.983 - 0s 99us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9641\n",
      "Epoch 808/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0563 - val_acc: 0.9641\n",
      "Epoch 809/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9671\n",
      "Epoch 810/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0571 - acc: 0.9810 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 811/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0571 - acc: 0.9810 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 812/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0561 - val_acc: 0.9671\n",
      "Epoch 813/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0571 - acc: 0.9810 - val_loss: 0.0560 - val_acc: 0.9671\n",
      "Epoch 814/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0570 - acc: 0.9810 - val_loss: 0.0560 - val_acc: 0.9671\n",
      "Epoch 815/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0570 - acc: 0.9800 - val_loss: 0.0560 - val_acc: 0.9671\n",
      "Epoch 816/3000\n",
      "1002/1002 [==============================] - 0s 92us/step - loss: 0.0570 - acc: 0.9800 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 817/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.0569 - acc: 0.9800 - val_loss: 0.0560 - val_acc: 0.9671\n",
      "Epoch 818/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.0569 - acc: 0.9800 - val_loss: 0.0558 - val_acc: 0.9671\n",
      "Epoch 819/3000\n",
      "1002/1002 [==============================] - 0s 102us/step - loss: 0.0569 - acc: 0.9810 - val_loss: 0.0562 - val_acc: 0.9671\n",
      "Epoch 820/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0569 - acc: 0.9810 - val_loss: 0.0561 - val_acc: 0.9641\n",
      "Epoch 00820: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adagrad,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist4=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/3000\n",
      "1002/1002 [==============================] - 1s 820us/step - loss: 1.9007 - acc: 0.3144 - val_loss: 1.8217 - val_acc: 0.2994\n",
      "Epoch 2/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 1.7961 - acc: 0.2924 - val_loss: 1.7277 - val_acc: 0.3054\n",
      "Epoch 3/3000\n",
      "1002/1002 [==============================] - 0s 122us/step - loss: 1.7037 - acc: 0.3234 - val_loss: 1.6226 - val_acc: 0.4641\n",
      "Epoch 4/3000\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 1.6037 - acc: 0.5110 - val_loss: 1.5171 - val_acc: 0.5509\n",
      "Epoch 5/3000\n",
      "1002/1002 [==============================] - 0s 119us/step - loss: 1.5020 - acc: 0.5329 - val_loss: 1.4172 - val_acc: 0.5509\n",
      "Epoch 6/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 1.4050 - acc: 0.5329 - val_loss: 1.3226 - val_acc: 0.5509\n",
      "Epoch 7/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 1.3137 - acc: 0.5329 - val_loss: 1.2383 - val_acc: 0.5509\n",
      "Epoch 8/3000\n",
      "1002/1002 [==============================] - 0s 120us/step - loss: 1.2276 - acc: 0.5329 - val_loss: 1.1573 - val_acc: 0.5539\n",
      "Epoch 9/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 1.1480 - acc: 0.5329 - val_loss: 1.0887 - val_acc: 0.5539\n",
      "Epoch 10/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 1.0814 - acc: 0.5349 - val_loss: 1.0344 - val_acc: 0.5539\n",
      "Epoch 11/3000\n",
      "1002/1002 [==============================] - 0s 144us/step - loss: 1.0242 - acc: 0.5359 - val_loss: 0.9812 - val_acc: 0.5539\n",
      "Epoch 12/3000\n",
      "1002/1002 [==============================] - 0s 103us/step - loss: 0.9653 - acc: 0.5379 - val_loss: 0.9278 - val_acc: 0.5569\n",
      "Epoch 13/3000\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 0.9111 - acc: 0.5419 - val_loss: 0.8764 - val_acc: 0.5599\n",
      "Epoch 14/3000\n",
      "1002/1002 [==============================] - 0s 117us/step - loss: 0.8569 - acc: 0.5828 - val_loss: 0.8256 - val_acc: 0.6078\n",
      "Epoch 15/3000\n",
      "1002/1002 [==============================] - 0s 123us/step - loss: 0.8061 - acc: 0.6168 - val_loss: 0.7746 - val_acc: 0.6437\n",
      "Epoch 16/3000\n",
      "1002/1002 [==============================] - 0s 118us/step - loss: 0.7536 - acc: 0.7016 - val_loss: 0.7268 - val_acc: 0.7305\n",
      "Epoch 17/3000\n",
      "1002/1002 [==============================] - 0s 117us/step - loss: 0.7039 - acc: 0.7515 - val_loss: 0.6803 - val_acc: 0.7425\n",
      "Epoch 18/3000\n",
      "1002/1002 [==============================] - 0s 117us/step - loss: 0.6578 - acc: 0.7814 - val_loss: 0.6384 - val_acc: 0.7874\n",
      "Epoch 19/3000\n",
      "1002/1002 [==============================] - 0s 121us/step - loss: 0.6151 - acc: 0.7944 - val_loss: 0.5972 - val_acc: 0.7904\n",
      "Epoch 20/3000\n",
      "1002/1002 [==============================] - 0s 117us/step - loss: 0.5757 - acc: 0.8154 - val_loss: 0.5596 - val_acc: 0.8084\n",
      "Epoch 21/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.5357 - acc: 0.8283 - val_loss: 0.5214 - val_acc: 0.8383\n",
      "Epoch 22/3000\n",
      "1002/1002 [==============================] - 0s 125us/step - loss: 0.5005 - acc: 0.8603 - val_loss: 0.4855 - val_acc: 0.8503\n",
      "Epoch 23/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.4647 - acc: 0.9122 - val_loss: 0.4514 - val_acc: 0.9401\n",
      "Epoch 24/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.4319 - acc: 0.9461 - val_loss: 0.4208 - val_acc: 0.9431\n",
      "Epoch 25/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.4012 - acc: 0.9471 - val_loss: 0.3901 - val_acc: 0.9521\n",
      "Epoch 26/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.3731 - acc: 0.9561 - val_loss: 0.3618 - val_acc: 0.9641\n",
      "Epoch 27/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.3496 - acc: 0.9591 - val_loss: 0.3381 - val_acc: 0.9641\n",
      "Epoch 28/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.3238 - acc: 0.9601 - val_loss: 0.3154 - val_acc: 0.9581\n",
      "Epoch 29/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.3024 - acc: 0.9671 - val_loss: 0.2949 - val_acc: 0.9581\n",
      "Epoch 30/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.2845 - acc: 0.9601 - val_loss: 0.2776 - val_acc: 0.9611\n",
      "Epoch 31/3000\n",
      "1002/1002 [==============================] - 0s 111us/step - loss: 0.2655 - acc: 0.9681 - val_loss: 0.2584 - val_acc: 0.9611\n",
      "Epoch 32/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.2492 - acc: 0.9651 - val_loss: 0.2425 - val_acc: 0.9611\n",
      "Epoch 33/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.2345 - acc: 0.9631 - val_loss: 0.2288 - val_acc: 0.9551\n",
      "Epoch 34/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.2206 - acc: 0.9691 - val_loss: 0.2156 - val_acc: 0.9581\n",
      "Epoch 35/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.2082 - acc: 0.9731 - val_loss: 0.2060 - val_acc: 0.9581\n",
      "Epoch 36/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.1976 - acc: 0.9711 - val_loss: 0.1946 - val_acc: 0.9611\n",
      "Epoch 37/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.1886 - acc: 0.9721 - val_loss: 0.1854 - val_acc: 0.9581\n",
      "Epoch 38/3000\n",
      "1002/1002 [==============================] - ETA: 0s - loss: 0.1806 - acc: 0.975 - 0s 107us/step - loss: 0.1792 - acc: 0.9701 - val_loss: 0.1767 - val_acc: 0.9641\n",
      "Epoch 39/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.1715 - acc: 0.9731 - val_loss: 0.1699 - val_acc: 0.9611\n",
      "Epoch 40/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.1648 - acc: 0.9731 - val_loss: 0.1645 - val_acc: 0.9611\n",
      "Epoch 41/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1584 - acc: 0.9721 - val_loss: 0.1579 - val_acc: 0.9611\n",
      "Epoch 42/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.1528 - acc: 0.9701 - val_loss: 0.1513 - val_acc: 0.9611\n",
      "Epoch 43/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.1469 - acc: 0.9731 - val_loss: 0.1463 - val_acc: 0.9641\n",
      "Epoch 44/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.1423 - acc: 0.9711 - val_loss: 0.1428 - val_acc: 0.9641\n",
      "Epoch 45/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.1385 - acc: 0.9731 - val_loss: 0.1392 - val_acc: 0.9611\n",
      "Epoch 46/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.1351 - acc: 0.9741 - val_loss: 0.1367 - val_acc: 0.9581\n",
      "Epoch 47/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1299 - acc: 0.9750 - val_loss: 0.1312 - val_acc: 0.9641\n",
      "Epoch 48/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.1262 - acc: 0.9721 - val_loss: 0.1283 - val_acc: 0.9611\n",
      "Epoch 49/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1228 - acc: 0.9741 - val_loss: 0.1251 - val_acc: 0.9671\n",
      "Epoch 50/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.1208 - acc: 0.9731 - val_loss: 0.1215 - val_acc: 0.9641\n",
      "Epoch 51/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.1173 - acc: 0.9760 - val_loss: 0.1188 - val_acc: 0.9611\n",
      "Epoch 52/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.1167 - acc: 0.9661 - val_loss: 0.1181 - val_acc: 0.9641\n",
      "Epoch 53/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.1129 - acc: 0.9741 - val_loss: 0.1132 - val_acc: 0.9641\n",
      "Epoch 54/3000\n",
      "1002/1002 [==============================] - 0s 104us/step - loss: 0.1093 - acc: 0.9770 - val_loss: 0.1128 - val_acc: 0.9671\n",
      "Epoch 55/3000\n",
      "1002/1002 [==============================] - 0s 110us/step - loss: 0.1077 - acc: 0.9711 - val_loss: 0.1093 - val_acc: 0.9641\n",
      "Epoch 56/3000\n",
      "1002/1002 [==============================] - 0s 119us/step - loss: 0.1063 - acc: 0.9741 - val_loss: 0.1089 - val_acc: 0.9641\n",
      "Epoch 57/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.1044 - acc: 0.9721 - val_loss: 0.1066 - val_acc: 0.9641\n",
      "Epoch 58/3000\n",
      "1002/1002 [==============================] - 0s 127us/step - loss: 0.1007 - acc: 0.9741 - val_loss: 0.1052 - val_acc: 0.9671\n",
      "Epoch 59/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0990 - acc: 0.9741 - val_loss: 0.1028 - val_acc: 0.9671\n",
      "Epoch 60/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0974 - acc: 0.9760 - val_loss: 0.1020 - val_acc: 0.9641\n",
      "Epoch 61/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0963 - acc: 0.9750 - val_loss: 0.1005 - val_acc: 0.9671\n",
      "Epoch 62/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0948 - acc: 0.9770 - val_loss: 0.0986 - val_acc: 0.9671\n",
      "Epoch 63/3000\n",
      "1002/1002 [==============================] - 0s 54us/step - loss: 0.0935 - acc: 0.9750 - val_loss: 0.0972 - val_acc: 0.9641\n",
      "Epoch 64/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0922 - acc: 0.9790 - val_loss: 0.0963 - val_acc: 0.9611\n",
      "Epoch 65/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0914 - acc: 0.9750 - val_loss: 0.0955 - val_acc: 0.9641\n",
      "Epoch 66/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0890 - acc: 0.9770 - val_loss: 0.0943 - val_acc: 0.9671\n",
      "Epoch 67/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0882 - acc: 0.9741 - val_loss: 0.0920 - val_acc: 0.9671\n",
      "Epoch 68/3000\n",
      "1002/1002 [==============================] - 0s 64us/step - loss: 0.0870 - acc: 0.9750 - val_loss: 0.0940 - val_acc: 0.9611\n",
      "Epoch 69/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0869 - acc: 0.9760 - val_loss: 0.0916 - val_acc: 0.9611\n",
      "Epoch 70/3000\n",
      "1002/1002 [==============================] - 0s 107us/step - loss: 0.0847 - acc: 0.9790 - val_loss: 0.0902 - val_acc: 0.9641\n",
      "Epoch 71/3000\n",
      "1002/1002 [==============================] - 0s 109us/step - loss: 0.0834 - acc: 0.9770 - val_loss: 0.0889 - val_acc: 0.9641\n",
      "Epoch 72/3000\n",
      "1002/1002 [==============================] - 0s 111us/step - loss: 0.0827 - acc: 0.9790 - val_loss: 0.0895 - val_acc: 0.9641\n",
      "Epoch 73/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0827 - acc: 0.9800 - val_loss: 0.0889 - val_acc: 0.9641\n",
      "Epoch 74/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0825 - acc: 0.9750 - val_loss: 0.0861 - val_acc: 0.9641\n",
      "Epoch 75/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0804 - acc: 0.9780 - val_loss: 0.0860 - val_acc: 0.9611\n",
      "Epoch 76/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0795 - acc: 0.9780 - val_loss: 0.0865 - val_acc: 0.9611\n",
      "Epoch 77/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0798 - acc: 0.9790 - val_loss: 0.0850 - val_acc: 0.9611\n",
      "Epoch 78/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0774 - acc: 0.9770 - val_loss: 0.0840 - val_acc: 0.9671\n",
      "Epoch 79/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0773 - acc: 0.9790 - val_loss: 0.0843 - val_acc: 0.9641\n",
      "Epoch 80/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0774 - acc: 0.9770 - val_loss: 0.0842 - val_acc: 0.9611\n",
      "Epoch 81/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0757 - acc: 0.9760 - val_loss: 0.0821 - val_acc: 0.9641\n",
      "Epoch 82/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0751 - acc: 0.9810 - val_loss: 0.0828 - val_acc: 0.9671\n",
      "Epoch 83/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0768 - acc: 0.9750 - val_loss: 0.0801 - val_acc: 0.9671\n",
      "Epoch 84/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0743 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9671\n",
      "Epoch 85/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0730 - acc: 0.9770 - val_loss: 0.0805 - val_acc: 0.9611\n",
      "Epoch 86/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0724 - acc: 0.9790 - val_loss: 0.0795 - val_acc: 0.9641\n",
      "Epoch 87/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0732 - acc: 0.9770 - val_loss: 0.0819 - val_acc: 0.9671\n",
      "Epoch 88/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0727 - acc: 0.9760 - val_loss: 0.0801 - val_acc: 0.9671\n",
      "Epoch 89/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0712 - acc: 0.9750 - val_loss: 0.0805 - val_acc: 0.9641\n",
      "Epoch 90/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0709 - acc: 0.9800 - val_loss: 0.0826 - val_acc: 0.9671\n",
      "Epoch 91/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0702 - acc: 0.9760 - val_loss: 0.0790 - val_acc: 0.9641\n",
      "Epoch 92/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0698 - acc: 0.9790 - val_loss: 0.0808 - val_acc: 0.9641\n",
      "Epoch 93/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0701 - acc: 0.9780 - val_loss: 0.0792 - val_acc: 0.9641\n",
      "Epoch 94/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0686 - acc: 0.9800 - val_loss: 0.0799 - val_acc: 0.9671\n",
      "Epoch 95/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0683 - acc: 0.9790 - val_loss: 0.0801 - val_acc: 0.9611\n",
      "Epoch 96/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0678 - acc: 0.9770 - val_loss: 0.0779 - val_acc: 0.9701\n",
      "Epoch 97/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0677 - acc: 0.9790 - val_loss: 0.0787 - val_acc: 0.9641\n",
      "Epoch 98/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0693 - acc: 0.9790 - val_loss: 0.0814 - val_acc: 0.9611\n",
      "Epoch 99/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0674 - acc: 0.9780 - val_loss: 0.0801 - val_acc: 0.9701\n",
      "Epoch 100/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0676 - acc: 0.9810 - val_loss: 0.0821 - val_acc: 0.9641\n",
      "Epoch 101/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0665 - acc: 0.9770 - val_loss: 0.0795 - val_acc: 0.9671\n",
      "Epoch 102/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0671 - acc: 0.9800 - val_loss: 0.0794 - val_acc: 0.9671\n",
      "Epoch 103/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0653 - acc: 0.9770 - val_loss: 0.0793 - val_acc: 0.9611\n",
      "Epoch 104/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0654 - acc: 0.9780 - val_loss: 0.0792 - val_acc: 0.9701\n",
      "Epoch 105/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0647 - acc: 0.9790 - val_loss: 0.0790 - val_acc: 0.9611\n",
      "Epoch 106/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0660 - acc: 0.9770 - val_loss: 0.0782 - val_acc: 0.9641\n",
      "Epoch 107/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0646 - acc: 0.9800 - val_loss: 0.0779 - val_acc: 0.9671\n",
      "Epoch 108/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0637 - acc: 0.9790 - val_loss: 0.0797 - val_acc: 0.9611\n",
      "Epoch 109/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0644 - acc: 0.9790 - val_loss: 0.0802 - val_acc: 0.9641\n",
      "Epoch 110/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0639 - acc: 0.9770 - val_loss: 0.0799 - val_acc: 0.9641\n",
      "Epoch 111/3000\n",
      "1002/1002 [==============================] - 0s 80us/step - loss: 0.0635 - acc: 0.9800 - val_loss: 0.0810 - val_acc: 0.9671\n",
      "Epoch 112/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0633 - acc: 0.9770 - val_loss: 0.0811 - val_acc: 0.9671\n",
      "Epoch 113/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0631 - acc: 0.9780 - val_loss: 0.0804 - val_acc: 0.9641\n",
      "Epoch 114/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0629 - acc: 0.9760 - val_loss: 0.0823 - val_acc: 0.9611\n",
      "Epoch 115/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0633 - acc: 0.9810 - val_loss: 0.0785 - val_acc: 0.9671\n",
      "Epoch 116/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0636 - acc: 0.9760 - val_loss: 0.0824 - val_acc: 0.9611\n",
      "Epoch 117/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0631 - acc: 0.9800 - val_loss: 0.0783 - val_acc: 0.9671\n",
      "Epoch 118/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0620 - acc: 0.9780 - val_loss: 0.0810 - val_acc: 0.9641\n",
      "Epoch 119/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0625 - acc: 0.9790 - val_loss: 0.0809 - val_acc: 0.9641\n",
      "Epoch 120/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0610 - acc: 0.9800 - val_loss: 0.0799 - val_acc: 0.9641\n",
      "Epoch 121/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0608 - acc: 0.9810 - val_loss: 0.0797 - val_acc: 0.9641\n",
      "Epoch 122/3000\n",
      "1002/1002 [==============================] - 0s 70us/step - loss: 0.0617 - acc: 0.9790 - val_loss: 0.0785 - val_acc: 0.9641\n",
      "Epoch 123/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0611 - acc: 0.9810 - val_loss: 0.0811 - val_acc: 0.9641\n",
      "Epoch 124/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0605 - acc: 0.9780 - val_loss: 0.0791 - val_acc: 0.9611\n",
      "Epoch 125/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0607 - acc: 0.9770 - val_loss: 0.0803 - val_acc: 0.9701\n",
      "Epoch 126/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0602 - acc: 0.9800 - val_loss: 0.0815 - val_acc: 0.9641\n",
      "Epoch 127/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0609 - acc: 0.9780 - val_loss: 0.0818 - val_acc: 0.9611\n",
      "Epoch 00127: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adamax,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist5=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 334 samples\n",
      "Epoch 1/3000\n",
      "1002/1002 [==============================] - 1s 833us/step - loss: 1.8967 - acc: 0.2655 - val_loss: 1.8003 - val_acc: 0.5389\n",
      "Epoch 2/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 1.6576 - acc: 0.5279 - val_loss: 1.4974 - val_acc: 0.5509\n",
      "Epoch 3/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 1.3639 - acc: 0.5329 - val_loss: 1.2186 - val_acc: 0.5539\n",
      "Epoch 4/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 1.1315 - acc: 0.5329 - val_loss: 1.0424 - val_acc: 0.5539\n",
      "Epoch 5/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.9810 - acc: 0.5379 - val_loss: 0.9141 - val_acc: 0.5569\n",
      "Epoch 6/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.8472 - acc: 0.5878 - val_loss: 0.7829 - val_acc: 0.6826\n",
      "Epoch 7/3000\n",
      "1002/1002 [==============================] - 0s 96us/step - loss: 0.7269 - acc: 0.7315 - val_loss: 0.6639 - val_acc: 0.7275\n",
      "Epoch 8/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.6298 - acc: 0.7615 - val_loss: 0.6004 - val_acc: 0.8054\n",
      "Epoch 9/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.5439 - acc: 0.8204 - val_loss: 0.5306 - val_acc: 0.9042\n",
      "Epoch 10/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.4794 - acc: 0.8423 - val_loss: 0.4435 - val_acc: 0.9521\n",
      "Epoch 11/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.4207 - acc: 0.8653 - val_loss: 0.3894 - val_acc: 0.8563\n",
      "Epoch 12/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.3723 - acc: 0.8862 - val_loss: 0.3410 - val_acc: 0.9641\n",
      "Epoch 13/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.3312 - acc: 0.9361 - val_loss: 0.3085 - val_acc: 0.9701\n",
      "Epoch 14/3000\n",
      "1002/1002 [==============================] - 0s 89us/step - loss: 0.2951 - acc: 0.9611 - val_loss: 0.2777 - val_acc: 0.9551\n",
      "Epoch 15/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.2680 - acc: 0.9661 - val_loss: 0.2464 - val_acc: 0.9701\n",
      "Epoch 16/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.2426 - acc: 0.9661 - val_loss: 0.2298 - val_acc: 0.9731\n",
      "Epoch 17/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.2229 - acc: 0.9661 - val_loss: 0.2046 - val_acc: 0.9731\n",
      "Epoch 18/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.2021 - acc: 0.9671 - val_loss: 0.1907 - val_acc: 0.9671\n",
      "Epoch 19/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.1881 - acc: 0.9721 - val_loss: 0.1810 - val_acc: 0.9521\n",
      "Epoch 20/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1764 - acc: 0.9671 - val_loss: 0.1765 - val_acc: 0.9701\n",
      "Epoch 21/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.1626 - acc: 0.9681 - val_loss: 0.1610 - val_acc: 0.9731\n",
      "Epoch 22/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.1542 - acc: 0.9750 - val_loss: 0.1462 - val_acc: 0.9731\n",
      "Epoch 23/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.1457 - acc: 0.9671 - val_loss: 0.1362 - val_acc: 0.9701\n",
      "Epoch 24/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.1377 - acc: 0.9711 - val_loss: 0.1322 - val_acc: 0.9671\n",
      "Epoch 25/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.1328 - acc: 0.9721 - val_loss: 0.1230 - val_acc: 0.9701\n",
      "Epoch 26/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.1254 - acc: 0.9741 - val_loss: 0.1204 - val_acc: 0.9701\n",
      "Epoch 27/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.1196 - acc: 0.9750 - val_loss: 0.1231 - val_acc: 0.9701\n",
      "Epoch 28/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.1169 - acc: 0.9731 - val_loss: 0.1099 - val_acc: 0.9760\n",
      "Epoch 29/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.1114 - acc: 0.9750 - val_loss: 0.1061 - val_acc: 0.9701\n",
      "Epoch 30/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.1070 - acc: 0.9750 - val_loss: 0.1086 - val_acc: 0.9701\n",
      "Epoch 31/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.1047 - acc: 0.9731 - val_loss: 0.1002 - val_acc: 0.9701\n",
      "Epoch 32/3000\n",
      "1002/1002 [==============================] - 0s 105us/step - loss: 0.1009 - acc: 0.9760 - val_loss: 0.1056 - val_acc: 0.9701\n",
      "Epoch 33/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0992 - acc: 0.9741 - val_loss: 0.0935 - val_acc: 0.9731\n",
      "Epoch 34/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0962 - acc: 0.9721 - val_loss: 0.0922 - val_acc: 0.9701\n",
      "Epoch 35/3000\n",
      "1002/1002 [==============================] - 0s 130us/step - loss: 0.0925 - acc: 0.9790 - val_loss: 0.0922 - val_acc: 0.9701\n",
      "Epoch 36/3000\n",
      "1002/1002 [==============================] - 0s 90us/step - loss: 0.0914 - acc: 0.9770 - val_loss: 0.0869 - val_acc: 0.9731\n",
      "Epoch 37/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0899 - acc: 0.9780 - val_loss: 0.0860 - val_acc: 0.9731\n",
      "Epoch 38/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0877 - acc: 0.9741 - val_loss: 0.0911 - val_acc: 0.9731\n",
      "Epoch 39/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0861 - acc: 0.9741 - val_loss: 0.0832 - val_acc: 0.9701\n",
      "Epoch 40/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0848 - acc: 0.9760 - val_loss: 0.0902 - val_acc: 0.9731\n",
      "Epoch 41/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0841 - acc: 0.9731 - val_loss: 0.0788 - val_acc: 0.9701\n",
      "Epoch 42/3000\n",
      "1002/1002 [==============================] - 0s 113us/step - loss: 0.0811 - acc: 0.9770 - val_loss: 0.0842 - val_acc: 0.9701\n",
      "Epoch 43/3000\n",
      "1002/1002 [==============================] - 0s 114us/step - loss: 0.0796 - acc: 0.9770 - val_loss: 0.0852 - val_acc: 0.9701\n",
      "Epoch 44/3000\n",
      "1002/1002 [==============================] - 0s 106us/step - loss: 0.0808 - acc: 0.9760 - val_loss: 0.0769 - val_acc: 0.9760\n",
      "Epoch 45/3000\n",
      "1002/1002 [==============================] - 0s 93us/step - loss: 0.0777 - acc: 0.9760 - val_loss: 0.0804 - val_acc: 0.9760\n",
      "Epoch 46/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0771 - acc: 0.9790 - val_loss: 0.0753 - val_acc: 0.9731\n",
      "Epoch 47/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0764 - acc: 0.9760 - val_loss: 0.0808 - val_acc: 0.9760\n",
      "Epoch 48/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0763 - acc: 0.9780 - val_loss: 0.0713 - val_acc: 0.9731\n",
      "Epoch 49/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0744 - acc: 0.9780 - val_loss: 0.0718 - val_acc: 0.9701\n",
      "Epoch 50/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0747 - acc: 0.9790 - val_loss: 0.0699 - val_acc: 0.9731\n",
      "Epoch 51/3000\n",
      "1002/1002 [==============================] - 0s 44us/step - loss: 0.0727 - acc: 0.9770 - val_loss: 0.0698 - val_acc: 0.9760\n",
      "Epoch 52/3000\n",
      "1002/1002 [==============================] - 0s 57us/step - loss: 0.0737 - acc: 0.9780 - val_loss: 0.0721 - val_acc: 0.9760\n",
      "Epoch 53/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0723 - acc: 0.9770 - val_loss: 0.0742 - val_acc: 0.9731\n",
      "Epoch 54/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0716 - acc: 0.9741 - val_loss: 0.0685 - val_acc: 0.9731\n",
      "Epoch 55/3000\n",
      "1002/1002 [==============================] - 0s 86us/step - loss: 0.0696 - acc: 0.9770 - val_loss: 0.0668 - val_acc: 0.9701\n",
      "Epoch 56/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0700 - acc: 0.9770 - val_loss: 0.0683 - val_acc: 0.9701\n",
      "Epoch 57/3000\n",
      "1002/1002 [==============================] - 0s 49us/step - loss: 0.0701 - acc: 0.9750 - val_loss: 0.0665 - val_acc: 0.9731\n",
      "Epoch 58/3000\n",
      "1002/1002 [==============================] - 0s 63us/step - loss: 0.0693 - acc: 0.9780 - val_loss: 0.0690 - val_acc: 0.9731\n",
      "Epoch 59/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0691 - acc: 0.9780 - val_loss: 0.0699 - val_acc: 0.9760\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002/1002 [==============================] - 0s 51us/step - loss: 0.0689 - acc: 0.9770 - val_loss: 0.0672 - val_acc: 0.9760\n",
      "Epoch 61/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0683 - acc: 0.9721 - val_loss: 0.0632 - val_acc: 0.9731\n",
      "Epoch 62/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0674 - acc: 0.9741 - val_loss: 0.0651 - val_acc: 0.9731\n",
      "Epoch 63/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0681 - acc: 0.9780 - val_loss: 0.0639 - val_acc: 0.9760\n",
      "Epoch 64/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0674 - acc: 0.9790 - val_loss: 0.0641 - val_acc: 0.9701\n",
      "Epoch 65/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0664 - acc: 0.9770 - val_loss: 0.0647 - val_acc: 0.9731\n",
      "Epoch 66/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0667 - acc: 0.9760 - val_loss: 0.0619 - val_acc: 0.9760\n",
      "Epoch 67/3000\n",
      "1002/1002 [==============================] - 0s 60us/step - loss: 0.0664 - acc: 0.9790 - val_loss: 0.0647 - val_acc: 0.9731\n",
      "Epoch 68/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0647 - acc: 0.9780 - val_loss: 0.0639 - val_acc: 0.9760\n",
      "Epoch 69/3000\n",
      "1002/1002 [==============================] - 0s 48us/step - loss: 0.0643 - acc: 0.9760 - val_loss: 0.0625 - val_acc: 0.9701\n",
      "Epoch 70/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.0641 - acc: 0.9770 - val_loss: 0.0655 - val_acc: 0.9701\n",
      "Epoch 71/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0651 - acc: 0.9750 - val_loss: 0.0655 - val_acc: 0.9760\n",
      "Epoch 72/3000\n",
      "1002/1002 [==============================] - 0s 59us/step - loss: 0.0643 - acc: 0.9780 - val_loss: 0.0643 - val_acc: 0.9760\n",
      "Epoch 73/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0632 - acc: 0.9760 - val_loss: 0.0637 - val_acc: 0.9731\n",
      "Epoch 74/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0642 - acc: 0.9770 - val_loss: 0.0595 - val_acc: 0.9760\n",
      "Epoch 75/3000\n",
      "1002/1002 [==============================] - 0s 65us/step - loss: 0.0610 - acc: 0.9800 - val_loss: 0.0653 - val_acc: 0.9731\n",
      "Epoch 76/3000\n",
      "1002/1002 [==============================] - 0s 50us/step - loss: 0.0625 - acc: 0.9780 - val_loss: 0.0618 - val_acc: 0.9731\n",
      "Epoch 77/3000\n",
      "1002/1002 [==============================] - 0s 68us/step - loss: 0.0621 - acc: 0.9780 - val_loss: 0.0617 - val_acc: 0.9760\n",
      "Epoch 78/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0622 - acc: 0.9780 - val_loss: 0.0610 - val_acc: 0.9701\n",
      "Epoch 79/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0612 - acc: 0.9810 - val_loss: 0.0680 - val_acc: 0.9701\n",
      "Epoch 80/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0623 - acc: 0.9750 - val_loss: 0.0619 - val_acc: 0.9701\n",
      "Epoch 81/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.0638 - val_acc: 0.9760\n",
      "Epoch 82/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0621 - acc: 0.9810 - val_loss: 0.0585 - val_acc: 0.9790\n",
      "Epoch 83/3000\n",
      "1002/1002 [==============================] - 0s 56us/step - loss: 0.0606 - acc: 0.9800 - val_loss: 0.0606 - val_acc: 0.9731\n",
      "Epoch 84/3000\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.0604 - acc: 0.9760 - val_loss: 0.0576 - val_acc: 0.9820\n",
      "Epoch 85/3000\n",
      "1002/1002 [==============================] - 0s 55us/step - loss: 0.0605 - acc: 0.9780 - val_loss: 0.0619 - val_acc: 0.9731\n",
      "Epoch 86/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0600 - acc: 0.9780 - val_loss: 0.0582 - val_acc: 0.9790\n",
      "Epoch 87/3000\n",
      "1002/1002 [==============================] - 0s 47us/step - loss: 0.0595 - acc: 0.9800 - val_loss: 0.0592 - val_acc: 0.9760\n",
      "Epoch 88/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0597 - acc: 0.9770 - val_loss: 0.0596 - val_acc: 0.9760\n",
      "Epoch 89/3000\n",
      "1002/1002 [==============================] - 0s 66us/step - loss: 0.0593 - acc: 0.9760 - val_loss: 0.0596 - val_acc: 0.9701\n",
      "Epoch 90/3000\n",
      "1002/1002 [==============================] - 0s 61us/step - loss: 0.0590 - acc: 0.9790 - val_loss: 0.0596 - val_acc: 0.9760\n",
      "Epoch 91/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0584 - acc: 0.9800 - val_loss: 0.0612 - val_acc: 0.9701\n",
      "Epoch 92/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0578 - acc: 0.9810 - val_loss: 0.0586 - val_acc: 0.9790\n",
      "Epoch 93/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0587 - acc: 0.9780 - val_loss: 0.0581 - val_acc: 0.9790\n",
      "Epoch 94/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0574 - acc: 0.9820 - val_loss: 0.0629 - val_acc: 0.9731\n",
      "Epoch 95/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0580 - acc: 0.9780 - val_loss: 0.0587 - val_acc: 0.9731\n",
      "Epoch 96/3000\n",
      "1002/1002 [==============================] - 0s 71us/step - loss: 0.0562 - acc: 0.9800 - val_loss: 0.0680 - val_acc: 0.9731\n",
      "Epoch 97/3000\n",
      "1002/1002 [==============================] - 0s 52us/step - loss: 0.0586 - acc: 0.9770 - val_loss: 0.0602 - val_acc: 0.9731\n",
      "Epoch 98/3000\n",
      "1002/1002 [==============================] - 0s 126us/step - loss: 0.0574 - acc: 0.9810 - val_loss: 0.0574 - val_acc: 0.9790\n",
      "Epoch 99/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0563 - acc: 0.9810 - val_loss: 0.0605 - val_acc: 0.9760\n",
      "Epoch 100/3000\n",
      "1002/1002 [==============================] - 0s 97us/step - loss: 0.0562 - acc: 0.9810 - val_loss: 0.0595 - val_acc: 0.9701\n",
      "Epoch 101/3000\n",
      "1002/1002 [==============================] - 0s 100us/step - loss: 0.0562 - acc: 0.9820 - val_loss: 0.0579 - val_acc: 0.9790\n",
      "Epoch 102/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0557 - acc: 0.9830 - val_loss: 0.0586 - val_acc: 0.9820\n",
      "Epoch 103/3000\n",
      "1002/1002 [==============================] - 0s 124us/step - loss: 0.0563 - acc: 0.9800 - val_loss: 0.0594 - val_acc: 0.9760\n",
      "Epoch 104/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0554 - acc: 0.9810 - val_loss: 0.0568 - val_acc: 0.9790\n",
      "Epoch 105/3000\n",
      "1002/1002 [==============================] - 0s 98us/step - loss: 0.0551 - acc: 0.9810 - val_loss: 0.0568 - val_acc: 0.9790\n",
      "Epoch 106/3000\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0551 - acc: 0.9830 - val_loss: 0.0569 - val_acc: 0.9790\n",
      "Epoch 107/3000\n",
      "1002/1002 [==============================] - 0s 99us/step - loss: 0.0554 - acc: 0.9830 - val_loss: 0.0576 - val_acc: 0.9790\n",
      "Epoch 108/3000\n",
      "1002/1002 [==============================] - 0s 101us/step - loss: 0.0553 - acc: 0.9800 - val_loss: 0.0595 - val_acc: 0.9731\n",
      "Epoch 109/3000\n",
      "1002/1002 [==============================] - 0s 87us/step - loss: 0.0534 - acc: 0.9840 - val_loss: 0.0636 - val_acc: 0.9760\n",
      "Epoch 110/3000\n",
      "1002/1002 [==============================] - 0s 91us/step - loss: 0.0552 - acc: 0.9800 - val_loss: 0.0578 - val_acc: 0.9731\n",
      "Epoch 111/3000\n",
      "1002/1002 [==============================] - 0s 85us/step - loss: 0.0538 - acc: 0.9840 - val_loss: 0.0568 - val_acc: 0.9820\n",
      "Epoch 112/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0549 - acc: 0.9800 - val_loss: 0.0574 - val_acc: 0.9790\n",
      "Epoch 113/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0543 - acc: 0.9790 - val_loss: 0.0556 - val_acc: 0.9790\n",
      "Epoch 114/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0544 - acc: 0.9840 - val_loss: 0.0581 - val_acc: 0.9760\n",
      "Epoch 115/3000\n",
      "1002/1002 [==============================] - 0s 82us/step - loss: 0.0543 - acc: 0.9820 - val_loss: 0.0572 - val_acc: 0.9760\n",
      "Epoch 116/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0526 - acc: 0.9800 - val_loss: 0.0558 - val_acc: 0.9820\n",
      "Epoch 117/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0533 - acc: 0.9830 - val_loss: 0.0558 - val_acc: 0.9790\n",
      "Epoch 118/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0536 - acc: 0.9800 - val_loss: 0.0621 - val_acc: 0.9820\n",
      "Epoch 119/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0537 - acc: 0.9840 - val_loss: 0.0565 - val_acc: 0.9790\n",
      "Epoch 120/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0530 - acc: 0.9800 - val_loss: 0.0554 - val_acc: 0.9790\n",
      "Epoch 121/3000\n",
      "1002/1002 [==============================] - 0s 95us/step - loss: 0.0526 - acc: 0.9840 - val_loss: 0.0570 - val_acc: 0.9790\n",
      "Epoch 122/3000\n",
      "1002/1002 [==============================] - 0s 94us/step - loss: 0.0525 - acc: 0.9830 - val_loss: 0.0558 - val_acc: 0.9790\n",
      "Epoch 123/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0517 - acc: 0.9860 - val_loss: 0.0575 - val_acc: 0.9760\n",
      "Epoch 124/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0519 - acc: 0.9830 - val_loss: 0.0577 - val_acc: 0.9760\n",
      "Epoch 125/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0519 - acc: 0.9850 - val_loss: 0.0582 - val_acc: 0.9760\n",
      "Epoch 126/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0507 - acc: 0.9800 - val_loss: 0.0664 - val_acc: 0.9701\n",
      "Epoch 127/3000\n",
      "1002/1002 [==============================] - 0s 76us/step - loss: 0.0525 - acc: 0.9850 - val_loss: 0.0575 - val_acc: 0.9760\n",
      "Epoch 128/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0512 - acc: 0.9790 - val_loss: 0.0562 - val_acc: 0.9790\n",
      "Epoch 129/3000\n",
      "1002/1002 [==============================] - 0s 84us/step - loss: 0.0504 - acc: 0.9850 - val_loss: 0.0561 - val_acc: 0.9790\n",
      "Epoch 130/3000\n",
      "1002/1002 [==============================] - 0s 73us/step - loss: 0.0504 - acc: 0.9850 - val_loss: 0.0564 - val_acc: 0.9820\n",
      "Epoch 131/3000\n",
      "1002/1002 [==============================] - 0s 75us/step - loss: 0.0519 - acc: 0.9810 - val_loss: 0.0563 - val_acc: 0.9820\n",
      "Epoch 132/3000\n",
      "1002/1002 [==============================] - 0s 62us/step - loss: 0.0502 - acc: 0.9830 - val_loss: 0.0571 - val_acc: 0.9790\n",
      "Epoch 133/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0509 - acc: 0.9830 - val_loss: 0.0582 - val_acc: 0.9790\n",
      "Epoch 134/3000\n",
      "1002/1002 [==============================] - 0s 77us/step - loss: 0.0498 - acc: 0.9830 - val_loss: 0.0562 - val_acc: 0.9820\n",
      "Epoch 135/3000\n",
      "1002/1002 [==============================] - 0s 78us/step - loss: 0.0505 - acc: 0.9830 - val_loss: 0.0555 - val_acc: 0.9790\n",
      "Epoch 136/3000\n",
      "1002/1002 [==============================] - 0s 74us/step - loss: 0.0497 - acc: 0.9820 - val_loss: 0.0568 - val_acc: 0.9790\n",
      "Epoch 137/3000\n",
      "1002/1002 [==============================] - 0s 72us/step - loss: 0.0494 - acc: 0.9830 - val_loss: 0.0563 - val_acc: 0.9790\n",
      "Epoch 138/3000\n",
      "1002/1002 [==============================] - 0s 83us/step - loss: 0.0501 - acc: 0.9830 - val_loss: 0.0564 - val_acc: 0.9790\n",
      "Epoch 139/3000\n",
      "1002/1002 [==============================] - 0s 79us/step - loss: 0.0495 - acc: 0.9850 - val_loss: 0.0559 - val_acc: 0.9790\n",
      "Epoch 140/3000\n",
      "1002/1002 [==============================] - 0s 81us/step - loss: 0.0499 - acc: 0.9820 - val_loss: 0.0555 - val_acc: 0.9790\n",
      "Epoch 00140: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsp=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adagrad=optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adad=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "adamax=optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from decimal import ROUND_UP\n",
    "max_features = X_train.shape[1]\n",
    "m = Sequential()\n",
    "m.add(Dense(39, input_shape=(dims,)))\n",
    "m.add(Activation('elu'))\n",
    "m.add(Dense(25))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dense(7))\n",
    "m.add(Activation('softmax'))\n",
    "#m.add(Round())\n",
    "m.compile(loss='categorical_crossentropy', optimizer=adad,metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "hist6=m.fit(X_train_scaled,\n",
    "          Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=3000, verbose=1,\n",
    "          validation_data=(X_test_scaled, Y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x169a7f78f28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyV9glfEuQ2P/vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169a7fa3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist3.history['loss'])\n",
    "plt.plot(hist4.history['loss'])\n",
    "plt.plot(hist5.history['loss'])\n",
    "plt.plot(hist6.history['loss'])\n",
    "plt.savefig('loss with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('loss with diff. optimizers.eps', format='eps', dpi=1000)\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREI0kYERHRyP8BnoaoPWMPddwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169aac19908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "#plt.plot(hist1.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "#plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist2.history['val_loss'])\n",
    "#plt.plot(hist3.history['loss'])\n",
    "plt.plot(hist3.history['val_loss'])\n",
    "#plt.plot(hist4.history['loss'])\n",
    "plt.plot(hist4.history['val_loss'])\n",
    "#plt.plot(hist5.history['loss'])\n",
    "plt.plot(hist5.history['val_loss'])\n",
    "#plt.plot(hist6.history['loss'])\n",
    "plt.plot(hist6.history['val_loss'])\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])\n",
    "plt.savefig('Validation loss with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('Validation loss with diff. optimizers.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmAxLCsYYYzIsKRhjjMn4/wEJZ2FY0UFOyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169aac2b358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.plot(hist1.history['acc'])\n",
    "#plt.plot(hist1.history['val_loss'])\n",
    "plt.plot(hist2.history['acc'])\n",
    "#plt.plot(hist2.history['val_loss'])\n",
    "plt.plot(hist3.history['acc'])\n",
    "#plt.plot(hist3.history['val_loss'])\n",
    "plt.plot(hist4.history['acc'])\n",
    "#plt.plot(hist4.history['val_loss'])\n",
    "plt.plot(hist5.history['acc'])\n",
    "#plt.plot(hist5.history['val_loss'])\n",
    "plt.plot(hist6.history['acc'])\n",
    "#plt.plot(hist6.history['val_loss'])\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])\n",
    "plt.savefig('training accuracy with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('training accuracy with diff. optimizers.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x169aabe8828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.plot(hist1.history['val_acc'])\n",
    "plt.plot(hist2.history['val_acc'])\n",
    "plt.plot(hist3.history['val_acc'])\n",
    "plt.plot(hist4.history['val_acc'])\n",
    "plt.plot(hist5.history['val_acc'])\n",
    "plt.legend(['adam','sgd','rmsp','adagrad','adamax','adad'])\n",
    "plt.savefig('validation accuracy with diff. optimizers.fig', format='eps', dpi=1000)\n",
    "plt.savefig('validation accuracy with diff. optimizers.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
